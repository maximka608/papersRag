[
    {
        "title": "Expected Frequency Matrices of Elections: Computation, Geometry, and Preference Learning",
        "text": "We use the \"map of elections\" approach of Szufa et al. (AAMAS 2020) to analyze several well-known vote distributions. For each of them, we give an explicit formula or an efficient algorithm for computing its frequency matrix, which captures the probability"
    },
    {
        "title": "Expected Frequency Matrices of Elections: Computation, Geometry, and Preference Learning",
        "text": " that a given candidate appears in a given position in a sampled vote. We use these matrices to draw the \"skeleton map\" of distributions, evaluate its robustness, and analyze its properties. We further use them to identify the nature of several real-world "
    },
    {
        "title": "Expected Frequency Matrices of Elections: Computation, Geometry, and Preference Learning",
        "text": "elections."
    },
    {
        "title": "Deep Normed Embeddings for Patient Representation",
        "text": "We introduce a novel contrastive representation learning objective and a training scheme for clinical time series. Specifically, we project high dimensional E.H.R. data to a closed unit ball of low dimension, encoding geometric priors so that the origin re"
    },
    {
        "title": "Deep Normed Embeddings for Patient Representation",
        "text": "presents an idealized perfect health state and the euclidean norm is associated with the patient's mortality risk. Moreover, using septic patients as an example, we show how we could learn to associate the angle between two vectors with the different organ"
    },
    {
        "title": "Deep Normed Embeddings for Patient Representation",
        "text": " system failures, thereby, learning a compact representation which is indicative of both mortality risk and specific organ failure. We show how the learned embedding can be used for online patient monitoring, supplement clinicians and improve performance o"
    },
    {
        "title": "Deep Normed Embeddings for Patient Representation",
        "text": "f downstream machine learning tasks. This work was partially motivated from the desire and the need to introduce a systematic way of defining intermediate rewards for Reinforcement Learning in critical care medicine. Hence, we also show how such a design i"
    },
    {
        "title": "Deep Normed Embeddings for Patient Representation",
        "text": "n terms of the learned embedding can result in qualitatively different policies and value distributions, as compared with using only terminal rewards."
    },
    {
        "title": "An Efficient Mixture of Deep and Machine Learning Models for COVID-19   and Tuberculosis Detection Using X-Ray Images in Resource Limited Settings",
        "text": "Clinicians in the frontline need to assess quickly whether a patient with symptoms indeed has COVID-19 or not. The difficulty of this task is exacerbated in low resource settings that may not have access to biotechnology tests. Furthermore, Tuberculosis (T"
    },
    {
        "title": "An Efficient Mixture of Deep and Machine Learning Models for COVID-19   and Tuberculosis Detection Using X-Ray Images in Resource Limited Settings",
        "text": "B) remains a major health problem in several low- and middle-income countries and its common symptoms include fever, cough and tiredness, similarly to COVID-19. In order to help in the detection of COVID-19, we propose the extraction of deep features (DF) "
    },
    {
        "title": "An Efficient Mixture of Deep and Machine Learning Models for COVID-19   and Tuberculosis Detection Using X-Ray Images in Resource Limited Settings",
        "text": "from chest X-ray images, a technology available in most hospitals, and their subsequent classification using machine learning methods that do not require large computational resources. We compiled a five-class dataset of X-ray chest images including a bala"
    },
    {
        "title": "An Efficient Mixture of Deep and Machine Learning Models for COVID-19   and Tuberculosis Detection Using X-Ray Images in Resource Limited Settings",
        "text": "nced number of COVID-19, viral pneumonia, bacterial pneumonia, TB, and healthy cases. We compared the performance of pipelines combining 14 individual state-of-the-art pre-trained deep networks for DF extraction with traditional machine learning classifier"
    },
    {
        "title": "An Efficient Mixture of Deep and Machine Learning Models for COVID-19   and Tuberculosis Detection Using X-Ray Images in Resource Limited Settings",
        "text": "s. A pipeline consisting of ResNet-50 for DF computation and ensemble of subspace discriminant classifier was the best performer in the classification of the five classes, achieving a detection accuracy of 91.6+ 2.6% (accuracy + 95% Confidence Interval). F"
    },
    {
        "title": "An Efficient Mixture of Deep and Machine Learning Models for COVID-19   and Tuberculosis Detection Using X-Ray Images in Resource Limited Settings",
        "text": "urthermore, the same pipeline achieved accuracies of 98.6+1.4% and 99.9+0.5% in simpler three-class and two-class classification problems focused on distinguishing COVID-19, TB and healthy cases; and COVID-19 and healthy images, respectively. The pipeline "
    },
    {
        "title": "An Efficient Mixture of Deep and Machine Learning Models for COVID-19   and Tuberculosis Detection Using X-Ray Images in Resource Limited Settings",
        "text": "was computationally efficient requiring just 0.19 second to extract DF per X-ray image and 2 minutes for training a traditional classifier with more than 2000 images on a CPU machine. The results suggest the potential benefits of using our pipeline in the "
    },
    {
        "title": "An Efficient Mixture of Deep and Machine Learning Models for COVID-19   and Tuberculosis Detection Using X-Ray Images in Resource Limited Settings",
        "text": "detection of COVID-19, particularly in resource-limited settings and it can run with limited computational resources."
    },
    {
        "title": "Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning",
        "text": "Federated learning (FL) is a challenging setting for optimization due to the heterogeneity of the data across different clients which gives rise to the client drift phenomenon. In fact, obtaining an algorithm for FL which is uniformly better than simple ce"
    },
    {
        "title": "Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning",
        "text": "ntralized training has been a major open problem thus far. In this work, we propose a general algorithmic framework, Mime, which i) mitigates client drift and ii) adapts arbitrary centralized optimization algorithms such as momentum and Adam to the cross-d"
    },
    {
        "title": "Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning",
        "text": "evice federated learning setting. Mime uses a combination of control-variates and server-level statistics (e.g. momentum) at every client-update step to ensure that each local update mimics that of the centralized method run on iid data. We prove a reducti"
    },
    {
        "title": "Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning",
        "text": "on result showing that Mime can translate the convergence of a generic algorithm in the centralized setting into convergence in the federated setting. Further, we show that when combined with momentum based variance reduction, Mime is provably faster than "
    },
    {
        "title": "Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning",
        "text": "any centralized method--the first such result. We also perform a thorough experimental exploration of Mime's performance on real world datasets."
    },
    {
        "title": "Max-Margin Deep Generative Models for (Semi-)Supervised Learning",
        "text": "Deep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability. However, it is relatively insufficient to empower the discriminative ability of"
    },
    {
        "title": "Max-Margin Deep Generative Models for (Semi-)Supervised Learning",
        "text": " DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs) and a class-conditional variant (mmDCGMs), which explore the strongly discriminative principle of max-margin learning to improve the predictive performance"
    },
    {
        "title": "Max-Margin Deep Generative Models for (Semi-)Supervised Learning",
        "text": " of DGMs in both supervised and semi-supervised learning, while retaining the generative capability. In semi-supervised learning, we use the predictions of a max-margin classifier as the missing labels instead of performing full posterior inference for eff"
    },
    {
        "title": "Max-Margin Deep Generative Models for (Semi-)Supervised Learning",
        "text": "iciency; we also introduce additional max-margin and label-balance regularization terms of unlabeled data for effectiveness. We develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objectives in different settings. Empirica"
    },
    {
        "title": "Max-Margin Deep Generative Models for (Semi-)Supervised Learning",
        "text": "l results on various datasets demonstrate that: (1) max-margin learning can significantly improve the prediction performance of DGMs and meanwhile retain the generative ability; (2) in supervised learning, mmDGMs are competitive to the best fully discrimin"
    },
    {
        "title": "Max-Margin Deep Generative Models for (Semi-)Supervised Learning",
        "text": "ative networks when employing convolutional neural networks as the generative and recognition models; and (3) in semi-supervised learning, mmDCGMs can perform efficient inference and achieve state-of-the-art classification results on several benchmarks."
    },
    {
        "title": "Temporal Phenotyping using Deep Predictive Clustering of Disease   Progression",
        "text": "Due to the wider availability of modern electronic health records, patient care data is often being stored in the form of time-series. Clustering such time-series data is crucial for patient phenotyping, anticipating patients' prognoses by identifying \"sim"
    },
    {
        "title": "Temporal Phenotyping using Deep Predictive Clustering of Disease   Progression",
        "text": "ilar\" patients, and designing treatment guidelines that are tailored to homogeneous patient subgroups. In this paper, we develop a deep learning approach for clustering time-series data, where each cluster comprises patients who share similar future outcom"
    },
    {
        "title": "Temporal Phenotyping using Deep Predictive Clustering of Disease   Progression",
        "text": "es of interest (e.g., adverse events, the onset of comorbidities). To encourage each cluster to have homogeneous future outcomes, the clustering is carried out by learning discrete representations that best describe the future outcome distribution based on"
    },
    {
        "title": "Temporal Phenotyping using Deep Predictive Clustering of Disease   Progression",
        "text": " novel loss functions. Experiments on two real-world datasets show that our model achieves superior clustering performance over state-of-the-art benchmarks and identifies meaningful clusters that can be translated into actionable information for clinical d"
    },
    {
        "title": "Temporal Phenotyping using Deep Predictive Clustering of Disease   Progression",
        "text": "ecision-making."
    },
    {
        "title": "ARM 4-BIT PQ: SIMD-based Acceleration for Approximate Nearest Neighbor   Search on ARM",
        "text": "We accelerate the 4-bit product quantization (PQ) on the ARM architecture. Notably, the drastic performance of the conventional 4-bit PQ strongly relies on x64-specific SIMD register, such as AVX2; hence, we cannot yet achieve such good performance on ARM."
    },
    {
        "title": "ARM 4-BIT PQ: SIMD-based Acceleration for Approximate Nearest Neighbor   Search on ARM",
        "text": " To fill this gap, we first bundle two 128-bit registers as one 256-bit component. We then apply shuffle operations for each using the ARM-specific NEON instruction. By making this simple but critical modification, we achieve a dramatic speedup for the 4-b"
    },
    {
        "title": "ARM 4-BIT PQ: SIMD-based Acceleration for Approximate Nearest Neighbor   Search on ARM",
        "text": "it PQ on an ARM architecture. Experiments show that the proposed method consistently achieves a 10x improvement over the naive PQ with the same accuracy."
    },
    {
        "title": "Neural Networks for Entity Matching: A Survey",
        "text": "Entity matching is the problem of identifying which records refer to the same real-world entity. It has been actively researched for decades, and a variety of different approaches have been developed. Even today, it remains a challenging problem, and there"
    },
    {
        "title": "Neural Networks for Entity Matching: A Survey",
        "text": " is still generous room for improvement. In recent years we have seen new methods based upon deep learning techniques for natural language processing emerge.   In this survey, we present how neural networks have been used for entity matching. Specifically,"
    },
    {
        "title": "Neural Networks for Entity Matching: A Survey",
        "text": " we identify which steps of the entity matching process existing work have targeted using neural networks, and provide an overview of the different techniques used at each step. We also discuss contributions from deep learning in entity matching compared t"
    },
    {
        "title": "Neural Networks for Entity Matching: A Survey",
        "text": "o traditional methods, and propose a taxonomy of deep neural networks for entity matching."
    },
    {
        "title": "Compensating trajectory bias for unsupervised patient stratification   using adversarial recurrent neural networks",
        "text": "Electronic healthcare records are an important source of information which can be used in patient stratification to discover novel disease phenotypes. However, they can be challenging to work with as data is often sparse and irregularly sampled. One approa"
    },
    {
        "title": "Compensating trajectory bias for unsupervised patient stratification   using adversarial recurrent neural networks",
        "text": "ch to solve these limitations is learning dense embeddings that represent individual patient trajectories using a recurrent neural network autoencoder (RNN-AE). This process can be susceptible to unwanted data biases. We show that patient embeddings and cl"
    },
    {
        "title": "Compensating trajectory bias for unsupervised patient stratification   using adversarial recurrent neural networks",
        "text": "usters using previously proposed RNN-AE models might be impacted by a trajectory bias, meaning that results are dominated by the amount of data contained in each patients trajectory, instead of clinically relevant details. We investigate this bias on 2 dat"
    },
    {
        "title": "Compensating trajectory bias for unsupervised patient stratification   using adversarial recurrent neural networks",
        "text": "asets (from different hospitals) and 2 disease areas as well as using different parts of the patient trajectory. Our results using 2 previously published baseline methods indicate a particularly strong bias in case of an event-to-end trajectory. We present"
    },
    {
        "title": "Compensating trajectory bias for unsupervised patient stratification   using adversarial recurrent neural networks",
        "text": " a method that can overcome this issue using an adversarial training scheme on top of a RNN-AE. Our results show that our approach can reduce the trajectory bias in all cases."
    },
    {
        "title": "Application of Deep Self-Attention in Knowledge Tracing",
        "text": "The development of intelligent tutoring system has greatly influenced the way students learn and practice, which increases their learning efficiency. The intelligent tutoring system must model learners' mastery of the knowledge before providing feedback an"
    },
    {
        "title": "Application of Deep Self-Attention in Knowledge Tracing",
        "text": "d advices to learners, so one class of algorithm called \"knowledge tracing\" is surely important. This paper proposed Deep Self-Attentive Knowledge Tracing (DSAKT) based on the data of PTA, an online assessment system used by students in many universities i"
    },
    {
        "title": "Application of Deep Self-Attention in Knowledge Tracing",
        "text": "n China, to help these students learn more efficiently. Experimentation on the data of PTA shows that DSAKT outperforms the other models for knowledge tracing an improvement of AUC by 2.1% on average, and this model also has a good performance on the ASSIS"
    },
    {
        "title": "Application of Deep Self-Attention in Knowledge Tracing",
        "text": "T dataset."
    },
    {
        "title": "Acoustic Echo Cancellation by Combining Adaptive Digital Filter and   Recurrent Neural Network",
        "text": "Acoustic Echo Cancellation (AEC) plays a key role in voice interaction. Due to the explicit mathematical principle and intelligent nature to accommodate conditions, adaptive filters with different types of implementations are always used for AEC, giving co"
    },
    {
        "title": "Acoustic Echo Cancellation by Combining Adaptive Digital Filter and   Recurrent Neural Network",
        "text": "nsiderable performance. However, there would be some kinds of residual echo in the results, including linear residue introduced by mismatching between estimation and the reality and non-linear residue mostly caused by non-linear components on the audio dev"
    },
    {
        "title": "Acoustic Echo Cancellation by Combining Adaptive Digital Filter and   Recurrent Neural Network",
        "text": "ices. The linear residue can be reduced with elaborate structure and methods, leaving the non-linear residue intractable for suppression. Though, some non-linear processing methods have already be raised, they are complicated and inefficient for suppressio"
    },
    {
        "title": "Acoustic Echo Cancellation by Combining Adaptive Digital Filter and   Recurrent Neural Network",
        "text": "n, and would bring damage to the speech audio. In this paper, a fusion scheme by combining adaptive filter and neural network is proposed for AEC. The echo could be reduced in a large scale by adaptive filtering, resulting in little residual echo. Though i"
    },
    {
        "title": "Acoustic Echo Cancellation by Combining Adaptive Digital Filter and   Recurrent Neural Network",
        "text": "t is much smaller than speech audio, it could also be perceived by human ear and would make communication annoy. The neural network is elaborately designed and trained for suppressing such residual echo. Experiments compared with prevailing methods are con"
    },
    {
        "title": "Acoustic Echo Cancellation by Combining Adaptive Digital Filter and   Recurrent Neural Network",
        "text": "ducted, validating the effectiveness and superiority of the proposed combination scheme."
    },
    {
        "title": "PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement   Learning for Robust Decision-Making",
        "text": "Reinforcement learning and symbolic planning have both been used to build intelligent autonomous agents. Reinforcement learning relies on learning from interactions with real world, which often requires an unfeasibly large amount of experience. Symbolic pl"
    },
    {
        "title": "PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement   Learning for Robust Decision-Making",
        "text": "anning relies on manually crafted symbolic knowledge, which may not be robust to domain uncertainties and changes. In this paper we present a unified framework {\\em PEORL} that integrates symbolic planning with hierarchical reinforcement learning (HRL) to "
    },
    {
        "title": "PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement   Learning for Robust Decision-Making",
        "text": "cope with decision-making in a dynamic environment with uncertainties.   Symbolic plans are used to guide the agent's task execution and learning, and the learned experience is fed back to symbolic knowledge to improve planning. This method leads to rapid "
    },
    {
        "title": "PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement   Learning for Robust Decision-Making",
        "text": "policy search and robust symbolic plans in complex domains. The framework is tested on benchmark domains of HRL."
    },
    {
        "title": "Impact of Dataset on Acoustic Models for Automatic Speech Recognition",
        "text": "In Automatic Speech Recognition, GMM-HMM had been widely used for acoustic modelling. With the current advancement of deep learning, the Gaussian Mixture Model (GMM) from acoustic models has been replaced with Deep Neural Network, namely DNN-HMM Acoustic M"
    },
    {
        "title": "Impact of Dataset on Acoustic Models for Automatic Speech Recognition",
        "text": "odels. The GMM models are widely used to create the alignments of the training data for the hybrid deep neural network model, thus making it an important task to create accurate alignments. Many factors such as training dataset size, training data augmenta"
    },
    {
        "title": "Impact of Dataset on Acoustic Models for Automatic Speech Recognition",
        "text": "tion, model hyperparameters, etc., affect the model learning. Traditionally in machine learning, larger datasets tend to have better performance, while smaller datasets tend to trigger over-fitting. The collection of speech data and their accurate transcri"
    },
    {
        "title": "Impact of Dataset on Acoustic Models for Automatic Speech Recognition",
        "text": "ptions is a significant challenge that varies over different languages, and in most cases, it might be limited to big organizations. Moreover, in the case of available large datasets, training a model using such data requires additional time and computing "
    },
    {
        "title": "Impact of Dataset on Acoustic Models for Automatic Speech Recognition",
        "text": "resources, which may not be available. While the data about the accuracy of state-of-the-art ASR models on open-source datasets are published, the study about the impact of the size of a dataset on acoustic models is not readily available. This work aims t"
    },
    {
        "title": "Impact of Dataset on Acoustic Models for Automatic Speech Recognition",
        "text": "o investigate the impact of dataset size variations on the performance of various GMM-HMM Acoustic Models and their respective computational costs."
    },
    {
        "title": "Solving Rubik's Cube with a Robot Hand",
        "text": "We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR"
    },
    {
        "title": "Solving Rubik's Cube with a Robot Hand",
        "text": ") and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfe"
    },
    {
        "title": "Solving Rubik's Cube with a Robot Hand",
        "text": "r. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with "
    },
    {
        "title": "Solving Rubik's Cube with a Robot Hand",
        "text": "a humanoid robot hand, which involves both control and state estimation problems. Videos summarizing our results are available: https://openai.com/blog/solving-rubiks-cube/"
    },
    {
        "title": "Learning Riemannian Manifolds for Geodesic Motion Skills",
        "text": "For robots to work alongside humans and perform in unstructured environments, they must learn new motion skills and adapt them to unseen situations on the fly. This demands learning models that capture relevant motion patterns, while offering enough flexib"
    },
    {
        "title": "Learning Riemannian Manifolds for Geodesic Motion Skills",
        "text": "ility to adapt the encoded skills to new requirements, such as dynamic obstacle avoidance. We introduce a Riemannian manifold perspective on this problem, and propose to learn a Riemannian manifold from human demonstrations on which geodesics are natural m"
    },
    {
        "title": "Learning Riemannian Manifolds for Geodesic Motion Skills",
        "text": "otion skills. We realize this with a variational autoencoder (VAE) over the space of position and orientations of the robot end-effector. Geodesic motion skills let a robot plan movements from and to arbitrary points on the data manifold. They also provide"
    },
    {
        "title": "Learning Riemannian Manifolds for Geodesic Motion Skills",
        "text": " a straightforward method to avoid obstacles by redefining the ambient metric in an online fashion. Moreover, geodesics naturally exploit the manifold resulting from multiple--mode tasks to design motions that were not explicitly demonstrated previously. W"
    },
    {
        "title": "Learning Riemannian Manifolds for Geodesic Motion Skills",
        "text": "e test our learning framework using a 7-DoF robotic manipulator, where the robot satisfactorily learns and reproduces realistic skills featuring elaborated motion patterns, avoids previously unseen obstacles, and generates novel movements in multiple-mode "
    },
    {
        "title": "Learning Riemannian Manifolds for Geodesic Motion Skills",
        "text": "settings."
    },
    {
        "title": "Linear Speedup in Personalized Collaborative Learning",
        "text": "Collaborative training can improve the accuracy of a model for a user by trading off the model's bias (introduced by using data from other users who are potentially different) against its variance (due to the limited amount of data on any single user). In "
    },
    {
        "title": "Linear Speedup in Personalized Collaborative Learning",
        "text": "this work, we formalize the personalized collaborative learning problem as a stochastic optimization of a task $0$ while given access to $N$ related but different tasks $1,\\dots, N$. We give convergence guarantees for two algorithms in this setting -- a po"
    },
    {
        "title": "Linear Speedup in Personalized Collaborative Learning",
        "text": "pular collaboration method known as \\emph{weighted gradient averaging}, and a novel \\emph{bias correction} method -- and explore conditions under which we can achieve linear speedup w.r.t. the number of auxiliary tasks $N$. Further, we also empirically stu"
    },
    {
        "title": "Linear Speedup in Personalized Collaborative Learning",
        "text": "dy their performance confirming our theoretical insights."
    },
    {
        "title": "ENCORE: Ensemble Learning using Convolution Neural Machine Translation   for Automatic Program Repair",
        "text": "Automated generate-and-validate (G&V) program repair techniques typically rely on hard-coded rules, only fix bugs following specific patterns, and are hard to adapt to different programming languages. We propose ENCORE, a new G&V technique, which uses ense"
    },
    {
        "title": "ENCORE: Ensemble Learning using Convolution Neural Machine Translation   for Automatic Program Repair",
        "text": "mble learning on convolutional neural machine translation (NMT) models to automatically fix bugs in multiple programming languages.   We take advantage of the randomness in hyper-parameter tuning to build multiple models that fix different bugs and combine"
    },
    {
        "title": "ENCORE: Ensemble Learning using Convolution Neural Machine Translation   for Automatic Program Repair",
        "text": " them using ensemble learning. This new convolutional NMT approach outperforms the standard long short-term memory (LSTM) approach used in previous work, as it better captures both local and long-distance connections between tokens.   Our evaluation on two"
    },
    {
        "title": "ENCORE: Ensemble Learning using Convolution Neural Machine Translation   for Automatic Program Repair",
        "text": " popular benchmarks, Defects4J and QuixBugs, shows that ENCORE fixed 42 bugs, including 16 that have not been fixed by existing techniques. In addition, ENCORE is the first G&V repair technique to be applied to four popular programming languages (Java, C++"
    },
    {
        "title": "ENCORE: Ensemble Learning using Convolution Neural Machine Translation   for Automatic Program Repair",
        "text": ", Python, and JavaScript), fixing a total of 67 bugs across five benchmarks."
    },
    {
        "title": "Domain Knowledge Aids in Signal Disaggregation; the Example of the   Cumulative Water Heater",
        "text": "In this article we present an unsupervised low-frequency method aimed at detecting and disaggregating the power used by Cumulative Water Heaters (CWH) in residential homes. Our model circumvents the inherent difficulty of unsupervised signal disaggregation"
    },
    {
        "title": "Domain Knowledge Aids in Signal Disaggregation; the Example of the   Cumulative Water Heater",
        "text": " by using both the shape of a power spike and its time of occurrence to identify the contribution of CWH reliably. Indeed, many CHWs in France are configured to turn on automatically during off-peak hours only, and we are able to use this domain knowledge "
    },
    {
        "title": "Domain Knowledge Aids in Signal Disaggregation; the Example of the   Cumulative Water Heater",
        "text": "to aid peak identification despite the low sampling frequency. In order to test our model, we equipped a home with sensors to record the ground-truth consumption of a water heater. We then apply the model to a larger dataset of energy consumption of Hello "
    },
    {
        "title": "Domain Knowledge Aids in Signal Disaggregation; the Example of the   Cumulative Water Heater",
        "text": "Watt users consisting of one month of consumption data for 5k homes at 30-minute resolution. In this dataset we successfully identified CWHs in the majority of cases where consumers declared using them. The remaining part is likely due to possible misconfi"
    },
    {
        "title": "Domain Knowledge Aids in Signal Disaggregation; the Example of the   Cumulative Water Heater",
        "text": "guration of CWHs, since triggering them during off-peak hours requires specific wiring in the electrical panel of the house. Our model, despite its simplicity, offers promising applications: detection of mis-configured CWHs on off-peak contracts and slow p"
    },
    {
        "title": "Domain Knowledge Aids in Signal Disaggregation; the Example of the   Cumulative Water Heater",
        "text": "erformance degradation."
    },
    {
        "title": "LambdaNet: Probabilistic Type Inference using Graph Neural Networks",
        "text": "As gradual typing becomes increasingly popular in languages like Python and TypeScript, there is a growing need to infer type annotations automatically. While type annotations help with tasks like code completion and static error catching, these annotation"
    },
    {
        "title": "LambdaNet: Probabilistic Type Inference using Graph Neural Networks",
        "text": "s cannot be fully determined by compilers and are tedious to annotate by hand. This paper proposes a probabilistic type inference scheme for TypeScript based on a graph neural network. Our approach first uses lightweight source code analysis to generate a "
    },
    {
        "title": "LambdaNet: Probabilistic Type Inference using Graph Neural Networks",
        "text": "program abstraction called a type dependency graph, which links type variables with logical constraints as well as name and usage information. Given this program abstraction, we then use a graph neural network to propagate information between related type "
    },
    {
        "title": "LambdaNet: Probabilistic Type Inference using Graph Neural Networks",
        "text": "variables and eventually make type predictions. Our neural architecture can predict both standard types, like number or string, as well as user-defined types that have not been encountered during training. Our experimental results show that our approach ou"
    },
    {
        "title": "LambdaNet: Probabilistic Type Inference using Graph Neural Networks",
        "text": "tperforms prior work in this space by $14\\%$ (absolute) on library types, while having the ability to make type predictions that are out of scope for existing techniques."
    },
    {
        "title": "About one 3-parameter Model of Testing",
        "text": "This article offers a 3-parameter model of testing, with 1) the difference between the ability level of the examinee and item difficulty; 2) the examinee discrimination and 3) the item discrimination as model parameters."
    },
    {
        "title": "Generative Adversarial Networks for Distributed Intrusion Detection in   the Internet of Things",
        "text": "To reap the benefits of the Internet of Things (IoT), it is imperative to secure the system against cyber attacks in order to enable mission critical and real-time applications. To this end, intrusion detection systems (IDSs) have been widely used to detec"
    },
    {
        "title": "Generative Adversarial Networks for Distributed Intrusion Detection in   the Internet of Things",
        "text": "t anomalies caused by a cyber attacker in IoT systems. However, due to the large-scale nature of the IoT, an IDS must operate in a distributed manner with minimum dependence on a central controller. Moreover, in many scenarios such as health and financial "
    },
    {
        "title": "Generative Adversarial Networks for Distributed Intrusion Detection in   the Internet of Things",
        "text": "applications, the datasets are private and IoTDs may not intend to share such data. To this end, in this paper, a distributed generative adversarial network (GAN) is proposed to provide a fully distributed IDS for the IoT so as to detect anomalous behavior"
    },
    {
        "title": "Generative Adversarial Networks for Distributed Intrusion Detection in   the Internet of Things",
        "text": " without reliance on any centralized controller. In this architecture, every IoTD can monitor its own data as well as neighbor IoTDs to detect internal and external attacks. In addition, the proposed distributed IDS does not require sharing the datasets be"
    },
    {
        "title": "Generative Adversarial Networks for Distributed Intrusion Detection in   the Internet of Things",
        "text": "tween the IoTDs, thus, it can be implemented in IoTs that preserve the privacy of user data such as health monitoring systems or financial applications. It is shown analytically that the proposed distributed GAN has higher accuracy of detecting intrusion c"
    },
    {
        "title": "Generative Adversarial Networks for Distributed Intrusion Detection in   the Internet of Things",
        "text": "ompared to a standalone IDS that has access to only a single IoTD dataset. Simulation results show that, the proposed distributed GAN-based IDS has up to 20% higher accuracy, 25% higher precision, and 60% lower false positive rate compared to a standalone "
    },
    {
        "title": "Generative Adversarial Networks for Distributed Intrusion Detection in   the Internet of Things",
        "text": "GAN-based IDS."
    },
    {
        "title": "Unsupervised Discovery of Temporal Structure in Noisy Data with   Dynamical Components Analysis",
        "text": "Linear dimensionality reduction methods are commonly used to extract low-dimensional structure from high-dimensional data. However, popular methods disregard temporal structure, rendering them prone to extracting noise rather than meaningful dynamics when "
    },
    {
        "title": "Unsupervised Discovery of Temporal Structure in Noisy Data with   Dynamical Components Analysis",
        "text": "applied to time series data. At the same time, many successful unsupervised learning methods for temporal, sequential and spatial data extract features which are predictive of their surrounding context. Combining these approaches, we introduce Dynamical Co"
    },
    {
        "title": "Unsupervised Discovery of Temporal Structure in Noisy Data with   Dynamical Components Analysis",
        "text": "mponents Analysis (DCA), a linear dimensionality reduction method which discovers a subspace of high-dimensional time series data with maximal predictive information, defined as the mutual information between the past and future. We test DCA on synthetic e"
    },
    {
        "title": "Unsupervised Discovery of Temporal Structure in Noisy Data with   Dynamical Components Analysis",
        "text": "xamples and demonstrate its superior ability to extract dynamical structure compared to commonly used linear methods. We also apply DCA to several real-world datasets, showing that the dimensions extracted by DCA are more useful than those extracted by oth"
    },
    {
        "title": "Unsupervised Discovery of Temporal Structure in Noisy Data with   Dynamical Components Analysis",
        "text": "er methods for predicting future states and decoding auxiliary variables. Overall, DCA robustly extracts dynamical structure in noisy, high-dimensional data while retaining the computational efficiency and geometric interpretability of linear dimensionalit"
    },
    {
        "title": "Unsupervised Discovery of Temporal Structure in Noisy Data with   Dynamical Components Analysis",
        "text": "y reduction methods."
    },
    {
        "title": "TAGLETS: A System for Automatic Semi-Supervised Learning with Auxiliary   Data",
        "text": "Machine learning practitioners often have access to a spectrum of data: labeled data for the target task (which is often limited), unlabeled data, and auxiliary data, the many available labeled datasets for other tasks. We describe TAGLETS, a system built "
    },
    {
        "title": "TAGLETS: A System for Automatic Semi-Supervised Learning with Auxiliary   Data",
        "text": "to study techniques for automatically exploiting all three types of data and creating high-quality, servable classifiers. The key components of TAGLETS are: (1) auxiliary data organized according to a knowledge graph, (2) modules encapsulating different me"
    },
    {
        "title": "TAGLETS: A System for Automatic Semi-Supervised Learning with Auxiliary   Data",
        "text": "thods for exploiting auxiliary and unlabeled data, and (3) a distillation stage in which the ensembled modules are combined into a servable model. We compare TAGLETS with state-of-the-art transfer learning and semi-supervised learning methods on four image"
    },
    {
        "title": "TAGLETS: A System for Automatic Semi-Supervised Learning with Auxiliary   Data",
        "text": " classification tasks. Our study covers a range of settings, varying the amount of labeled data and the semantic relatedness of the auxiliary data to the target task. We find that the intelligent incorporation of auxiliary and unlabeled data into multiple "
    },
    {
        "title": "TAGLETS: A System for Automatic Semi-Supervised Learning with Auxiliary   Data",
        "text": "learning techniques enables TAGLETS to match-and most often significantly surpass-these alternatives. TAGLETS is available as an open-source system at github.com/BatsResearch/taglets."
    },
    {
        "title": "Machine Learning Classifiers Do Not Improve the Prediction of Academic   Risk: Evidence from Australia",
        "text": "Machine learning methods tend to outperform traditional statistical models at prediction. In the prediction of academic achievement, ML models have not shown substantial improvement over logistic regression. So far, these results have almost entirely focus"
    },
    {
        "title": "Machine Learning Classifiers Do Not Improve the Prediction of Academic   Risk: Evidence from Australia",
        "text": "ed on college achievement, due to the availability of administrative datasets, and have contained relatively small sample sizes by ML standards. In this article we apply popular machine learning models to a large dataset ($n=1.2$ million) containing primar"
    },
    {
        "title": "Machine Learning Classifiers Do Not Improve the Prediction of Academic   Risk: Evidence from Australia",
        "text": "y and middle school performance on a standardized test given annually to Australian students. We show that machine learning models do not outperform logistic regression for detecting students who will perform in the `below standard' band of achievement upo"
    },
    {
        "title": "Machine Learning Classifiers Do Not Improve the Prediction of Academic   Risk: Evidence from Australia",
        "text": "n sitting their next test, even in a large-$n$ setting."
    },
    {
        "title": "Nonparametric Score Estimators",
        "text": "Estimating the score, i.e., the gradient of log density function, from a set of samples generated by an unknown distribution is a fundamental task in inference and learning of probabilistic models that involve flexible yet intractable densities. Kernel est"
    },
    {
        "title": "Nonparametric Score Estimators",
        "text": "imators based on Stein's methods or score matching have shown promise, however their theoretical properties and relationships have not been fully-understood. We provide a unifying view of these estimators under the framework of regularized nonparametric re"
    },
    {
        "title": "Nonparametric Score Estimators",
        "text": "gression. It allows us to analyse existing estimators and construct new ones with desirable properties by choosing different hypothesis spaces and regularizers. A unified convergence analysis is provided for such estimators. Finally, we propose score estim"
    },
    {
        "title": "Nonparametric Score Estimators",
        "text": "ators based on iterative regularization that enjoy computational benefits from curl-free kernels and fast convergence."
    },
    {
        "title": "Keyphrase Extraction from Disaster-related Tweets",
        "text": "While keyphrase extraction has received considerable attention in recent years, relatively few studies exist on extracting keyphrases from social media platforms such as Twitter, and even fewer for extracting disaster-related keyphrases from such sources. "
    },
    {
        "title": "Keyphrase Extraction from Disaster-related Tweets",
        "text": "During a disaster, keyphrases can be extremely useful for filtering relevant tweets that can enhance situational awareness. Previously, joint training of two different layers of a stacked Recurrent Neural Network for keyword discovery and keyphrase extract"
    },
    {
        "title": "Keyphrase Extraction from Disaster-related Tweets",
        "text": "ion had been shown to be effective in extracting keyphrases from general Twitter data. We improve the model's performance on both general Twitter data and disaster-related Twitter data by incorporating contextual word embeddings, POS-tags, phonetics, and p"
    },
    {
        "title": "Keyphrase Extraction from Disaster-related Tweets",
        "text": "honological features. Moreover, we discuss the shortcomings of the often used F1-measure for evaluating the quality of predicted keyphrases with respect to the ground truth annotations. Instead of the F1-measure, we propose the use of embedding-based metri"
    },
    {
        "title": "Keyphrase Extraction from Disaster-related Tweets",
        "text": "cs to better capture the correctness of the predicted keyphrases. In addition, we also present a novel extension of an embedding-based metric. The extension allows one to better control the penalty for the difference in the number of ground-truth and predi"
    },
    {
        "title": "Keyphrase Extraction from Disaster-related Tweets",
        "text": "cted keyphrases"
    },
    {
        "title": "Filtering Variational Objectives",
        "text": "When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined b"
    },
    {
        "title": "Filtering Variational Objectives",
        "text": "y a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. We present results that relate the t"
    },
    {
        "title": "Filtering Variational Objectives",
        "text": "ightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over"
    },
    {
        "title": "Filtering Variational Objectives",
        "text": " training the same model architecture with the ELBO on sequential data."
    },
    {
        "title": "Outline to Story: Fine-grained Controllable Story Generation from   Cascaded Events",
        "text": "Large-scale pretrained language models have shown thrilling generation capabilities, especially when they generate consistent long text in thousands of words with ease. However, users of these models can only control the prefix of sentences or certain glob"
    },
    {
        "title": "Outline to Story: Fine-grained Controllable Story Generation from   Cascaded Events",
        "text": "al aspects of generated text. It is challenging to simultaneously achieve fine-grained controllability and preserve the state-of-the-art unconditional text generation capability. In this paper, we first propose a new task named \"Outline to Story\" (O2S) as "
    },
    {
        "title": "Outline to Story: Fine-grained Controllable Story Generation from   Cascaded Events",
        "text": "a test bed for fine-grained controllable generation of long text, which generates a multi-paragraph story from cascaded events, i.e. a sequence of outline events that guide subsequent paragraph generation. We then create dedicate datasets for future benchm"
    },
    {
        "title": "Outline to Story: Fine-grained Controllable Story Generation from   Cascaded Events",
        "text": "arks, built by state-of-the-art keyword extraction techniques. Finally, we propose an extremely simple yet strong baseline method for the O2S task, which fine tunes pre-trained language models on augmented sequences of outline-story pairs with simple langu"
    },
    {
        "title": "Outline to Story: Fine-grained Controllable Story Generation from   Cascaded Events",
        "text": "age modeling objective. Our method does not introduce any new parameters or perform any architecture modification, except several special tokens as delimiters to build augmented sequences. Extensive experiments on various datasets demonstrate state-of-the-"
    },
    {
        "title": "Outline to Story: Fine-grained Controllable Story Generation from   Cascaded Events",
        "text": "art conditional story generation performance with our model, achieving better fine-grained controllability and user flexibility. Our paper is among the first ones by our knowledge to propose a model and to create datasets for the task of \"outline to story\""
    },
    {
        "title": "Outline to Story: Fine-grained Controllable Story Generation from   Cascaded Events",
        "text": ". Our work also instantiates research interest of fine-grained controllable generation of open-domain long text, where controlling inputs are represented by short text."
    },
    {
        "title": "A Review of Single-Source Deep Unsupervised Visual Domain Adaptation",
        "text": "Large-scale labeled training datasets have enabled deep neural networks to excel across a wide range of benchmark vision tasks. However, in many applications, it is prohibitively expensive and time-consuming to obtain large quantities of labeled data. To c"
    },
    {
        "title": "A Review of Single-Source Deep Unsupervised Visual Domain Adaptation",
        "text": "ope with limited labeled training data, many have attempted to directly apply models trained on a large-scale labeled source domain to another sparsely labeled or unlabeled target domain. Unfortunately, direct transfer across domains often performs poorly "
    },
    {
        "title": "A Review of Single-Source Deep Unsupervised Visual Domain Adaptation",
        "text": "due to the presence of domain shift or dataset bias. Domain adaptation is a machine learning paradigm that aims to learn a model from a source domain that can perform well on a different (but related) target domain. In this paper, we review the latest sing"
    },
    {
        "title": "A Review of Single-Source Deep Unsupervised Visual Domain Adaptation",
        "text": "le-source deep unsupervised domain adaptation methods focused on visual tasks and discuss new perspectives for future research. We begin with the definitions of different domain adaptation strategies and the descriptions of existing benchmark datasets. We "
    },
    {
        "title": "A Review of Single-Source Deep Unsupervised Visual Domain Adaptation",
        "text": "then summarize and compare different categories of single-source unsupervised domain adaptation methods, including discrepancy-based methods, adversarial discriminative methods, adversarial generative methods, and self-supervision-based methods. Finally, w"
    },
    {
        "title": "A Review of Single-Source Deep Unsupervised Visual Domain Adaptation",
        "text": "e discuss future research directions with challenges and possible solutions."
    },
    {
        "title": "Supervised Contrastive Replay: Revisiting the Nearest Class Mean   Classifier in Online Class-Incremental Continual Learning",
        "text": "Online class-incremental continual learning (CL) studies the problem of learning new classes continually from an online non-stationary data stream, intending to adapt to new data while mitigating catastrophic forgetting. While memory replay has shown promi"
    },
    {
        "title": "Supervised Contrastive Replay: Revisiting the Nearest Class Mean   Classifier in Online Class-Incremental Continual Learning",
        "text": "sing results, the recency bias in online learning caused by the commonly used Softmax classifier remains an unsolved challenge. Although the Nearest-Class-Mean (NCM) classifier is significantly undervalued in the CL community, we demonstrate that it is a s"
    },
    {
        "title": "Supervised Contrastive Replay: Revisiting the Nearest Class Mean   Classifier in Online Class-Incremental Continual Learning",
        "text": "imple yet effective substitute for the Softmax classifier. It addresses the recency bias and avoids structural changes in the fully-connected layer for new classes. Moreover, we observe considerable and consistent performance gains when replacing the Softm"
    },
    {
        "title": "Supervised Contrastive Replay: Revisiting the Nearest Class Mean   Classifier in Online Class-Incremental Continual Learning",
        "text": "ax classifier with the NCM classifier for several state-of-the-art replay methods. To leverage the NCM classifier more effectively, data embeddings belonging to the same class should be clustered and well-separated from those with a different class label. "
    },
    {
        "title": "Supervised Contrastive Replay: Revisiting the Nearest Class Mean   Classifier in Online Class-Incremental Continual Learning",
        "text": "To this end, we contribute Supervised Contrastive Replay (SCR), which explicitly encourages samples from the same class to cluster tightly in embedding space while pushing those of different classes further apart during replay-based training. Overall, we o"
    },
    {
        "title": "Supervised Contrastive Replay: Revisiting the Nearest Class Mean   Classifier in Online Class-Incremental Continual Learning",
        "text": "bserve that our proposed SCR substantially reduces catastrophic forgetting and outperforms state-of-the-art CL methods by a significant margin on a variety of datasets."
    },
    {
        "title": "The Labeling Distribution Matrix (LDM): A Tool for Estimating Machine   Learning Algorithm Capacity",
        "text": "Algorithm performance in supervised learning is a combination of memorization, generalization, and luck. By estimating how much information an algorithm can memorize from a dataset, we can set a lower bound on the amount of performance due to other factors"
    },
    {
        "title": "The Labeling Distribution Matrix (LDM): A Tool for Estimating Machine   Learning Algorithm Capacity",
        "text": " such as generalization and luck. With this goal in mind, we introduce the Labeling Distribution Matrix (LDM) as a tool for estimating the capacity of learning algorithms. The method attempts to characterize the diversity of possible outputs by an algorith"
    },
    {
        "title": "The Labeling Distribution Matrix (LDM): A Tool for Estimating Machine   Learning Algorithm Capacity",
        "text": "m for different training datasets, using this to measure algorithm flexibility and responsiveness to data. We test the method on several supervised learning algorithms, and find that while the results are not conclusive, the LDM does allow us to gain poten"
    },
    {
        "title": "The Labeling Distribution Matrix (LDM): A Tool for Estimating Machine   Learning Algorithm Capacity",
        "text": "tially valuable insight into the prediction behavior of algorithms. We also introduce the Label Recorder as an additional tool for estimating algorithm capacity, with more promising initial results."
    },
    {
        "title": "On the Replicability and Reproducibility of Deep Learning in Software   Engineering",
        "text": "Deep learning (DL) techniques have gained significant popularity among software engineering (SE) researchers in recent years. This is because they can often solve many SE challenges without enormous manual feature engineering effort and complex domain know"
    },
    {
        "title": "On the Replicability and Reproducibility of Deep Learning in Software   Engineering",
        "text": "ledge. Although many DL studies have reported substantial advantages over other state-of-the-art models on effectiveness, they often ignore two factors: (1) replicability - whether the reported experimental result can be approximately reproduced in high pr"
    },
    {
        "title": "On the Replicability and Reproducibility of Deep Learning in Software   Engineering",
        "text": "obability with the same DL model and the same data; and (2) reproducibility - whether one reported experimental findings can be reproduced by new experiments with the same experimental protocol and DL model, but different sampled real-world data. Unlike tr"
    },
    {
        "title": "On the Replicability and Reproducibility of Deep Learning in Software   Engineering",
        "text": "aditional machine learning (ML) models, DL studies commonly overlook these two factors and declare them as minor threats or leave them for future work. This is mainly due to high model complexity with many manually set parameters and the time-consuming opt"
    },
    {
        "title": "On the Replicability and Reproducibility of Deep Learning in Software   Engineering",
        "text": "imization process. In this study, we conducted a literature review on 93 DL studies recently published in twenty SE journals or conferences. Our statistics show the urgency of investigating these two factors in SE. Moreover, we re-ran four representative D"
    },
    {
        "title": "On the Replicability and Reproducibility of Deep Learning in Software   Engineering",
        "text": "L models in SE. Experimental results show the importance of replicability and reproducibility, where the reported performance of a DL model could not be replicated for an unstable optimization process. Reproducibility could be substantially compromised if "
    },
    {
        "title": "On the Replicability and Reproducibility of Deep Learning in Software   Engineering",
        "text": "the model training is not convergent, or if performance is sensitive to the size of vocabulary and testing data. It is therefore urgent for the SE community to provide a long-lasting link to a replication package, enhance DL-based solution stability and co"
    },
    {
        "title": "On the Replicability and Reproducibility of Deep Learning in Software   Engineering",
        "text": "nvergence, and avoid performance sensitivity on different sampled data."
    },
    {
        "title": "Improving image generative models with human interactions",
        "text": "GANs provide a framework for training generative models which mimic a data distribution. However, in many cases we wish to train these generative models to optimize some auxiliary objective function within the data it generates, such as making more aesthet"
    },
    {
        "title": "Improving image generative models with human interactions",
        "text": "ically pleasing images. In some cases, these objective functions are difficult to evaluate, e.g. they may require human interaction. Here, we develop a system for efficiently improving a GAN to target an objective involving human interaction, specifically "
    },
    {
        "title": "Improving image generative models with human interactions",
        "text": "generating images that increase rates of positive user interactions. To improve the generative model, we build a model of human behavior in the targeted domain from a relatively small set of interactions, and then use this behavioral model as an auxiliary "
    },
    {
        "title": "Improving image generative models with human interactions",
        "text": "loss function to improve the generative model. We show that this system is successful at improving positive interaction rates, at least on simulated data, and characterize some of the factors that affect its performance."
    },
    {
        "title": "Survey on Multi-Agent Q-Learning frameworks for resource management in   wireless sensor network",
        "text": "This report aims to survey multi-agent Q-Learning algorithms, analyze different game theory frameworks used, address each framework's applications, and report challenges and future directions. The target application for this study is resource management in"
    },
    {
        "title": "Survey on Multi-Agent Q-Learning frameworks for resource management in   wireless sensor network",
        "text": " the wireless sensor network.   In the first section, the author provided an introduction regarding the applications of wireless sensor networks. After that, the author presented a summary of the Q-Learning algorithm, a well-known classic solution for mode"
    },
    {
        "title": "Survey on Multi-Agent Q-Learning frameworks for resource management in   wireless sensor network",
        "text": "l-free reinforcement learning problems.   In the third section, the author extended the Q-Learning algorithm for multi-agent scenarios and discussed its challenges.   In the fourth section, the author surveyed sets of game-theoretic frameworks that researc"
    },
    {
        "title": "Survey on Multi-Agent Q-Learning frameworks for resource management in   wireless sensor network",
        "text": "hers used to address this problem for resource allocation and task scheduling in the wireless sensor networks. Lastly, the author mentioned some interesting open challenges in this domain."
    },
    {
        "title": "A survey of Bayesian Network structure learning",
        "text": "Bayesian Networks (BNs) have become increasingly popular over the last few decades as a tool for reasoning under uncertainty in fields as diverse as medicine, biology, epidemiology, economics and the social sciences. This is especially true in real-world a"
    },
    {
        "title": "A survey of Bayesian Network structure learning",
        "text": "reas where we seek to answer complex questions based on hypothetical evidence to determine actions for intervention. However, determining the graphical structure of a BN remains a major challenge, especially when modelling a problem under causal assumption"
    },
    {
        "title": "A survey of Bayesian Network structure learning",
        "text": "s. Solutions to this problem include the automated discovery of BN graphs from data, constructing them based on expert knowledge, or a combination of the two. This paper provides a comprehensive review of combinatoric algorithms proposed for learning BN st"
    },
    {
        "title": "A survey of Bayesian Network structure learning",
        "text": "ructure from data, describing 61 algorithms including prototypical, well-established and state-of-the-art approaches. The basic approach of each algorithm is described in consistent terms, and the similarities and differences between them highlighted. Meth"
    },
    {
        "title": "A survey of Bayesian Network structure learning",
        "text": "ods of evaluating algorithms and their comparative performance are discussed including the consistency of claims made in the literature. Approaches for dealing with data noise in real-world datasets and incorporating expert knowledge into the learning proc"
    },
    {
        "title": "A survey of Bayesian Network structure learning",
        "text": "ess are also covered."
    },
    {
        "title": "DAMNED: A Distributed and Multithreaded Neural Event-Driven simulation   framework",
        "text": "In a Spiking Neural Networks (SNN), spike emissions are sparsely and irregularly distributed both in time and in the network architecture. Since a current feature of SNNs is a low average activity, efficient implementations of SNNs are usually based on an "
    },
    {
        "title": "DAMNED: A Distributed and Multithreaded Neural Event-Driven simulation   framework",
        "text": "Event-Driven Simulation (EDS). On the other hand, simulations of large scale neural networks can take advantage of distributing the neurons on a set of processors (either workstation cluster or parallel computer). This article presents DAMNED, a large scal"
    },
    {
        "title": "DAMNED: A Distributed and Multithreaded Neural Event-Driven simulation   framework",
        "text": "e SNN simulation framework able to gather the benefits of EDS and parallel computing. Two levels of parallelism are combined: Distributed mapping of the neural topology, at the network level, and local multithreaded allocation of resources for simultaneous"
    },
    {
        "title": "DAMNED: A Distributed and Multithreaded Neural Event-Driven simulation   framework",
        "text": " processing of events, at the neuron level. Based on the causality of events, a distributed solution is proposed for solving the complex problem of scheduling without synchronization barrier."
    },
    {
        "title": "Quasi-Global Momentum: Accelerating Decentralized Deep Learning on   Heterogeneous Data",
        "text": "Decentralized training of deep learning models is a key element for enabling data privacy and on-device learning over networks. In realistic learning scenarios, the presence of heterogeneity across different clients' local datasets poses an optimization ch"
    },
    {
        "title": "Quasi-Global Momentum: Accelerating Decentralized Deep Learning on   Heterogeneous Data",
        "text": "allenge and may severely deteriorate the generalization performance. In this paper, we investigate and identify the limitation of several decentralized optimization algorithms for different degrees of data heterogeneity. We propose a novel momentum-based m"
    },
    {
        "title": "Quasi-Global Momentum: Accelerating Decentralized Deep Learning on   Heterogeneous Data",
        "text": "ethod to mitigate this decentralized training difficulty. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10, ImageNet, and AG News) and several network topologies (Ring and Social Network) that our method is much more robust t"
    },
    {
        "title": "Quasi-Global Momentum: Accelerating Decentralized Deep Learning on   Heterogeneous Data",
        "text": "o the heterogeneity of clients' data than other existing methods, by a significant improvement in test performance ($1\\% \\!-\\! 20\\%$). Our code is publicly available."
    },
    {
        "title": "A Mixed Observability Markov Decision Process Model for Musical Pitch",
        "text": "Partially observable Markov decision processes have been widely used to provide models for real-world decision making problems. In this paper, we will provide a method in which a slightly different version of them called Mixed observability Markov decision"
    },
    {
        "title": "A Mixed Observability Markov Decision Process Model for Musical Pitch",
        "text": " process, MOMDP, is going to join with our problem. Basically, we aim at offering a behavioural model for interaction of intelligent agents with musical pitch environment and we will show that how MOMDP can shed some light on building up a decision making "
    },
    {
        "title": "A Mixed Observability Markov Decision Process Model for Musical Pitch",
        "text": "model for musical pitch conveniently."
    },
    {
        "title": "One-Class Kernel Spectral Regression",
        "text": "The paper introduces a new efficient nonlinear one-class classifier formulated as the Rayleigh quotient criterion optimisation. The method, operating in a reproducing kernel Hilbert space, minimises the scatter of target distribution along an optimal proje"
    },
    {
        "title": "One-Class Kernel Spectral Regression",
        "text": "ction direction while at the same time keeping projections of positive observations distant from the mean of the negative class. We provide a graph embedding view of the problem which can then be solved efficiently using the spectral regression approach. I"
    },
    {
        "title": "One-Class Kernel Spectral Regression",
        "text": "n this sense, unlike previous similar methods which often require costly eigen-computations of dense matrices, the proposed approach casts the problem under consideration into a regression framework which is computationally more efficient. In particular, i"
    },
    {
        "title": "One-Class Kernel Spectral Regression",
        "text": "t is shown that the dominant complexity of the proposed method is the complexity of computing the kernel matrix. Additional appealing characteristics of the proposed one-class classifier are: 1-the ability to be trained in an incremental fashion (allowing "
    },
    {
        "title": "One-Class Kernel Spectral Regression",
        "text": "for application in streaming data scenarios while also reducing the computational complexity in a non-streaming operation mode); 2-being unsupervised, but providing the option for refining the solution using negative training examples, when available; And "
    },
    {
        "title": "One-Class Kernel Spectral Regression",
        "text": "last but not the least, 3-the use of the kernel trick which facilitates a nonlinear mapping of the data into a high-dimensional feature space to seek better solutions."
    },
    {
        "title": "Augmenting Neural Networks with First-order Logic",
        "text": "Today, the dominant paradigm for training neural networks involves minimizing task loss on a large dataset. Using world knowledge to inform a model, and yet retain the ability to perform end-to-end training remains an open question. In this paper, we prese"
    },
    {
        "title": "Augmenting Neural Networks with First-order Logic",
        "text": "nt a novel framework for introducing declarative knowledge to neural network architectures in order to guide training and prediction. Our framework systematically compiles logical statements into computation graphs that augment a neural network without ext"
    },
    {
        "title": "Augmenting Neural Networks with First-order Logic",
        "text": "ra learnable parameters or manual redesign. We evaluate our modeling strategy on three tasks: machine comprehension, natural language inference, and text chunking. Our experiments show that knowledge-augmented networks can strongly improve over baselines, "
    },
    {
        "title": "Augmenting Neural Networks with First-order Logic",
        "text": "especially in low-data regimes."
    },
    {
        "title": "\"The Human Body is a Black Box\": Supporting Clinical Decision-Making   with Deep Learning",
        "text": "Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to accuracy, f"
    },
    {
        "title": "\"The Human Body is a Black Box\": Supporting Clinical Decision-Making   with Deep Learning",
        "text": "airness, accountability, and transparency that come from actual, situated use. Serious questions remain under examined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions t"
    },
    {
        "title": "\"The Human Body is a Black Box\": Supporting Clinical Decision-Making   with Deep Learning",
        "text": "o professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in"
    },
    {
        "title": "\"The Human Body is a Black Box\": Supporting Clinical Decision-Making   with Deep Learning",
        "text": " the early diagnosis and treatment of sepsis. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing soc"
    },
    {
        "title": "\"The Human Body is a Black Box\": Supporting Clinical Decision-Making   with Deep Learning",
        "text": "ial and professional contexts. Rather than focusing on model interpretability to ensure a fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical de"
    },
    {
        "title": "\"The Human Body is a Black Box\": Supporting Clinical Decision-Making   with Deep Learning",
        "text": "cision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechan"
    },
    {
        "title": "\"The Human Body is a Black Box\": Supporting Clinical Decision-Making   with Deep Learning",
        "text": "isms of institutional accountability and considerations for designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demo"
    },
    {
        "title": "\"The Human Body is a Black Box\": Supporting Clinical Decision-Making   with Deep Learning",
        "text": "nstrates other means and goals to achieve FATML values in design and in practice."
    },
    {
        "title": "Model-Based Safe Policy Search from Signal Temporal Logic Specifications   Using Recurrent Neural Networks",
        "text": "We propose a policy search approach to learn controllers from specifications given as Signal Temporal Logic (STL) formulae. The system model, which is unknown but assumed to be an affine control system, is learned together with the control policy. The mode"
    },
    {
        "title": "Model-Based Safe Policy Search from Signal Temporal Logic Specifications   Using Recurrent Neural Networks",
        "text": "l is implemented as two feedforward neural networks (FNNs) - one for the drift, and one for the control directions. To capture the history dependency of STL specifications, we use a recurrent neural network (RNN) to implement the control policy. In contras"
    },
    {
        "title": "Model-Based Safe Policy Search from Signal Temporal Logic Specifications   Using Recurrent Neural Networks",
        "text": "t to prevalent model-free methods, the learning approach proposed here takes advantage of the learned model and is more efficient. We use control barrier functions (CBFs) with the learned model to improve the safety of the system. We validate our algorithm"
    },
    {
        "title": "Model-Based Safe Policy Search from Signal Temporal Logic Specifications   Using Recurrent Neural Networks",
        "text": " via simulations and experiments. The results show that our approach can satisfy the given specification within very few system runs, and can be used for on-line control."
    },
    {
        "title": "Continuum armed bandit problem of few variables in high dimensions",
        "text": "We consider the stochastic and adversarial settings of continuum armed bandits where the arms are indexed by [0,1]^d. The reward functions r:[0,1]^d -> R are assumed to intrinsically depend on at most k coordinate variables implying r(x_1,..,x_d) = g(x_{i_"
    },
    {
        "title": "Continuum armed bandit problem of few variables in high dimensions",
        "text": "1},..,x_{i_k}) for distinct and unknown i_1,..,i_k from {1,..,d} and some locally Holder continuous g:[0,1]^k -> R with exponent 0 < alpha <= 1. Firstly, assuming (i_1,..,i_k) to be fixed across time, we propose a simple modification of the CAB1 algorithm "
    },
    {
        "title": "Continuum armed bandit problem of few variables in high dimensions",
        "text": "where we construct the discrete set of sampling points to obtain a bound of O(n^((alpha+k)/(2*alpha+k)) (log n)^((alpha)/(2*alpha+k)) C(k,d)) on the regret, with C(k,d) depending at most polynomially in k and sub-logarithmically in d. The construction is b"
    },
    {
        "title": "Continuum armed bandit problem of few variables in high dimensions",
        "text": "ased on creating partitions of {1,..,d} into k disjoint subsets and is probabilistic, hence our result holds with high probability. Secondly we extend our results to also handle the more general case where (i_1,...,i_k) can change over time and derive regr"
    },
    {
        "title": "Continuum armed bandit problem of few variables in high dimensions",
        "text": "et bounds for the same."
    },
    {
        "title": "Online Learning Schemes for Power Allocation in Energy Harvesting   Communications",
        "text": "We consider the problem of power allocation over a time-varying channel with unknown distribution in energy harvesting communication systems. In this problem, the transmitter has to choose the transmit power based on the amount of stored energy in its batt"
    },
    {
        "title": "Online Learning Schemes for Power Allocation in Energy Harvesting   Communications",
        "text": "ery with the goal of maximizing the average rate obtained over time. We model this problem as a Markov decision process (MDP) with the transmitter as the agent, the battery status as the state, the transmit power as the action and the rate obtained as the "
    },
    {
        "title": "Online Learning Schemes for Power Allocation in Energy Harvesting   Communications",
        "text": "reward. The average reward maximization problem over the MDP can be solved by a linear program (LP) that uses the transition probabilities for the state-action pairs and their reward values to choose a power allocation policy. Since the rewards associated "
    },
    {
        "title": "Online Learning Schemes for Power Allocation in Energy Harvesting   Communications",
        "text": "the state-action pairs are unknown, we propose two online learning algorithms: UCLP and Epoch-UCLP that learn these rewards and adapt their policies along the way. The UCLP algorithm solves the LP at each step to decide its current policy using the upper c"
    },
    {
        "title": "Online Learning Schemes for Power Allocation in Energy Harvesting   Communications",
        "text": "onfidence bounds on the rewards, while the Epoch-UCLP algorithm divides the time into epochs, solves the LP only at the beginning of the epochs and follows the obtained policy in that epoch. We prove that the reward losses or regrets incurred by both these"
    },
    {
        "title": "Online Learning Schemes for Power Allocation in Energy Harvesting   Communications",
        "text": " algorithms are upper bounded by constants. Epoch-UCLP incurs a higher regret compared to UCLP, but reduces the computational requirements substantially. We also show that the presented algorithms work for online learning in cost minimization problems like"
    },
    {
        "title": "Online Learning Schemes for Power Allocation in Energy Harvesting   Communications",
        "text": " the packet scheduling with power-delay tradeoff with minor changes."
    },
    {
        "title": "Online Learning Rate Adaptation with Hypergradient Descent",
        "text": "We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice. We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochas"
    },
    {
        "title": "Online Learning Rate Adaptation with Hypergradient Descent",
        "text": "tic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms. Our method works by dynamically updati"
    },
    {
        "title": "Online Learning Rate Adaptation with Hypergradient Descent",
        "text": "ng the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself. Computing this \"hypergradient\" needs little additional computation, requires only one extra copy of the original gradient to be stored "
    },
    {
        "title": "Online Learning Rate Adaptation with Hypergradient Descent",
        "text": "in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation."
    },
    {
        "title": "Detecting Malicious Accounts showing Adversarial Behavior in   Permissionless Blockchains",
        "text": "Different types of malicious activities have been flagged in multiple permissionless blockchains such as bitcoin, Ethereum etc. While some malicious activities exploit vulnerabilities in the infrastructure of the blockchain, some target its users through s"
    },
    {
        "title": "Detecting Malicious Accounts showing Adversarial Behavior in   Permissionless Blockchains",
        "text": "ocial engineering techniques. To address these problems, we aim at automatically flagging blockchain accounts that originate such malicious exploitation of accounts of other participants. To that end, we identify a robust supervised machine learning (ML) a"
    },
    {
        "title": "Detecting Malicious Accounts showing Adversarial Behavior in   Permissionless Blockchains",
        "text": "lgorithm that is resistant to any bias induced by an over representation of certain malicious activity in the available dataset, as well as is robust against adversarial attacks. We find that most of the malicious activities reported thus far, for example,"
    },
    {
        "title": "Detecting Malicious Accounts showing Adversarial Behavior in   Permissionless Blockchains",
        "text": " in Ethereum blockchain ecosystem, behaves statistically similar. Further, the previously used ML algorithms for identifying malicious accounts show bias towards a particular malicious activity which is over-represented. In the sequel, we identify that Neu"
    },
    {
        "title": "Detecting Malicious Accounts showing Adversarial Behavior in   Permissionless Blockchains",
        "text": "ral Networks (NN) holds up the best in the face of such bias inducing dataset at the same time being robust against certain adversarial attacks."
    },
    {
        "title": "Geometrical complexity of data approximators",
        "text": "There are many methods developed to approximate a cloud of vectors embedded in high-dimensional space by simpler objects: starting from principal points and linear manifolds to self-organizing maps, neural gas, elastic maps, various types of principal curv"
    },
    {
        "title": "Geometrical complexity of data approximators",
        "text": "es and principal trees, and so on. For each type of approximators the measure of the approximator complexity was developed too. These measures are necessary to find the balance between accuracy and complexity and to define the optimal approximations of a g"
    },
    {
        "title": "Geometrical complexity of data approximators",
        "text": "iven type. We propose a measure of complexity (geometrical complexity) which is applicable to approximators of several types and which allows comparing data approximations of different types."
    },
    {
        "title": "Stochastic Shortest Path with Adversarially Changing Costs",
        "text": "Stochastic shortest path (SSP) is a well-known problem in planning and control, in which an agent has to reach a goal state in minimum total expected cost. In this paper we present the adversarial SSP model that also accounts for adversarial changes in the"
    },
    {
        "title": "Stochastic Shortest Path with Adversarially Changing Costs",
        "text": " costs over time, while the underlying transition function remains unchanged. Formally, an agent interacts with an SSP environment for $K$ episodes, the cost function changes arbitrarily between episodes, and the transitions are unknown to the agent. We de"
    },
    {
        "title": "Stochastic Shortest Path with Adversarially Changing Costs",
        "text": "velop the first algorithms for adversarial SSPs and prove high probability regret bounds of $\\widetilde O (\\sqrt{K})$ assuming all costs are strictly positive, and $\\widetilde O (K^{3/4})$ in the general case. We are the first to consider this natural sett"
    },
    {
        "title": "Stochastic Shortest Path with Adversarially Changing Costs",
        "text": "ing of adversarial SSP and obtain sub-linear regret for it."
    },
    {
        "title": "Phasic dopamine release identification using ensemble of AlexNet",
        "text": "Dopamine (DA) is an organic chemical that influences several parts of behaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is a technique used for in vivo phasic dopamine release measurements. The analysis of such measurements, though, req"
    },
    {
        "title": "Phasic dopamine release identification using ensemble of AlexNet",
        "text": "uires notable effort. In this paper, we present the use of convolutional neural networks (CNNs) for the identification of phasic dopamine releases."
    },
    {
        "title": "Universal Hopfield Networks: A General Framework for Single-Shot   Associative Memory Models",
        "text": "A large number of neural network models of associative memory have been proposed in the literature. These include the classical Hopfield networks (HNs), sparse distributed memories (SDMs), and more recently the modern continuous Hopfield networks (MCHNs), "
    },
    {
        "title": "Universal Hopfield Networks: A General Framework for Single-Shot   Associative Memory Models",
        "text": "which possesses close links with self-attention in machine learning. In this paper, we propose a general framework for understanding the operation of such memory networks as a sequence of three operations: similarity, separation, and projection. We derive "
    },
    {
        "title": "Universal Hopfield Networks: A General Framework for Single-Shot   Associative Memory Models",
        "text": "all these memory models as instances of our general framework with differing similarity and separation functions. We extend the mathematical framework of Krotov et al (2020) to express general associative memory models using neural network dynamics with on"
    },
    {
        "title": "Universal Hopfield Networks: A General Framework for Single-Shot   Associative Memory Models",
        "text": "ly second-order interactions between neurons, and derive a general energy function that is a Lyapunov function of the dynamics. Finally, using our framework, we empirically investigate the capacity of using different similarity functions for these associat"
    },
    {
        "title": "Universal Hopfield Networks: A General Framework for Single-Shot   Associative Memory Models",
        "text": "ive memory models, beyond the dot product similarity measure, and demonstrate empirically that Euclidean or Manhattan distance similarity metrics perform substantially better in practice on many tasks, enabling a more robust retrieval and higher memory cap"
    },
    {
        "title": "Universal Hopfield Networks: A General Framework for Single-Shot   Associative Memory Models",
        "text": "acity than existing models."
    },
    {
        "title": "One-to-many Approach for Improving Super-Resolution",
        "text": "Recently, there has been discussions on the ill-posed nature of super-resolution that multiple possible reconstructions exist for a given low-resolution image. Using normalizing flows, SRflow[23] achieves state-of-the-art perceptual quality by learning the"
    },
    {
        "title": "One-to-many Approach for Improving Super-Resolution",
        "text": " distribution of the output instead of a deterministic output to one estimate. In this paper, we adapt the concepts of SRFlow to improve GAN-based super-resolution by properly implementing the one-to-many property. We modify the generator to estimate a dis"
    },
    {
        "title": "One-to-many Approach for Improving Super-Resolution",
        "text": "tribution as a mapping from random noise. We improve the content loss that hampers the perceptual training objectives. We also propose additional training techniques to further enhance the perceptual quality of generated images. Using our proposed methods,"
    },
    {
        "title": "One-to-many Approach for Improving Super-Resolution",
        "text": " we were able to improve the performance of ESRGAN[1] in x4 perceptual SR and achieve the state-of-the-art LPIPS score in x16 perceptual extreme SR by applying our methods to RFB-ESRGAN[21]."
    },
    {
        "title": "Sparsity-Aware SSAF Algorithm with Individual Weighting Factors for   Acoustic Echo Cancellation",
        "text": "In this paper, we propose and analyze the sparsity-aware sign subband adaptive filtering with individual weighting factors (S-IWF-SSAF) algorithm, and consider its application in acoustic echo cancellation (AEC). Furthermore, we design a joint optimization"
    },
    {
        "title": "Sparsity-Aware SSAF Algorithm with Individual Weighting Factors for   Acoustic Echo Cancellation",
        "text": " scheme of the step-size and the sparsity penalty parameter to enhance the S-IWF-SSAF performance in terms of convergence rate and steady-state error. A theoretical analysis shows that the S-IWF-SSAF algorithm outperforms the previous sign subband adaptive"
    },
    {
        "title": "Sparsity-Aware SSAF Algorithm with Individual Weighting Factors for   Acoustic Echo Cancellation",
        "text": " filtering with individual weighting factors (IWF-SSAF) algorithm in sparse scenarios. In particular, compared with the existing analysis on the IWF-SSAF algorithm, the proposed analysis does not require the assumptions of large number of subbands, long ad"
    },
    {
        "title": "Sparsity-Aware SSAF Algorithm with Individual Weighting Factors for   Acoustic Echo Cancellation",
        "text": "aptive filter, and paraunitary analysis filter bank, and matches well the simulated results. Simulations in both system identification and AEC situations have demonstrated our theoretical analysis and the effectiveness of the proposed algorithms."
    },
    {
        "title": "New feature for Complex Network based on Ant Colony Optimization for   High Level Classification",
        "text": "Low level classification extracts features from the elements, i.e. physical to use them to train a model for a later classification. High level classification uses high level features, the existent patterns, relationship between the data and combines low a"
    },
    {
        "title": "New feature for Complex Network based on Ant Colony Optimization for   High Level Classification",
        "text": "nd high level features for classification. High Level features can be got from Complex Network created over the data. Local and global features are used to describe the structure of a Complex Network, i.e. Average Neighbor Degree, Average Clustering. The p"
    },
    {
        "title": "New feature for Complex Network based on Ant Colony Optimization for   High Level Classification",
        "text": "resent work proposed a novel feature to describe the architecture of the Network following a Ant Colony System approach. The experiments shows the advantage of using this feature because the sensibility with data of different classes."
    },
    {
        "title": "DataWords: Getting Contrarian with Text, Structured Data and   Explanations",
        "text": "Our goal is to build classification models using a combination of free-text and structured data. To do this, we represent structured data by text sentences, DataWords, so that similar data items are mapped into the same sentence. This permits modeling a mi"
    },
    {
        "title": "DataWords: Getting Contrarian with Text, Structured Data and   Explanations",
        "text": "xture of text and structured data by using only text-modeling algorithms. Several examples illustrate that it is possible to improve text classification performance by first running extraction tools (named entity recognition), then converting the output to"
    },
    {
        "title": "DataWords: Getting Contrarian with Text, Structured Data and   Explanations",
        "text": " DataWords, and adding the DataWords to the original text -- before model building and classification. This approach also allows us to produce explanations for inferences in terms of both free text and structured data."
    },
    {
        "title": "LADDER: A Human-Level Bidding Agent for Large-Scale Real-Time Online   Auctions",
        "text": "We present LADDER, the first deep reinforcement learning agent that can successfully learn control policies for large-scale real-world problems directly from raw inputs composed of high-level semantic information. The agent is based on an asynchronous stoc"
    },
    {
        "title": "LADDER: A Human-Level Bidding Agent for Large-Scale Real-Time Online   Auctions",
        "text": "hastic variant of DQN (Deep Q Network) named DASQN. The inputs of the agent are plain-text descriptions of states of a game of incomplete information, i.e. real-time large scale online auctions, and the rewards are auction profits of very large scale. We a"
    },
    {
        "title": "LADDER: A Human-Level Bidding Agent for Large-Scale Real-Time Online   Auctions",
        "text": "pply the agent to an essential portion of JD's online RTB (real-time bidding) advertising business and find that it easily beats the former state-of-the-art bidding policy that had been carefully engineered and calibrated by human experts: during JD.com's "
    },
    {
        "title": "LADDER: A Human-Level Bidding Agent for Large-Scale Real-Time Online   Auctions",
        "text": "June 18th anniversary sale, the agent increased the company's ads revenue from the portion by more than 50%, while the advertisers' ROI (return on investment) also improved significantly."
    },
    {
        "title": "Embedding Text in Hyperbolic Spaces",
        "text": "Natural language text exhibits hierarchical structure in a variety of respects. Ideally, we could incorporate our prior knowledge of this hierarchical structure into unsupervised learning algorithms that work on text data. Recent work by Nickel & Kiela (20"
    },
    {
        "title": "Embedding Text in Hyperbolic Spaces",
        "text": "17) proposed using hyperbolic instead of Euclidean embedding spaces to represent hierarchical data and demonstrated encouraging results when embedding graphs. In this work, we extend their method with a re-parameterization technique that allows us to learn"
    },
    {
        "title": "Embedding Text in Hyperbolic Spaces",
        "text": " hyperbolic embeddings of arbitrarily parameterized objects. We apply this framework to learn word and sentence embeddings in hyperbolic space in an unsupervised manner from text corpora. The resulting embeddings seem to encode certain intuitive notions of"
    },
    {
        "title": "Embedding Text in Hyperbolic Spaces",
        "text": " hierarchy, such as word-context frequency and phrase constituency. However, the implicit continuous hierarchy in the learned hyperbolic space makes interrogating the model's learned hierarchies more difficult than for models that learn explicit edges betw"
    },
    {
        "title": "Embedding Text in Hyperbolic Spaces",
        "text": "een items. The learned hyperbolic embeddings show improvements over Euclidean embeddings in some -- but not all -- downstream tasks, suggesting that hierarchical organization is more useful for some tasks than others."
    },
    {
        "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised   Learning",
        "text": "Self-supervised learning in computer vision aims to pre-train an image encoder using a large amount of unlabeled images or (image, text) pairs. The pre-trained image encoder can then be used as a feature extractor to build downstream classifiers for many d"
    },
    {
        "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised   Learning",
        "text": "ownstream tasks with a small amount of or no labeled training data. In this work, we propose BadEncoder, the first backdoor attack to self-supervised learning. In particular, our BadEncoder injects backdoors into a pre-trained image encoder such that the d"
    },
    {
        "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised   Learning",
        "text": "ownstream classifiers built based on the backdoored image encoder for different downstream tasks simultaneously inherit the backdoor behavior. We formulate our BadEncoder as an optimization problem and we propose a gradient descent based method to solve it"
    },
    {
        "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised   Learning",
        "text": ", which produces a backdoored image encoder from a clean one. Our extensive empirical evaluation results on multiple datasets show that our BadEncoder achieves high attack success rates while preserving the accuracy of the downstream classifiers. We also s"
    },
    {
        "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised   Learning",
        "text": "how the effectiveness of BadEncoder using two publicly available, real-world image encoders, i.e., Google's image encoder pre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training (CLIP) image encoder pre-trained on 400 million (image, t"
    },
    {
        "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised   Learning",
        "text": "ext) pairs collected from the Internet. Moreover, we consider defenses including Neural Cleanse and MNTD (empirical defenses) as well as PatchGuard (a provable defense). Our results show that these defenses are insufficient to defend against BadEncoder, hi"
    },
    {
        "title": "BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised   Learning",
        "text": "ghlighting the needs for new defenses against our BadEncoder. Our code is publicly available at: https://github.com/jjy1994/BadEncoder."
    },
    {
        "title": "Label Cleaning Multiple Instance Learning: Refining Coarse Annotations   on Single Whole-Slide Images",
        "text": "Annotating cancerous regions in whole-slide images (WSIs) of pathology samples plays a critical role in clinical diagnosis, biomedical research, and machine learning algorithms development. However, generating exhaustive and accurate annotations is labor-i"
    },
    {
        "title": "Label Cleaning Multiple Instance Learning: Refining Coarse Annotations   on Single Whole-Slide Images",
        "text": "ntensive, challenging, and costly. Drawing only coarse and approximate annotations is a much easier task, less costly, and it alleviates pathologists' workload. In this paper, we study the problem of refining these approximate annotations in digital pathol"
    },
    {
        "title": "Label Cleaning Multiple Instance Learning: Refining Coarse Annotations   on Single Whole-Slide Images",
        "text": "ogy to obtain more accurate ones. Some previous works have explored obtaining machine learning models from these inaccurate annotations, but few of them tackle the refinement problem where the mislabeled regions should be explicitly identified and correcte"
    },
    {
        "title": "Label Cleaning Multiple Instance Learning: Refining Coarse Annotations   on Single Whole-Slide Images",
        "text": "d, and all of them require a -- often very large -- number of training samples. We present a method, named Label Cleaning Multiple Instance Learning (LC-MIL), to refine coarse annotations on a single WSI without the need of external training data. Patches "
    },
    {
        "title": "Label Cleaning Multiple Instance Learning: Refining Coarse Annotations   on Single Whole-Slide Images",
        "text": "cropped from a WSI with inaccurate labels are processed jointly within a multiple instance learning framework, mitigating their impact on the predictive model and refining the segmentation. Our experiments on a heterogeneous WSI set with breast cancer lymp"
    },
    {
        "title": "Label Cleaning Multiple Instance Learning: Refining Coarse Annotations   on Single Whole-Slide Images",
        "text": "h node metastasis, liver cancer, and colorectal cancer samples show that LC-MIL significantly refines the coarse annotations, outperforming state-of-the-art alternatives, even while learning from a single slide. Moreover, we demonstrate how real annotation"
    },
    {
        "title": "Label Cleaning Multiple Instance Learning: Refining Coarse Annotations   on Single Whole-Slide Images",
        "text": "s drawn by pathologists can be efficiently refined and improved by the proposed approach. All these results demonstrate that LC-MIL is a promising, light-weight tool to provide fine-grained annotations from coarsely annotated pathology sets."
    },
    {
        "title": "Combining Observational and Randomized Data for Estimating Heterogeneous   Treatment Effects",
        "text": "Estimating heterogeneous treatment effects is an important problem across many domains. In order to accurately estimate such treatment effects, one typically relies on data from observational studies or randomized experiments. Currently, most existing work"
    },
    {
        "title": "Combining Observational and Randomized Data for Estimating Heterogeneous   Treatment Effects",
        "text": "s rely exclusively on observational data, which is often confounded and, hence, yields biased estimates. While observational data is confounded, randomized data is unconfounded, but its sample size is usually too small to learn heterogeneous treatment effe"
    },
    {
        "title": "Combining Observational and Randomized Data for Estimating Heterogeneous   Treatment Effects",
        "text": "cts. In this paper, we propose to estimate heterogeneous treatment effects by combining large amounts of observational data and small amounts of randomized data via representation learning. In particular, we introduce a two-step framework: first, we use ob"
    },
    {
        "title": "Combining Observational and Randomized Data for Estimating Heterogeneous   Treatment Effects",
        "text": "servational data to learn a shared structure (in form of a representation); and then, we use randomized data to learn the data-specific structures. We analyze the finite sample properties of our framework and compare them to several natural baselines. As s"
    },
    {
        "title": "Combining Observational and Randomized Data for Estimating Heterogeneous   Treatment Effects",
        "text": "uch, we derive conditions for when combining observational and randomized data is beneficial, and for when it is not. Based on this, we introduce a sample-efficient algorithm, called CorNet. We use extensive simulation studies to verify the theoretical pro"
    },
    {
        "title": "Combining Observational and Randomized Data for Estimating Heterogeneous   Treatment Effects",
        "text": "perties of CorNet and multiple real-world datasets to demonstrate our method's superiority compared to existing methods."
    },
    {
        "title": "Cumulative Stay-time Representation for Electronic Health Records in   Medical Event Time Prediction",
        "text": "We address the problem of predicting when a disease will develop, i.e., medical event time (MET), from a patient's electronic health record (EHR). The MET of non-communicable diseases like diabetes is highly correlated to cumulative health conditions, more"
    },
    {
        "title": "Cumulative Stay-time Representation for Electronic Health Records in   Medical Event Time Prediction",
        "text": " specifically, how much time the patient spent with specific health conditions in the past. The common time-series representation is indirect in extracting such information from EHR because it focuses on detailed dependencies between values in successive o"
    },
    {
        "title": "Cumulative Stay-time Representation for Electronic Health Records in   Medical Event Time Prediction",
        "text": "bservations, not cumulative information. We propose a novel data representation for EHR called cumulative stay-time representation (CTR), which directly models such cumulative health conditions. We derive a trainable construction of CTR based on neural net"
    },
    {
        "title": "Cumulative Stay-time Representation for Electronic Health Records in   Medical Event Time Prediction",
        "text": "works that has the flexibility to fit the target data and scalability to handle high-dimensional EHR. Numerical experiments using synthetic and real-world datasets demonstrate that CTR alone achieves a high prediction performance, and it enhances the perfo"
    },
    {
        "title": "Cumulative Stay-time Representation for Electronic Health Records in   Medical Event Time Prediction",
        "text": "rmance of existing models when combined with them."
    },
    {
        "title": "Residual Network Based Direct Synthesis of EM Structures: A Study on   One-to-One Transformers",
        "text": "We propose using machine learning models for the direct synthesis of on-chip electromagnetic (EM) passive structures to enable rapid or even automated designs and optimizations of RF/mm-Wave circuits. As a proof of concept, we demonstrate the direct synthe"
    },
    {
        "title": "Residual Network Based Direct Synthesis of EM Structures: A Study on   One-to-One Transformers",
        "text": "sis of a 1:1 transformer on a 45nm SOI process using our proposed neural network model. Using pre-existing transformer s-parameter files and their geometric design training samples, the model predicts target geometric designs."
    },
    {
        "title": "A Distributed Frank-Wolfe Framework for Learning Low-Rank Matrices with   the Trace Norm",
        "text": "We consider the problem of learning a high-dimensional but low-rank matrix from a large-scale dataset distributed over several machines, where low-rankness is enforced by a convex trace norm constraint. We propose DFW-Trace, a distributed Frank-Wolfe algor"
    },
    {
        "title": "A Distributed Frank-Wolfe Framework for Learning Low-Rank Matrices with   the Trace Norm",
        "text": "ithm which leverages the low-rank structure of its updates to achieve efficiency in time, memory and communication usage. The step at the heart of DFW-Trace is solved approximately using a distributed version of the power method. We provide a theoretical a"
    },
    {
        "title": "A Distributed Frank-Wolfe Framework for Learning Low-Rank Matrices with   the Trace Norm",
        "text": "nalysis of the convergence of DFW-Trace, showing that we can ensure sublinear convergence in expectation to an optimal solution with few power iterations per epoch. We implement DFW-Trace in the Apache Spark distributed programming framework and validate t"
    },
    {
        "title": "A Distributed Frank-Wolfe Framework for Learning Low-Rank Matrices with   the Trace Norm",
        "text": "he usefulness of our approach on synthetic and real data, including the ImageNet dataset with high-dimensional features extracted from a deep neural network."
    },
    {
        "title": "StandardGAN: Multi-source Domain Adaptation for Semantic Segmentation of   Very High Resolution Satellite Images by Data Standardization",
        "text": "Domain adaptation for semantic segmentation has recently been actively studied to increase the generalization capabilities of deep learning models. The vast majority of the domain adaptation methods tackle single-source case, where the model trained on a s"
    },
    {
        "title": "StandardGAN: Multi-source Domain Adaptation for Semantic Segmentation of   Very High Resolution Satellite Images by Data Standardization",
        "text": "ingle source domain is adapted to a target domain. However, these methods have limited practical real world applications, since usually one has multiple source domains with different data distributions. In this work, we deal with the multi-source domain ad"
    },
    {
        "title": "StandardGAN: Multi-source Domain Adaptation for Semantic Segmentation of   Very High Resolution Satellite Images by Data Standardization",
        "text": "aptation problem. Our method, namely StandardGAN, standardizes each source and target domains so that all the data have similar data distributions. We then use the standardized source domains to train a classifier and segment the standardized target domain"
    },
    {
        "title": "StandardGAN: Multi-source Domain Adaptation for Semantic Segmentation of   Very High Resolution Satellite Images by Data Standardization",
        "text": ". We conduct extensive experiments on two remote sensing data sets, in which the first one consists of multiple cities from a single country, and the other one contains multiple cities from different countries. Our experimental results show that the standa"
    },
    {
        "title": "StandardGAN: Multi-source Domain Adaptation for Semantic Segmentation of   Very High Resolution Satellite Images by Data Standardization",
        "text": "rdized data generated by StandardGAN allow the classifiers to generate significantly better segmentation."
    },
    {
        "title": "Safe Reinforcement Learning Using Black-Box Reachability Analysis",
        "text": "Reinforcement learning (RL) is capable of sophisticated motion planning and control for robots in uncertain environments. However, state-of-the-art deep RL approaches typically lack safety guarantees, especially when the robot and environment models are un"
    },
    {
        "title": "Safe Reinforcement Learning Using Black-Box Reachability Analysis",
        "text": "known. To justify widespread deployment, robots must respect safety constraints without sacrificing performance. Thus, we propose a Black-box Reachability-based Safety Layer (BRSL) with three main components: (1) data-driven reachability analysis for a bla"
    },
    {
        "title": "Safe Reinforcement Learning Using Black-Box Reachability Analysis",
        "text": "ck-box robot model, (2) a trajectory rollout planner that predicts future actions and observations using an ensemble of neural networks trained online, and (3) a differentiable polytope collision check between the reachable set and obstacles that enables c"
    },
    {
        "title": "Safe Reinforcement Learning Using Black-Box Reachability Analysis",
        "text": "orrecting unsafe actions. In simulation, BRSL outperforms other state-of-the-art safe RL methods on a Turtlebot 3, a quadrotor, and a trajectory-tracking point mass with an unsafe set adjacent to the area of highest reward."
    },
    {
        "title": "Deep Transfer Network with Joint Distribution Adaptation: A New   Intelligent Fault Diagnosis Framework for Industry Application",
        "text": "In recent years, an increasing popularity of deep learning model for intelligent condition monitoring and diagnosis as well as prognostics used for mechanical systems and structures has been observed. In the previous studies, however, a major assumption ac"
    },
    {
        "title": "Deep Transfer Network with Joint Distribution Adaptation: A New   Intelligent Fault Diagnosis Framework for Industry Application",
        "text": "cepted by default, is that the training and testing data are taking from same feature distribution. Unfortunately, this assumption is mostly invalid in real application, resulting in a certain lack of applicability for the traditional diagnosis approaches."
    },
    {
        "title": "Deep Transfer Network with Joint Distribution Adaptation: A New   Intelligent Fault Diagnosis Framework for Industry Application",
        "text": " Inspired by the idea of transfer learning that leverages the knowledge learnt from rich labeled data in source domain to facilitate diagnosing a new but similar target task, a new intelligent fault diagnosis framework, i.e., deep transfer network (DTN), w"
    },
    {
        "title": "Deep Transfer Network with Joint Distribution Adaptation: A New   Intelligent Fault Diagnosis Framework for Industry Application",
        "text": "hich generalizes deep learning model to domain adaptation scenario, is proposed in this paper. By extending the marginal distribution adaptation (MDA) to joint distribution adaptation (JDA), the proposed framework can exploit the discrimination structures "
    },
    {
        "title": "Deep Transfer Network with Joint Distribution Adaptation: A New   Intelligent Fault Diagnosis Framework for Industry Application",
        "text": "associated with the labeled data in source domain to adapt the conditional distribution of unlabeled target data, and thus guarantee a more accurate distribution matching. Extensive empirical evaluations on three fault datasets validate the applicability a"
    },
    {
        "title": "Deep Transfer Network with Joint Distribution Adaptation: A New   Intelligent Fault Diagnosis Framework for Industry Application",
        "text": "nd practicability of DTN, while achieving many state-of-the-art transfer results in terms of diverse operating conditions, fault severities and fault types."
    },
    {
        "title": "FedNILM: Applying Federated Learning to NILM Applications at the Edge",
        "text": "Non-intrusive load monitoring (NILM) helps disaggregate the household's main electricity consumption to energy usages of individual appliances, thus greatly cutting down the cost in fine-grained household load monitoring. To address the arisen privacy conc"
    },
    {
        "title": "FedNILM: Applying Federated Learning to NILM Applications at the Edge",
        "text": "ern in NILM applications, federated learning (FL) could be leveraged for NILM model training and sharing. When applying the FL paradigm in real-world NILM applications, however, we are faced with the challenges of edge resource restriction, edge model pers"
    },
    {
        "title": "FedNILM: Applying Federated Learning to NILM Applications at the Edge",
        "text": "onalization and edge training data scarcity.   In this paper we present FedNILM, a practical FL paradigm for NILM applications at the edge client. Specifically, FedNILM is designed to deliver privacy-preserving and personalized NILM services to large-scale"
    },
    {
        "title": "FedNILM: Applying Federated Learning to NILM Applications at the Edge",
        "text": " edge clients, by leveraging i) secure data aggregation through federated learning, ii) efficient cloud model compression via filter pruning and multi-task learning, and iii) personalized edge model building with unsupervised transfer learning. Our experim"
    },
    {
        "title": "FedNILM: Applying Federated Learning to NILM Applications at the Edge",
        "text": "ents on real-world energy data show that, FedNILM is able to achieve personalized energy disaggregation with the state-of-the-art accuracy, while ensuring privacy preserving at the edge client."
    },
    {
        "title": "Transfer Learning for Autonomous Chatter Detection in Machining",
        "text": "Large-amplitude chatter vibrations are one of the most important phenomena in machining processes. It is often detrimental in cutting operations causing a poor surface finish and decreased tool life. Therefore, chatter detection using machine learning has "
    },
    {
        "title": "Transfer Learning for Autonomous Chatter Detection in Machining",
        "text": "been an active research area over the last decade. Three challenges can be identified in applying machine learning for chatter detection at large in industry: an insufficient understanding of the universality of chatter features across different processes,"
    },
    {
        "title": "Transfer Learning for Autonomous Chatter Detection in Machining",
        "text": " the need for automating feature extraction, and the existence of limited data for each specific workpiece-machine tool combination. These three challenges can be grouped under the umbrella of transfer learning. This paper studies automating chatter detect"
    },
    {
        "title": "Transfer Learning for Autonomous Chatter Detection in Machining",
        "text": "ion by evaluating transfer learning of prominent as well as novel chatter detection methods. We investigate chatter classification accuracy using a variety of features extracted from turning and milling experiments with different cutting configurations. Th"
    },
    {
        "title": "Transfer Learning for Autonomous Chatter Detection in Machining",
        "text": "e studied methods include Fast Fourier Transform (FFT), Power Spectral Density (PSD), the Auto-correlation Function (ACF), Wavelet Packet Transform (WPT), and Ensemble Empirical Mode Decomposition (EEMD). We also examine more recent approaches based on Top"
    },
    {
        "title": "Transfer Learning for Autonomous Chatter Detection in Machining",
        "text": "ological Data Analysis (TDA) and similarity measures of time series based on Discrete Time Warping (DTW). We evaluate the transfer learning potential of each approach by training and testing both within and across the turning and milling data sets. Our res"
    },
    {
        "title": "Transfer Learning for Autonomous Chatter Detection in Machining",
        "text": "ults show that carefully chosen time-frequency features can lead to high classification accuracies albeit at the cost of requiring manual pre-processing and the tagging of an expert user. On the other hand, we found that the TDA and DTW approaches can prov"
    },
    {
        "title": "Transfer Learning for Autonomous Chatter Detection in Machining",
        "text": "ide accuracies and F1 scores on par with the time-frequency methods without the need for manual preprocessing."
    },
    {
        "title": "Quadruply Stochastic Gaussian Processes",
        "text": "We introduce a stochastic variational inference procedure for training scalable Gaussian process (GP) models whose per-iteration complexity is independent of both the number of training points, $n$, and the number basis functions used in the kernel approxi"
    },
    {
        "title": "Quadruply Stochastic Gaussian Processes",
        "text": "mation, $m$. Our central contributions include an unbiased stochastic estimator of the evidence lower bound (ELBO) for a Gaussian likelihood, as well as a stochastic estimator that lower bounds the ELBO for several other likelihoods such as Laplace and log"
    },
    {
        "title": "Quadruply Stochastic Gaussian Processes",
        "text": "istic. Independence of the stochastic optimization update complexity on $n$ and $m$ enables inference on huge datasets using large capacity GP models. We demonstrate accurate inference on large classification and regression datasets using GPs and relevance"
    },
    {
        "title": "Quadruply Stochastic Gaussian Processes",
        "text": " vector machines with up to $m = 10^7$ basis functions."
    },
    {
        "title": "Bugs in Machine Learning-based Systems: A Faultload Benchmark",
        "text": "The rapid escalation of applying Machine Learning (ML) in various domains has led to paying more attention to the quality of ML components. There is then a growth of techniques and tools aiming at improving the quality of ML components and integrating them"
    },
    {
        "title": "Bugs in Machine Learning-based Systems: A Faultload Benchmark",
        "text": " into the ML-based system safely. Although most of these tools use bugs' lifecycle, there is no standard benchmark of bugs to assess their performance, compare them and discuss their advantages and weaknesses. In this study, we firstly investigate the repr"
    },
    {
        "title": "Bugs in Machine Learning-based Systems: A Faultload Benchmark",
        "text": "oducibility and verifiability of the bugs in ML-based systems and show the most important factors in each one. Then, we explore the challenges of generating a benchmark of bugs in ML-based software systems and provide a bug benchmark namely defect4ML that "
    },
    {
        "title": "Bugs in Machine Learning-based Systems: A Faultload Benchmark",
        "text": "satisfies all criteria of standard benchmark, i.e. relevance, reproducibility, fairness, verifiability, and usability. This faultload benchmark contains 113 bugs reported by ML developers on GitHub and Stack Overflow, using two of the most popular ML frame"
    },
    {
        "title": "Bugs in Machine Learning-based Systems: A Faultload Benchmark",
        "text": "works: TensorFlow and Keras. defect4ML also addresses important challenges in Software Reliability Engineering of ML-based software systems, like: 1) fast changes in frameworks, by providing various bugs for different versions of frameworks, 2) code portab"
    },
    {
        "title": "Bugs in Machine Learning-based Systems: A Faultload Benchmark",
        "text": "ility, by delivering similar bugs in different ML frameworks, 3) bug reproducibility, by providing fully reproducible bugs with complete information about required dependencies and data, and 4) lack of detailed information on bugs, by presenting links to t"
    },
    {
        "title": "Bugs in Machine Learning-based Systems: A Faultload Benchmark",
        "text": "he bugs' origins. defect4ML can be of interest to ML-based systems practitioners and researchers to assess their testing tools and techniques."
    },
    {
        "title": "Glioma Grade Prediction Using Wavelet Scattering-Based Radiomics",
        "text": "Glioma grading before surgery is very critical for the prognosis prediction and treatment plan making. We present a novel wavelet scattering-based radiomic method to predict noninvasively and accurately the glioma grades. The method consists of wavelet sca"
    },
    {
        "title": "Glioma Grade Prediction Using Wavelet Scattering-Based Radiomics",
        "text": "ttering feature extraction, dimensionality reduction, and glioma grade prediction. The dimensionality reduction was achieved using partial least squares (PLS) regression and the glioma grade prediction using support vector machine (SVM), logistic regressio"
    },
    {
        "title": "Glioma Grade Prediction Using Wavelet Scattering-Based Radiomics",
        "text": "n (LR) and random forest (RF). The prediction obtained on multimodal magnetic resonance images of 285 patients with well-labeled intratumoral and peritumoral regions showed that the area under the receiver operating characteristic curve (AUC) of glioma gra"
    },
    {
        "title": "Glioma Grade Prediction Using Wavelet Scattering-Based Radiomics",
        "text": "de prediction was increased up to 0.99 when considering both intratumoral and peritumoral features in multimodal images, which represents an increase of about 13% compared to traditional radiomics. In addition, the features extracted from peritumoral regio"
    },
    {
        "title": "Glioma Grade Prediction Using Wavelet Scattering-Based Radiomics",
        "text": "ns further increase the accuracy of glioma grading."
    },
    {
        "title": "High correlated variables creator machine: Prediction of the compressive   strength of concrete",
        "text": "In this paper, we introduce a novel hybrid model for predicting the compressive strength of concrete using ultrasonic pulse velocity (UPV) and rebound number (RN). First, 516 data from 8 studies of UPV and rebound hammer (RH) tests was collected. Then, hig"
    },
    {
        "title": "High correlated variables creator machine: Prediction of the compressive   strength of concrete",
        "text": "h correlated variables creator machine (HVCM) is used to create the new variables that have a better correlation with the output and improve the prediction models. Three single models, including a step-by-step regression (SBSR), gene expression programming"
    },
    {
        "title": "High correlated variables creator machine: Prediction of the compressive   strength of concrete",
        "text": " (GEP) and an adaptive neuro-fuzzy inference system (ANFIS) as well as three hybrid models, i.e. HCVCM-SBSR, HCVCM-GEP and HCVCM-ANFIS, were employed to predict the compressive strength of concrete. The statistical parameters and error terms such as coeffi"
    },
    {
        "title": "High correlated variables creator machine: Prediction of the compressive   strength of concrete",
        "text": "cient of determination, root mean square error (RMSE), normalized mean square error (NMSE), fractional bias, the maximum positive and negative errors, and mean absolute percentage error (MAPE), were computed to evaluate and compare the models. The results "
    },
    {
        "title": "High correlated variables creator machine: Prediction of the compressive   strength of concrete",
        "text": "show that HCVCM-ANFIS can predict the compressive strength of concrete better than all other models. HCVCM improves the accuracy of ANFIS by 5% in the coefficient of determination, 10% in RMSE, 3% in NMSE, 20% in MAPE, and 7% in the maximum negative error."
    },
    {
        "title": "mSHINE: A Multiple-meta-paths Simultaneous Learning Framework for   Heterogeneous Information Network Embedding",
        "text": "Heterogeneous information networks(HINs) become popular in recent years for its strong capability of modelling objects with abundant information using explicit network structure. Network embedding has been proved as an effective method to convert informati"
    },
    {
        "title": "mSHINE: A Multiple-meta-paths Simultaneous Learning Framework for   Heterogeneous Information Network Embedding",
        "text": "on networks into lower-dimensional space, whereas the core information can be well preserved. However, traditional network embedding algorithms are sub-optimal in capturing rich while potentially incompatible semantics provided by HINs. To address this iss"
    },
    {
        "title": "mSHINE: A Multiple-meta-paths Simultaneous Learning Framework for   Heterogeneous Information Network Embedding",
        "text": "ue, a novel meta-path-based HIN representation learning framework named mSHINE is designed to simultaneously learn multiple node representations for different meta-paths. More specifically, one representation learning module inspired by the RNN structure i"
    },
    {
        "title": "mSHINE: A Multiple-meta-paths Simultaneous Learning Framework for   Heterogeneous Information Network Embedding",
        "text": "s developed and multiple node representations can be learned simultaneously, where each representation is associated with one respective meta-path. By measuring the relevance between nodes with the designed objective function, the learned module can be app"
    },
    {
        "title": "mSHINE: A Multiple-meta-paths Simultaneous Learning Framework for   Heterogeneous Information Network Embedding",
        "text": "lied in downstream link prediction tasks. A set of criteria for selecting initial meta-paths is proposed as the other module in mSHINE which is important to reduce the optimal meta-path selection cost when no prior knowledge of suitable meta-paths is avail"
    },
    {
        "title": "mSHINE: A Multiple-meta-paths Simultaneous Learning Framework for   Heterogeneous Information Network Embedding",
        "text": "able. To corroborate the effectiveness of mSHINE, extensive experimental studies including node classification and link prediction are conducted on five real-world datasets. The results demonstrate that mSHINE outperforms other state-of-the-art HIN embeddi"
    },
    {
        "title": "mSHINE: A Multiple-meta-paths Simultaneous Learning Framework for   Heterogeneous Information Network Embedding",
        "text": "ng methods."
    },
    {
        "title": "UST: Unifying Spatio-Temporal Context for Trajectory Prediction in   Autonomous Driving",
        "text": "Trajectory prediction has always been a challenging problem for autonomous driving, since it needs to infer the latent intention from the behaviors and interactions from traffic participants. This problem is intrinsically hard, because each participant may"
    },
    {
        "title": "UST: Unifying Spatio-Temporal Context for Trajectory Prediction in   Autonomous Driving",
        "text": " behave differently under different environments and interactions. This key is to effectively model the interlaced influence from both spatial context and temporal context. Existing work usually encodes these two types of context separately, which would le"
    },
    {
        "title": "UST: Unifying Spatio-Temporal Context for Trajectory Prediction in   Autonomous Driving",
        "text": "ad to inferior modeling of the scenarios. In this paper, we first propose a unified approach to treat time and space dimensions equally for modeling spatio-temporal context. The proposed module is simple and easy to implement within several lines of codes."
    },
    {
        "title": "UST: Unifying Spatio-Temporal Context for Trajectory Prediction in   Autonomous Driving",
        "text": " In contrast to existing methods which heavily rely on recurrent neural network for temporal context and hand-crafted structure for spatial context, our method could automatically partition the spatio-temporal space to adapt the data. Lastly, we test our p"
    },
    {
        "title": "UST: Unifying Spatio-Temporal Context for Trajectory Prediction in   Autonomous Driving",
        "text": "roposed framework on two recently proposed trajectory prediction dataset ApolloScape and Argoverse. We show that the proposed method substantially outperforms the previous state-of-the-art methods while maintaining its simplicity. These encouraging results"
    },
    {
        "title": "UST: Unifying Spatio-Temporal Context for Trajectory Prediction in   Autonomous Driving",
        "text": " further validate the superiority of our approach."
    },
    {
        "title": "Mitigating Unwanted Biases with Adversarial Learning",
        "text": "Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating"
    },
    {
        "title": "Mitigating Unwanted Biases with Adversarial Learning",
        "text": " such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while t"
    },
    {
        "title": "Mitigating Unwanted Biases with Adversarial Learning",
        "text": "he adversary tries to model a protected variable Z, here gender or zip code.   The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in ac"
    },
    {
        "title": "Mitigating Unwanted Biases with Adversarial Learning",
        "text": "curate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (H"
    },
    {
        "title": "Mitigating Unwanted Biases with Adversarial Learning",
        "text": "ardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks."
    },
    {
        "title": "Limiting Network Size within Finite Bounds for Optimization",
        "text": "Largest theoretical contribution to Neural Networks comes from VC Dimension which characterizes the sample complexity of classification model in a probabilistic view and are widely used to study the generalization error. So far in the literature the VC Dim"
    },
    {
        "title": "Limiting Network Size within Finite Bounds for Optimization",
        "text": "ension has only been used to approximate the generalization error bounds on different Neural Network architectures. VC Dimension has not yet been implicitly or explicitly stated to fix the network size which is important as the wrong configuration could le"
    },
    {
        "title": "Limiting Network Size within Finite Bounds for Optimization",
        "text": "ad to high computation effort in training and leads to over fitting. So there is a need to bound these units so that task can be computed with only sufficient number of parameters. For binary classification tasks shallow networks are used as they have univ"
    },
    {
        "title": "Limiting Network Size within Finite Bounds for Optimization",
        "text": "ersal approximation property and it is enough to size the hidden layer width for such networks. The paper brings out a theoretical justification on required attribute size and its corresponding hidden layer dimension for a given sample set that gives an op"
    },
    {
        "title": "Limiting Network Size within Finite Bounds for Optimization",
        "text": "timal binary classification results with minimum training complexity in a single layered feed forward network framework. The paper also establishes proof on the existence of bounds on the width of the hidden layer and its range subjected to certain conditi"
    },
    {
        "title": "Limiting Network Size within Finite Bounds for Optimization",
        "text": "ons. Findings in this paper are experimentally analyzed on three different dataset using Mathlab 2018 (b) software."
    },
    {
        "title": "Real-time Drift Detection on Time-series Data",
        "text": "Practical machine learning applications involving time series data, such as firewall log analysis to proactively detect anomalous behavior, are concerned with real time analysis of streaming data. Consequently, we need to update the ML models as the statis"
    },
    {
        "title": "Real-time Drift Detection on Time-series Data",
        "text": "tical characteristics of such data may shift frequently with time. One alternative explored in the literature is to retrain models with updated data whenever the models accuracy is observed to degrade. However, these methods rely on near real time availabi"
    },
    {
        "title": "Real-time Drift Detection on Time-series Data",
        "text": "lity of ground truth, which is rarely fulfilled. Further, in applications with seasonal data, temporal concept drift is confounded by seasonal variation. In this work, we propose an approach called Unsupervised Temporal Drift Detector or UTDD to flexibly a"
    },
    {
        "title": "Real-time Drift Detection on Time-series Data",
        "text": "ccount for seasonal variation, efficiently detect temporal concept drift in time series data in the absence of ground truth, and subsequently adapt our ML models to concept drift for better generalization."
    },
    {
        "title": "IFTT-PIN: Demonstrating the Self-Calibration Paradigm on a PIN-Entry   Task",
        "text": "We demonstrate IFTT-PIN, a self-calibrating version of the PIN-entry method introduced in Roth et al. (2004) [1]. In [1], digits are split into two sets and assigned a color respectively. To communicate their digit, users press the button with the same col"
    },
    {
        "title": "IFTT-PIN: Demonstrating the Self-Calibration Paradigm on a PIN-Entry   Task",
        "text": "or that is assigned to their digit, which can be identified by elimination after a few iterations. IFTT-PIN uses the same principle but does not pre-assign colors to each button. Instead, users are free to choose which button to use for each color. IFTT-PI"
    },
    {
        "title": "IFTT-PIN: Demonstrating the Self-Calibration Paradigm on a PIN-Entry   Task",
        "text": "N infers both the user's PIN and their preferred button-to-color mapping at the same time, a process called self-calibration. Different versions of IFTT-PIN can be tested at https://jgrizou.github.io/IFTT-PIN/ and a video introduction at https://youtu.be/5"
    },
    {
        "title": "IFTT-PIN: Demonstrating the Self-Calibration Paradigm on a PIN-Entry   Task",
        "text": "I1ibPJdLHM."
    },
    {
        "title": "DeepBrain: Towards Personalized EEG Interaction through Attentional and   Embedded LSTM Learning",
        "text": "The \"mind-controlling\" capability has always been in mankind's fantasy. With the recent advancements of electroencephalograph (EEG) techniques, brain-computer interface (BCI) researchers have explored various solutions to allow individuals to perform vario"
    },
    {
        "title": "DeepBrain: Towards Personalized EEG Interaction through Attentional and   Embedded LSTM Learning",
        "text": "us tasks using their minds. However, the commercial off-the-shelf devices to run accurate EGG signal collection are usually expensive and the comparably cheaper devices can only present coarse results, which prevents the practical application of these devi"
    },
    {
        "title": "DeepBrain: Towards Personalized EEG Interaction through Attentional and   Embedded LSTM Learning",
        "text": "ces in domestic services. To tackle this challenge, we propose and develop an end-to-end solution that enables fine brain-robot interaction (BRI) through embedded learning of coarse EEG signals from the low-cost devices, namely DeepBrain, so that people ha"
    },
    {
        "title": "DeepBrain: Towards Personalized EEG Interaction through Attentional and   Embedded LSTM Learning",
        "text": "ving difficulty to move, such as the elderly, can mildly command and control a robot to perform some basic household tasks. Our contributions are two folds: 1) We present a stacked long short term memory (Stacked LSTM) structure with specific pre-processin"
    },
    {
        "title": "DeepBrain: Towards Personalized EEG Interaction through Attentional and   Embedded LSTM Learning",
        "text": "g techniques to handle the time-dependency of EEG signals and their classification. 2) We propose personalized design to capture multiple features and achieve accurate recognition of individual EEG signals by enhancing the signal interpretation of Stacked "
    },
    {
        "title": "DeepBrain: Towards Personalized EEG Interaction through Attentional and   Embedded LSTM Learning",
        "text": "LSTM with attention mechanism. Our real-world experiments demonstrate that the proposed end-to-end solution with low cost can achieve satisfactory run-time speed, accuracy and energy-efficiency."
    },
    {
        "title": "On the Properties of the Softmax Function with Application in Game   Theory and Reinforcement Learning",
        "text": "In this paper, we utilize results from convex analysis and monotone operator theory to derive additional properties of the softmax function that have not yet been covered in the existing literature. In particular, we show that the softmax function is the m"
    },
    {
        "title": "On the Properties of the Softmax Function with Application in Game   Theory and Reinforcement Learning",
        "text": "onotone gradient map of the log-sum-exp function. By exploiting this connection, we show that the inverse temperature parameter determines the Lipschitz and co-coercivity properties of the softmax function. We then demonstrate the usefulness of these prope"
    },
    {
        "title": "On the Properties of the Softmax Function with Application in Game   Theory and Reinforcement Learning",
        "text": "rties through an application in game-theoretic reinforcement learning."
    },
    {
        "title": "Understanding Regularized Spectral Clustering via Graph Conductance",
        "text": "This paper uses the relationship between graph conductance and spectral clustering to study (i) the failures of spectral clustering and (ii) the benefits of regularization. The explanation is simple. Sparse and stochastic graphs create a lot of small trees"
    },
    {
        "title": "Understanding Regularized Spectral Clustering via Graph Conductance",
        "text": " that are connected to the core of the graph by only one edge. Graph conductance is sensitive to these noisy `dangling sets'. Spectral clustering inherits this sensitivity. The second part of the paper starts from a previously proposed form of regularized "
    },
    {
        "title": "Understanding Regularized Spectral Clustering via Graph Conductance",
        "text": "spectral clustering and shows that it is related to the graph conductance on a `regularized graph'. We call the conductance on the regularized graph CoreCut. Based upon previous arguments that relate graph conductance to spectral clustering (e.g. Cheeger i"
    },
    {
        "title": "Understanding Regularized Spectral Clustering via Graph Conductance",
        "text": "nequality), minimizing CoreCut relaxes to regularized spectral clustering. Simple inspection of CoreCut reveals why it is less sensitive to small cuts in the graph. Together, these results show that unbalanced partitions from spectral clustering can be und"
    },
    {
        "title": "Understanding Regularized Spectral Clustering via Graph Conductance",
        "text": "erstood as overfitting to noise in the periphery of a sparse and stochastic graph. Regularization fixes this overfitting. In addition to this statistical benefit, these results also demonstrate how regularization can improve the computational speed of spec"
    },
    {
        "title": "Understanding Regularized Spectral Clustering via Graph Conductance",
        "text": "tral clustering. We provide simulations and data examples to illustrate these results."
    },
    {
        "title": "Context models on sequences of covers",
        "text": "We present a class of models that, via a simple construction, enables exact, incremental, non-parametric, polynomial-time, Bayesian inference of conditional measures. The approach relies upon creating a sequence of covers on the conditioning variable and m"
    },
    {
        "title": "Context models on sequences of covers",
        "text": "aintaining a different model for each set within a cover. Inference remains tractable by specifying the probabilistic model in terms of a random walk within the sequence of covers. We demonstrate the approach on problems of conditional density estimation, "
    },
    {
        "title": "Context models on sequences of covers",
        "text": "which, to our knowledge is the first closed-form, non-parametric Bayesian approach to this problem."
    },
    {
        "title": "Texture Characterization of Histopathologic Images Using Ecological   Diversity Measures and Discrete Wavelet Transform",
        "text": "Breast cancer is a health problem that affects mainly the female population. An early detection increases the chances of effective treatment, improving the prognosis of the disease. In this regard, computational tools have been proposed to assist the speci"
    },
    {
        "title": "Texture Characterization of Histopathologic Images Using Ecological   Diversity Measures and Discrete Wavelet Transform",
        "text": "alist in interpreting the breast digital image exam, providing features for detecting and diagnosing tumors and cancerous cells. Nonetheless, detecting tumors with a high sensitivity rate and reducing the false positives rate is still challenging. Texture "
    },
    {
        "title": "Texture Characterization of Histopathologic Images Using Ecological   Diversity Measures and Discrete Wavelet Transform",
        "text": "descriptors have been quite popular in medical image analysis, particularly in histopathologic images (HI), due to the variability of both the texture found in such images and the tissue appearance due to irregularity in the staining process. Such variabil"
    },
    {
        "title": "Texture Characterization of Histopathologic Images Using Ecological   Diversity Measures and Discrete Wavelet Transform",
        "text": "ity may exist depending on differences in staining protocol such as fixation, inconsistency in the staining condition, and reagents, either between laboratories or in the same laboratory. Textural feature extraction for quantifying HI information in a disc"
    },
    {
        "title": "Texture Characterization of Histopathologic Images Using Ecological   Diversity Measures and Discrete Wavelet Transform",
        "text": "riminant way is challenging given the distribution of intrinsic properties of such images forms a non-deterministic complex system. This paper proposes a method for characterizing texture across HIs with a considerable success rate. By employing ecological"
    },
    {
        "title": "Texture Characterization of Histopathologic Images Using Ecological   Diversity Measures and Discrete Wavelet Transform",
        "text": " diversity measures and discrete wavelet transform, it is possible to quantify the intrinsic properties of such images with promising accuracy on two HI datasets compared with state-of-the-art methods."
    },
    {
        "title": "Optimal Time-Series Motifs",
        "text": "Motifs are the most repetitive/frequent patterns of a time-series. The discovery of motifs is crucial for practitioners in order to understand and interpret the phenomena occurring in sequential data. Currently, motifs are searched among series sub-sequenc"
    },
    {
        "title": "Optimal Time-Series Motifs",
        "text": "es, aiming at selecting the most frequently occurring ones. Search-based methods, which try out series sub-sequence as motif candidates, are currently believed to be the best methods in finding the most frequent patterns.   However, this paper proposes an "
    },
    {
        "title": "Optimal Time-Series Motifs",
        "text": "entirely new perspective in finding motifs. We demonstrate that searching is non-optimal since the domain of motifs is restricted, and instead we propose a principled optimization approach able to find optimal motifs. We treat the occurrence frequency as a"
    },
    {
        "title": "Optimal Time-Series Motifs",
        "text": " function and time-series motifs as its parameters, therefore we \\textit{learn} the optimal motifs that maximize the frequency function. In contrast to searching, our method is able to discover the most repetitive patterns (hence optimal), even in cases wh"
    },
    {
        "title": "Optimal Time-Series Motifs",
        "text": "ere they do not explicitly occur as sub-sequences. Experiments on several real-life time-series datasets show that the motifs found by our method are highly more frequent than the ones found through searching, for exactly the same distance threshold."
    },
    {
        "title": "Neural Model-based Optimization with Right-Censored Observations",
        "text": "In many fields of study, we only observe lower bounds on the true response value of some experiments. When fitting a regression model to predict the distribution of the outcomes, we cannot simply drop these right-censored observations, but need to properly"
    },
    {
        "title": "Neural Model-based Optimization with Right-Censored Observations",
        "text": " model them. In this work, we focus on the concept of censored data in the light of model-based optimization where prematurely terminating evaluations (and thus generating right-censored data) is a key factor for efficiency, e.g., when searching for an alg"
    },
    {
        "title": "Neural Model-based Optimization with Right-Censored Observations",
        "text": "orithm configuration that minimizes runtime of the algorithm at hand. Neural networks (NNs) have been demonstrated to work well at the core of model-based optimization procedures and here we extend them to handle these censored observations. We propose (i)"
    },
    {
        "title": "Neural Model-based Optimization with Right-Censored Observations",
        "text": "~a loss function based on the Tobit model to incorporate censored samples into training and (ii) use an ensemble of networks to model the posterior distribution. To nevertheless be efficient in terms of optimization-overhead, we propose to use Thompson sam"
    },
    {
        "title": "Neural Model-based Optimization with Right-Censored Observations",
        "text": "pling s.t. we only need to train a single NN in each iteration. Our experiments show that our trained regression models achieve a better predictive quality than several baselines and that our approach achieves new state-of-the-art performance for model-bas"
    },
    {
        "title": "Neural Model-based Optimization with Right-Censored Observations",
        "text": "ed optimization on two optimization problems: minimizing the solution time of a SAT solver and the time-to-accuracy of neural networks."
    },
    {
        "title": "Reinforced Curriculum Learning on Pre-trained Neural Machine Translation   Models",
        "text": "The competitive performance of neural machine translation (NMT) critically relies on large amounts of training data. However, acquiring high-quality translation pairs requires expert knowledge and is costly. Therefore, how to best utilize a given dataset o"
    },
    {
        "title": "Reinforced Curriculum Learning on Pre-trained Neural Machine Translation   Models",
        "text": "f samples with diverse quality and characteristics becomes an important yet understudied question in NMT. Curriculum learning methods have been introduced to NMT to optimize a model's performance by prescribing the data input order, based on heuristics suc"
    },
    {
        "title": "Reinforced Curriculum Learning on Pre-trained Neural Machine Translation   Models",
        "text": "h as the assessment of noise and difficulty levels. However, existing methods require training from scratch, while in practice most NMT models are pre-trained on big data already. Moreover, as heuristics, they do not generalize well. In this paper, we aim "
    },
    {
        "title": "Reinforced Curriculum Learning on Pre-trained Neural Machine Translation   Models",
        "text": "to learn a curriculum for improving a pre-trained NMT model by re-selecting influential data samples from the original training set and formulate this task as a reinforcement learning problem. Specifically, we propose a data selection framework based on De"
    },
    {
        "title": "Reinforced Curriculum Learning on Pre-trained Neural Machine Translation   Models",
        "text": "terministic Actor-Critic, in which a critic network predicts the expected change of model performance due to a certain sample, while an actor network learns to select the best sample out of a random batch of samples presented to it. Experiments on several "
    },
    {
        "title": "Reinforced Curriculum Learning on Pre-trained Neural Machine Translation   Models",
        "text": "translation datasets show that our method can further improve the performance of NMT when original batch training reaches its ceiling, without using additional new training data, and significantly outperforms several strong baseline methods."
    },
    {
        "title": "Predicting Temporal Sets with Deep Neural Networks",
        "text": "Given a sequence of sets, where each set contains an arbitrary number of elements, the problem of temporal sets prediction aims to predict the elements in the subsequent set. In practice, temporal sets prediction is much more complex than predictive modell"
    },
    {
        "title": "Predicting Temporal Sets with Deep Neural Networks",
        "text": "ing of temporal events and time series, and is still an open problem. Many possible existing methods, if adapted for the problem of temporal sets prediction, usually follow a two-step strategy by first projecting temporal sets into latent representations a"
    },
    {
        "title": "Predicting Temporal Sets with Deep Neural Networks",
        "text": "nd then learning a predictive model with the latent representations. The two-step approach often leads to information loss and unsatisfactory prediction performance. In this paper, we propose an integrated solution based on the deep neural networks for tem"
    },
    {
        "title": "Predicting Temporal Sets with Deep Neural Networks",
        "text": "poral sets prediction. A unique perspective of our approach is to learn element relationship by constructing set-level co-occurrence graph and then perform graph convolutions on the dynamic relationship graphs. Moreover, we design an attention-based module"
    },
    {
        "title": "Predicting Temporal Sets with Deep Neural Networks",
        "text": " to adaptively learn the temporal dependency of elements and sets. Finally, we provide a gated updating mechanism to find the hidden shared patterns in different sequences and fuse both static and dynamic information to improve the prediction performance. "
    },
    {
        "title": "Predicting Temporal Sets with Deep Neural Networks",
        "text": "Experiments on real-world data sets demonstrate that our approach can achieve competitive performances even with a portion of the training data and can outperform existing methods with a significant margin."
    },
    {
        "title": "Challenging common bolus advisor for self-monitoring type-I diabetes   patients using Reinforcement Learning",
        "text": "Patients with diabetes who are self-monitoring have to decide right before each meal how much insulin they should take. A standard bolus advisor exists, but has never actually been proven to be optimal in any sense. We challenged this rule applying Reinfor"
    },
    {
        "title": "Challenging common bolus advisor for self-monitoring type-I diabetes   patients using Reinforcement Learning",
        "text": "cement Learning techniques on data simulated with T1DM, an FDA-approved simulator developed by Kovatchev et al. modeling the gluco-insulin interaction. Results show that the optimal bolus rule is fairly different from the standard bolus advisor, and if fol"
    },
    {
        "title": "Challenging common bolus advisor for self-monitoring type-I diabetes   patients using Reinforcement Learning",
        "text": "lowed can actually avoid hypoglycemia episodes."
    },
    {
        "title": "Analyzing the Travel and Charging Behavior of Electric Vehicles -- A   Data-driven Approach",
        "text": "The increasing market penetration of electric vehicles (EVs) may pose significant electricity demand on power systems. This electricity demand is affected by the inherent uncertainties of EVs' travel behavior that makes forecasting the daily charging deman"
    },
    {
        "title": "Analyzing the Travel and Charging Behavior of Electric Vehicles -- A   Data-driven Approach",
        "text": "d (CD) very challenging. In this project, we use the National House Hold Survey (NHTS) data to form sequences of trips, and develop machine learning models to predict the parameters of the next trip of the drivers, including trip start time, end time, and "
    },
    {
        "title": "Analyzing the Travel and Charging Behavior of Electric Vehicles -- A   Data-driven Approach",
        "text": "distance. These parameters are later used to model the temporal charging behavior of EVs. The simulation results show that the proposed modeling can effectively estimate the daily CD pattern based on travel behavior of EVs, and simple machine learning tech"
    },
    {
        "title": "Analyzing the Travel and Charging Behavior of Electric Vehicles -- A   Data-driven Approach",
        "text": "niques can forecast the travel parameters with acceptable accuracy."
    },
    {
        "title": "A Deep-Bayesian Framework for Adaptive Speech Duration Modification",
        "text": "We propose the first method to adaptively modify the duration of a given speech signal. Our approach uses a Bayesian framework to define a latent attention map that links frames of the input and target utterances. We train a masked convolutional encoder-de"
    },
    {
        "title": "A Deep-Bayesian Framework for Adaptive Speech Duration Modification",
        "text": "coder network to produce this attention map via a stochastic version of the mean absolute error loss function; our model also predicts the length of the target speech signal using the encoder embeddings. The predicted length determines the number of steps "
    },
    {
        "title": "A Deep-Bayesian Framework for Adaptive Speech Duration Modification",
        "text": "for the decoder operation. During inference, we generate the attention map as a proxy for the similarity matrix between the given input speech and an unknown target speech signal. Using this similarity matrix, we compute a warping path of alignment between"
    },
    {
        "title": "A Deep-Bayesian Framework for Adaptive Speech Duration Modification",
        "text": " the two signals. Our experiments demonstrate that this adaptive framework produces similar results to dynamic time warping, which relies on a known target signal, on both voice conversion and emotion conversion tasks. We also show that our technique resul"
    },
    {
        "title": "A Deep-Bayesian Framework for Adaptive Speech Duration Modification",
        "text": "ts in a high quality of generated speech that is on par with state-of-the-art vocoders."
    },
    {
        "title": "Dynamic Environment Prediction in Urban Scenes using Recurrent   Representation Learning",
        "text": "A key challenge for autonomous driving is safe trajectory planning in cluttered, urban environments with dynamic obstacles, such as pedestrians, bicyclists, and other vehicles. A reliable prediction of the future environment, including the behavior of dyna"
    },
    {
        "title": "Dynamic Environment Prediction in Urban Scenes using Recurrent   Representation Learning",
        "text": "mic agents, would allow planning algorithms to proactively generate a trajectory in response to a rapidly changing environment. We present a novel framework that predicts the future occupancy state of the local environment surrounding an autonomous agent b"
    },
    {
        "title": "Dynamic Environment Prediction in Urban Scenes using Recurrent   Representation Learning",
        "text": "y learning a motion model from occupancy grid data using a neural network. We take advantage of the temporal structure of the grid data by utilizing a convolutional long-short term memory network in the form of the PredNet architecture. This method is vali"
    },
    {
        "title": "Dynamic Environment Prediction in Urban Scenes using Recurrent   Representation Learning",
        "text": "dated on the KITTI dataset and demonstrates higher accuracy and better predictive power than baseline methods."
    },
    {
        "title": "Self-Driving Car Steering Angle Prediction Based on Image Recognition",
        "text": "Self-driving vehicles have expanded dramatically over the last few years. Udacity has release a dataset containing, among other data, a set of images with the steering angle captured during driving. The Udacity challenge aimed to predict steering angle bas"
    },
    {
        "title": "Self-Driving Car Steering Angle Prediction Based on Image Recognition",
        "text": "ed on only the provided images. We explore two different models to perform high quality prediction of steering angles based on images using different deep learning techniques including Transfer Learning, 3D CNN, LSTM and ResNet. If the Udacity challenge wa"
    },
    {
        "title": "Self-Driving Car Steering Angle Prediction Based on Image Recognition",
        "text": "s still ongoing, both of our models would have placed in the top ten of all entries."
    },
    {
        "title": "HERO: Hierarchical Encoder for Video+Language Omni-representation   Pre-training",
        "text": "We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion,"
    },
    {
        "title": "HERO: Hierarchical Encoder for Video+Language Omni-representation   Pre-training",
        "text": " and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model pre"
    },
    {
        "title": "HERO: Hierarchical Encoder for Video+Language Omni-representation   Pre-training",
        "text": "dicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex s"
    },
    {
        "title": "HERO: Hierarchical Encoder for Video+Language Omni-representation   Pre-training",
        "text": "ocial dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference"
    },
    {
        "title": "HERO: Hierarchical Encoder for Video+Language Omni-representation   Pre-training",
        "text": " and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities."
    },
    {
        "title": "Deep Hierarchical Machine: a Flexible Divide-and-Conquer Architecture",
        "text": "We propose Deep Hierarchical Machine (DHM), a model inspired from the divide-and-conquer strategy while emphasizing representation learning ability and flexibility. A stochastic routing framework as used by recent deep neural decision/regression forests is"
    },
    {
        "title": "Deep Hierarchical Machine: a Flexible Divide-and-Conquer Architecture",
        "text": " incorporated, but we remove the need to evaluate unnecessary computation paths by utilizing a different topology and introducing a probabilistic pruning technique. We also show a specified version of DHM (DSHM) for efficiency, which inherits the sparse fe"
    },
    {
        "title": "Deep Hierarchical Machine: a Flexible Divide-and-Conquer Architecture",
        "text": "ature extraction process as in traditional decision tree with pixel-difference feature. To achieve sparse feature extraction, we propose to utilize sparse convolution operation in DSHM and show one possibility of introducing sparse convolution kernels by u"
    },
    {
        "title": "Deep Hierarchical Machine: a Flexible Divide-and-Conquer Architecture",
        "text": "sing local binary convolution layer. DHM can be applied to both classification and regression problems, and we validate it on standard image classification and face alignment tasks to show its advantages over past architectures."
    },
    {
        "title": "Hierarchic Kernel Recursive Least-Squares",
        "text": "We present a new kernel-based algorithm for modeling evenly distributed multidimensional datasets that does not rely on input space sparsification. The presented method reorganizes the typical single-layer kernel-based model into a deep hierarchical struct"
    },
    {
        "title": "Hierarchic Kernel Recursive Least-Squares",
        "text": "ure, such that the weights of a kernel model over each dimension are modeled over its adjacent dimension. We show that modeling weights in the suggested structure leads to significant computational speedup and improved modeling accuracy."
    },
    {
        "title": "Optimally fuzzy temporal memory",
        "text": "Any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past. If the signal has a characteristic timescale relevant to future prediction, the memory can be a simple shift register---a movi"
    },
    {
        "title": "Optimally fuzzy temporal memory",
        "text": "ng window extending into the past, requiring storage resources that linearly grows with the timescale to be represented. However, an independent general purpose learner cannot a priori know the characteristic prediction-relevant timescale of the signal. Mo"
    },
    {
        "title": "Optimally fuzzy temporal memory",
        "text": "reover, many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded. Hence the learner should maintain information from the longest possible timescale allowed by "
    },
    {
        "title": "Optimally fuzzy temporal memory",
        "text": "resource availability. Here we construct a fuzzy memory system that optimally sacrifices the temporal accuracy of information in a scale-free fashion in order to represent prediction-relevant information from exponentially long timescales. Using several il"
    },
    {
        "title": "Optimally fuzzy temporal memory",
        "text": "lustrative examples, we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals. When the available storage resources are limited, we suggest that a general purpose learner would be better of"
    },
    {
        "title": "Optimally fuzzy temporal memory",
        "text": "f committing to such a fuzzy memory system."
    },
    {
        "title": "Implicit Density Estimation by Local Moment Matching to Sample from   Auto-Encoders",
        "text": "Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of the unknown data generating density. This paper contributes to the mathematical understanding of this phenomenon and helps define better justifi"
    },
    {
        "title": "Implicit Density Estimation by Local Moment Matching to Sample from   Auto-Encoders",
        "text": "ed sampling algorithms for deep learning based on auto-encoder variants. We consider an MCMC where each step samples from a Gaussian whose mean and covariance matrix depend on the previous state, defines through its asymptotic distribution a target density"
    },
    {
        "title": "Implicit Density Estimation by Local Moment Matching to Sample from   Auto-Encoders",
        "text": ". First, we show that good choices (in the sense of consistency) for these mean and covariance functions are the local expected value and local covariance under that target density. Then we show that an auto-encoder with a contractive penalty captures esti"
    },
    {
        "title": "Implicit Density Estimation by Local Moment Matching to Sample from   Auto-Encoders",
        "text": "mators of these local moments in its reconstruction function and its Jacobian. A contribution of this work is thus a novel alternative to maximum-likelihood density estimation, which we call local moment matching. It also justifies a recently proposed samp"
    },
    {
        "title": "Implicit Density Estimation by Local Moment Matching to Sample from   Auto-Encoders",
        "text": "ling algorithm for the Contractive Auto-Encoder and extends it to the Denoising Auto-Encoder."
    },
    {
        "title": "Neural eliminators and classifiers",
        "text": "Classification may not be reliable for several reasons: noise in the data, insufficient input information, overlapping distributions and sharp definition of classes. Faced with several possibilities neural network may in such cases still be useful if inste"
    },
    {
        "title": "Neural eliminators and classifiers",
        "text": "ad of a classification elimination of improbable classes is done. Eliminators may be constructed using classifiers assigning new cases to a pool of several classes instead of just one winning class. Elimination may be done with the help of several classifi"
    },
    {
        "title": "Neural eliminators and classifiers",
        "text": "ers using modified error functions. A real life medical application of neural network is presented illustrating the usefulness of elimination."
    },
    {
        "title": "Appliance Level Short-term Load Forecasting via Recurrent Neural Network",
        "text": "Accurate load forecasting is critical for electricity market operations and other real-time decision-making tasks in power systems. This paper considers the short-term load forecasting (STLF) problem for residential customers within a community. Existing S"
    },
    {
        "title": "Appliance Level Short-term Load Forecasting via Recurrent Neural Network",
        "text": "TLF work mainly focuses on forecasting the aggregated load for either a feeder system or a single customer, but few efforts have been made on forecasting the load at individual appliance level. In this work, we present an STLF algorithm for efficiently pre"
    },
    {
        "title": "Appliance Level Short-term Load Forecasting via Recurrent Neural Network",
        "text": "dicting the power consumption of individual electrical appliances. The proposed method builds upon a powerful recurrent neural network (RNN) architecture in deep learning, termed as long short-term memory (LSTM). As each appliance has uniquely repetitive c"
    },
    {
        "title": "Appliance Level Short-term Load Forecasting via Recurrent Neural Network",
        "text": "onsumption patterns, the patterns of prediction error will be tracked such that past prediction errors can be used for improving the final prediction performance. Numerical tests on real-world load datasets demonstrate the improvement of the proposed metho"
    },
    {
        "title": "Appliance Level Short-term Load Forecasting via Recurrent Neural Network",
        "text": "d over existing LSTM-based method and other benchmark approaches."
    },
    {
        "title": "Private Learning and Sanitization: Pure vs. Approximate Differential   Privacy",
        "text": "We compare the sample complexity of private learning [Kasiviswanathan et al. 2008] and sanitization~[Blum et al. 2008] under pure $\\epsilon$-differential privacy [Dwork et al. TCC 2006] and approximate $(\\epsilon,\\delta)$-differential privacy [Dwork et al."
    },
    {
        "title": "Private Learning and Sanitization: Pure vs. Approximate Differential   Privacy",
        "text": " Eurocrypt 2006]. We show that the sample complexity of these tasks under approximate differential privacy can be significantly lower than that under pure differential privacy.   We define a family of optimization problems, which we call Quasi-Concave Prom"
    },
    {
        "title": "Private Learning and Sanitization: Pure vs. Approximate Differential   Privacy",
        "text": "ise Problems, that generalizes some of our considered tasks. We observe that a quasi-concave promise problem can be privately approximated using a solution to a smaller instance of a quasi-concave promise problem. This allows us to construct an efficient r"
    },
    {
        "title": "Private Learning and Sanitization: Pure vs. Approximate Differential   Privacy",
        "text": "ecursive algorithm solving such problems privately. Specifically, we construct private learners for point functions, threshold functions, and axis-aligned rectangles in high dimension. Similarly, we construct sanitizers for point functions and threshold fu"
    },
    {
        "title": "Private Learning and Sanitization: Pure vs. Approximate Differential   Privacy",
        "text": "nctions.   We also examine the sample complexity of label-private learners, a relaxation of private learning where the learner is required to only protect the privacy of the labels in the sample. We show that the VC dimension completely characterizes the s"
    },
    {
        "title": "Private Learning and Sanitization: Pure vs. Approximate Differential   Privacy",
        "text": "ample complexity of such learners, that is, the sample complexity of learning with label privacy is equal (up to constants) to learning without privacy."
    },
    {
        "title": "A Bayesian Evaluation Framework for Subjectively Annotated Visual   Recognition Tasks",
        "text": "An interesting development in automatic visual recognition has been the emergence of tasks where it is not possible to assign objective labels to images, yet still feasible to collect annotations that reflect human judgements about them. Machine learning-b"
    },
    {
        "title": "A Bayesian Evaluation Framework for Subjectively Annotated Visual   Recognition Tasks",
        "text": "ased predictors for these tasks rely on supervised training that models the behavior of the annotators, i.e., what would the average person's judgement be for an image? A key open question for this type of work, especially for applications where inconsiste"
    },
    {
        "title": "A Bayesian Evaluation Framework for Subjectively Annotated Visual   Recognition Tasks",
        "text": "ncy with human behavior can lead to ethical lapses, is how to evaluate the epistemic uncertainty of trained predictors, i.e., the uncertainty that comes from the predictor's model. We propose a Bayesian framework for evaluating black box predictors in this"
    },
    {
        "title": "A Bayesian Evaluation Framework for Subjectively Annotated Visual   Recognition Tasks",
        "text": " regime, agnostic to the predictor's internal structure. The framework specifies how to estimate the epistemic uncertainty that comes from the predictor with respect to human labels by approximating a conditional distribution and producing a credible inter"
    },
    {
        "title": "A Bayesian Evaluation Framework for Subjectively Annotated Visual   Recognition Tasks",
        "text": "val for the predictions and their measures of performance. The framework is successfully applied to four image classification tasks that use subjective human judgements: facial beauty assessment, social attribute assignment, apparent age estimation, and am"
    },
    {
        "title": "A Bayesian Evaluation Framework for Subjectively Annotated Visual   Recognition Tasks",
        "text": "biguous scene labeling."
    },
    {
        "title": "Balancing New Against Old Information: The Role of Surprise in Learning",
        "text": "Surprise describes a range of phenomena from unexpected events to behavioral responses. We propose a measure of surprise and use it for surprise-driven learning. Our surprise measure takes into account data likelihood as well as the degree of commitment to"
    },
    {
        "title": "Balancing New Against Old Information: The Role of Surprise in Learning",
        "text": " a belief via the entropy of the belief distribution. We find that surprise-minimizing learning dynamically adjusts the balance between new and old information without the need of knowledge about the temporal statistics of the environment. We apply our fra"
    },
    {
        "title": "Balancing New Against Old Information: The Role of Surprise in Learning",
        "text": "mework to a dynamic decision-making task and a maze exploration task. Our surprise minimizing framework is suitable for learning in complex environments, even if the environment undergoes gradual or sudden changes and could eventually provide a framework t"
    },
    {
        "title": "Balancing New Against Old Information: The Role of Surprise in Learning",
        "text": "o study the behavior of humans and animals encountering surprising events."
    },
    {
        "title": "Classification of COVID-19 in Chest CT Images using Convolutional   Support Vector Machines",
        "text": "Purpose: Coronavirus 2019 (COVID-19), which emerged in Wuhan, China and affected the whole world, has cost the lives of thousands of people. Manual diagnosis is inefficient due to the rapid spread of this virus. For this reason, automatic COVID-19 detectio"
    },
    {
        "title": "Classification of COVID-19 in Chest CT Images using Convolutional   Support Vector Machines",
        "text": "n studies are carried out with the support of artificial intelligence algorithms. Methods: In this study, a deep learning model that detects COVID-19 cases with high performance is presented. The proposed method is defined as Convolutional Support Vector M"
    },
    {
        "title": "Classification of COVID-19 in Chest CT Images using Convolutional   Support Vector Machines",
        "text": "achine (CSVM) and can automatically classify Computed Tomography (CT) images. Unlike the pre-trained Convolutional Neural Networks (CNN) trained with the transfer learning method, the CSVM model is trained as a scratch. To evaluate the performance of the C"
    },
    {
        "title": "Classification of COVID-19 in Chest CT Images using Convolutional   Support Vector Machines",
        "text": "SVM method, the dataset is divided into two parts as training (%75) and testing (%25). The CSVM model consists of blocks containing three different numbers of SVM kernels. Results: When the performance of pre-trained CNN networks and CSVM models is assesse"
    },
    {
        "title": "Classification of COVID-19 in Chest CT Images using Convolutional   Support Vector Machines",
        "text": "d, CSVM (7x7, 3x3, 1x1) model shows the highest performance with 94.03% ACC, 96.09% SEN, 92.01% SPE, 92.19% PRE, 94.10% F1-Score, 88.15% MCC and 88.07% Kappa metric values. Conclusion: The proposed method is more effective than other methods. It has proven"
    },
    {
        "title": "Classification of COVID-19 in Chest CT Images using Convolutional   Support Vector Machines",
        "text": " in experiments performed to be an inspiration for combating COVID and for future studies."
    },
    {
        "title": "Distal Explanations for Model-free Explainable Reinforcement Learning",
        "text": "In this paper we introduce and evaluate a distal explanation model for model-free reinforcement learning agents that can generate explanations for `why' and `why not' questions. Our starting point is the observation that causal models can generate opportun"
    },
    {
        "title": "Distal Explanations for Model-free Explainable Reinforcement Learning",
        "text": "ity chains that take the form of `A enables B and B causes C'. Using insights from an analysis of 240 explanations generated in a human-agent experiment, we define a distal explanation model that can analyse counterfactuals and opportunity chains using dec"
    },
    {
        "title": "Distal Explanations for Model-free Explainable Reinforcement Learning",
        "text": "ision trees and causal models. A recurrent neural network is employed to learn opportunity chains, and decision trees are used to improve the accuracy of task prediction and the generated counterfactuals. We computationally evaluate the model in 6 reinforc"
    },
    {
        "title": "Distal Explanations for Model-free Explainable Reinforcement Learning",
        "text": "ement learning benchmarks using different reinforcement learning algorithms. From a study with 90 human participants, we show that our distal explanation model results in improved outcomes over three scenarios compared with two baseline explanation models."
    },
    {
        "title": "Missing Data Imputation using Optimal Transport",
        "text": "Missing data is a crucial issue when applying machine learning algorithms to real-world datasets. Starting from the simple assumption that two batches extracted randomly from the same dataset should share the same distribution, we leverage optimal transpor"
    },
    {
        "title": "Missing Data Imputation using Optimal Transport",
        "text": "t distances to quantify that criterion and turn it into a loss function to impute missing data values. We propose practical methods to minimize these losses using end-to-end learning, that can exploit or not parametric assumptions on the underlying distrib"
    },
    {
        "title": "Missing Data Imputation using Optimal Transport",
        "text": "utions of values. We evaluate our methods on datasets from the UCI repository, in MCAR, MAR and MNAR settings. These experiments show that OT-based methods match or out-perform state-of-the-art imputation methods, even for high percentages of missing value"
    },
    {
        "title": "Missing Data Imputation using Optimal Transport",
        "text": "s."
    },
    {
        "title": "Reconstruct Anomaly to Normal: Adversarial Learned and Latent   Vector-constrained Autoencoder for Time-series Anomaly Detection",
        "text": "Anomaly detection in time series has been widely researched and has important practical applications. In recent years, anomaly detection algorithms are mostly based on deep-learning generative models and use the reconstruction error to detect anomalies. Th"
    },
    {
        "title": "Reconstruct Anomaly to Normal: Adversarial Learned and Latent   Vector-constrained Autoencoder for Time-series Anomaly Detection",
        "text": "ey try to capture the distribution of normal data by reconstructing normal data in the training phase, then calculate the reconstruction error of test data to do anomaly detection. However, most of them only use the normal data in the training phase and ca"
    },
    {
        "title": "Reconstruct Anomaly to Normal: Adversarial Learned and Latent   Vector-constrained Autoencoder for Time-series Anomaly Detection",
        "text": "n not ensure the reconstruction process of anomaly data. So, anomaly data can also be well reconstructed sometimes and gets low reconstruction error, which leads to the omission of anomalies. What's more, the neighbor information of data points in time ser"
    },
    {
        "title": "Reconstruct Anomaly to Normal: Adversarial Learned and Latent   Vector-constrained Autoencoder for Time-series Anomaly Detection",
        "text": "ies data has not been fully utilized in these algorithms. In this paper, we propose RAN based on the idea of Reconstruct Anomalies to Normal and apply it for unsupervised time series anomaly detection. To minimize the reconstruction error of normal data an"
    },
    {
        "title": "Reconstruct Anomaly to Normal: Adversarial Learned and Latent   Vector-constrained Autoencoder for Time-series Anomaly Detection",
        "text": "d maximize this of anomaly data, we do not just ensure normal data to reconstruct well, but also try to make the reconstruction of anomaly data consistent with the distribution of normal data, then anomalies will get higher reconstruction errors. We implem"
    },
    {
        "title": "Reconstruct Anomaly to Normal: Adversarial Learned and Latent   Vector-constrained Autoencoder for Time-series Anomaly Detection",
        "text": "ent this idea by introducing the \"imitated anomaly data\" and combining a specially designed latent vector-constrained Autoencoder with the discriminator to construct an adversary network. Extensive experiments on time-series datasets from different scenes "
    },
    {
        "title": "Reconstruct Anomaly to Normal: Adversarial Learned and Latent   Vector-constrained Autoencoder for Time-series Anomaly Detection",
        "text": "such as ECG diagnosis also show that RAN can detect meaningful anomalies, and it outperforms other algorithms in terms of AUC-ROC."
    },
    {
        "title": "An advanced spatio-temporal convolutional recurrent neural network for   storm surge predictions",
        "text": "In this research paper, we study the capability of artificial neural network models to emulate storm surge based on the storm track/size/intensity history, leveraging a database of synthetic storm simulations. Traditionally, Computational Fluid Dynamics so"
    },
    {
        "title": "An advanced spatio-temporal convolutional recurrent neural network for   storm surge predictions",
        "text": "lvers are employed to numerically solve the storm surge governing equations that are Partial Differential Equations and are generally very costly to simulate. This study presents a neural network model that can predict storm surge, informed by a database o"
    },
    {
        "title": "An advanced spatio-temporal convolutional recurrent neural network for   storm surge predictions",
        "text": "f synthetic storm simulations. This model can serve as a fast and affordable emulator for the very expensive CFD solvers. The neural network model is trained with the storm track parameters used to drive the CFD solvers, and the output of the model is the "
    },
    {
        "title": "An advanced spatio-temporal convolutional recurrent neural network for   storm surge predictions",
        "text": "time-series evolution of the predicted storm surge across multiple nodes within the spatial domain of interest. Once the model is trained, it can be deployed for further predictions based on new storm track inputs. The developed neural network model is a t"
    },
    {
        "title": "An advanced spatio-temporal convolutional recurrent neural network for   storm surge predictions",
        "text": "ime-series model, a Long short-term memory, a variation of Recurrent Neural Network, which is enriched with Convolutional Neural Networks. The convolutional neural network is employed to capture the correlation of data spatially. Therefore, the temporal an"
    },
    {
        "title": "An advanced spatio-temporal convolutional recurrent neural network for   storm surge predictions",
        "text": "d spatial correlations of data are captured by the combination of the mentioned models, the ConvLSTM model. As the problem is a sequence to sequence time-series problem, an encoder-decoder ConvLSTM model is designed. Some other techniques in the process of"
    },
    {
        "title": "An advanced spatio-temporal convolutional recurrent neural network for   storm surge predictions",
        "text": " model training are also employed to enrich the model performance. The results show the proposed convolutional recurrent neural network outperforms the Gaussian Process implementation for the examined synthetic storm database."
    },
    {
        "title": "Dirichlet Mixture Model based VQ Performance Prediction for Line   Spectral Frequency",
        "text": "In this paper, we continue our previous work on the Dirichlet mixture model (DMM)-based VQ to derive the performance bound of the LSF VQ. The LSF parameters are transformed into the $\\Delta$LSF domain and the underlying distribution of the $\\Delta$LSF para"
    },
    {
        "title": "Dirichlet Mixture Model based VQ Performance Prediction for Line   Spectral Frequency",
        "text": "meters are modelled by a DMM with finite number of mixture components. The quantization distortion, in terms of the mean squared error (MSE), is calculated with the high rate theory. The mapping relation between the perceptually motivated log spectral dist"
    },
    {
        "title": "Dirichlet Mixture Model based VQ Performance Prediction for Line   Spectral Frequency",
        "text": "ortion (LSD) and the MSE is empirically approximated by a polynomial. With this mapping function, the minimum required bit rate for transparent coding of the LSF is estimated."
    },
    {
        "title": "Federated Learning with Noisy User Feedback",
        "text": "Machine Learning (ML) systems are getting increasingly popular, and drive more and more applications and services in our daily life. This has led to growing concerns over user privacy, since human interaction data typically needs to be transmitted to the c"
    },
    {
        "title": "Federated Learning with Noisy User Feedback",
        "text": "loud in order to train and improve such systems. Federated learning (FL) has recently emerged as a method for training ML models on edge devices using sensitive user data and is seen as a way to mitigate concerns over data privacy. However, since ML models"
    },
    {
        "title": "Federated Learning with Noisy User Feedback",
        "text": " are most commonly trained with label supervision, we need a way to extract labels on edge to make FL viable. In this work, we propose a strategy for training FL models using positive and negative user feedback. We also design a novel framework to study di"
    },
    {
        "title": "Federated Learning with Noisy User Feedback",
        "text": "fferent noise patterns in user feedback, and explore how well standard noise-robust objectives can help mitigate this noise when training models in a federated setting. We evaluate our proposed training setup through detailed experiments on two text classi"
    },
    {
        "title": "Federated Learning with Noisy User Feedback",
        "text": "fication datasets and analyze the effects of varying levels of user reliability and feedback noise on model performance. We show that our method improves substantially over a self-training baseline, achieving performance closer to models trained with full "
    },
    {
        "title": "Federated Learning with Noisy User Feedback",
        "text": "supervision."
    },
    {
        "title": "Fisher Task Distance and Its Application in Neural Architecture Search",
        "text": "We formulate an asymmetric (or non-commutative) distance between tasks based on Fisher Information Matrices, called Fisher task distance. This distance represents the complexity of transferring the knowledge from one task to another. We provide a proof of "
    },
    {
        "title": "Fisher Task Distance and Its Application in Neural Architecture Search",
        "text": "consistency for our distance through theorems and experiments on various classification tasks from MNIST, CIFAR-10, CIFAR-100, ImageNet, and Taskonomy datasets. Next, we construct an online neural architecture search framework using the Fisher task distanc"
    },
    {
        "title": "Fisher Task Distance and Its Application in Neural Architecture Search",
        "text": "e, in which we have access to the past learned tasks. By using the Fisher task distance, we can identify the closest learned tasks to the target task, and utilize the knowledge learned from these related tasks for the target task. Here, we show how the pro"
    },
    {
        "title": "Fisher Task Distance and Its Application in Neural Architecture Search",
        "text": "posed distance between a target task and a set of learned tasks can be used to reduce the neural architecture search space for the target task. The complexity reduction in search space for task-specific architectures is achieved by building on the optimize"
    },
    {
        "title": "Fisher Task Distance and Its Application in Neural Architecture Search",
        "text": "d architectures for similar tasks instead of doing a full search and without using this side information. Experimental results for tasks in MNIST, CIFAR-10, CIFAR-100, ImageNet datasets demonstrate the efficacy of the proposed approach and its improvements"
    },
    {
        "title": "Fisher Task Distance and Its Application in Neural Architecture Search",
        "text": ", in terms of the performance and the number of parameters, over other gradient-based search methods, such as ENAS, DARTS, PC-DARTS."
    },
    {
        "title": "Explaining The Efficacy of Counterfactually Augmented Data",
        "text": "In attempts to produce ML models less reliant on spurious patterns in NLP datasets, researchers have recently proposed curating counterfactually augmented data (CAD) via a human-in-the-loop process in which given some documents and their (initial) labels, "
    },
    {
        "title": "Explaining The Efficacy of Counterfactually Augmented Data",
        "text": "humans must revise the text to make a counterfactual label applicable. Importantly, edits that are not necessary to flip the applicable label are prohibited. Models trained on the augmented data appear, empirically, to rely less on semantically irrelevant "
    },
    {
        "title": "Explaining The Efficacy of Counterfactually Augmented Data",
        "text": "words and to generalize better out of domain. While this work draws loosely on causal thinking, the underlying causal model (even at an abstract level) and the principles underlying the observed out-of-domain improvements remain unclear. In this paper, we "
    },
    {
        "title": "Explaining The Efficacy of Counterfactually Augmented Data",
        "text": "introduce a toy analog based on linear Gaussian models, observing interesting relationships between causal models, measurement noise, out-of-domain generalization, and reliance on spurious signals. Our analysis provides some insights that help to explain t"
    },
    {
        "title": "Explaining The Efficacy of Counterfactually Augmented Data",
        "text": "he efficacy of CAD. Moreover, we develop the hypothesis that while adding noise to causal features should degrade both in-domain and out-of-domain performance, adding noise to non-causal features should lead to relative improvements in out-of-domain perfor"
    },
    {
        "title": "Explaining The Efficacy of Counterfactually Augmented Data",
        "text": "mance. This idea inspires a speculative test for determining whether a feature attribution technique has identified the causal spans. If adding noise (e.g., by random word flips) to the highlighted spans degrades both in-domain and out-of-domain performanc"
    },
    {
        "title": "Explaining The Efficacy of Counterfactually Augmented Data",
        "text": "e on a battery of challenge datasets, but adding noise to the complement gives improvements out-of-domain, it suggests we have identified causal spans. We present a large-scale empirical study comparing spans edited to create CAD to those selected by atten"
    },
    {
        "title": "Explaining The Efficacy of Counterfactually Augmented Data",
        "text": "tion and saliency maps. Across numerous domains and models, we find that the hypothesized phenomenon is pronounced for CAD."
    },
    {
        "title": "Bioplastic Design using Multitask Deep Neural Networks",
        "text": "Non-degradable plastic waste stays for decades on land and in water, jeopardizing our environment; yet our modern lifestyle and current technologies are impossible to sustain without plastics. Bio-synthesized and biodegradable alternatives such as the poly"
    },
    {
        "title": "Bioplastic Design using Multitask Deep Neural Networks",
        "text": "mer family of polyhydroxyalkanoates (PHAs) have the potential to replace large portions of the world's plastic supply with cradle-to-cradle materials, but their chemical complexity and diversity limit traditional resource-intensive experimentation. In this"
    },
    {
        "title": "Bioplastic Design using Multitask Deep Neural Networks",
        "text": " work, we develop multitask deep neural network property predictors using available experimental data for a diverse set of nearly 23000 homo- and copolymer chemistries. Using the predictors, we identify 14 PHA-based bioplastics from a search space of almos"
    },
    {
        "title": "Bioplastic Design using Multitask Deep Neural Networks",
        "text": "t 1.4 million candidates which could serve as potential replacements for seven petroleum-based commodity plastics that account for 75% of the world's yearly plastic production. We discuss possible synthesis routes for these identified promising materials. "
    },
    {
        "title": "Bioplastic Design using Multitask Deep Neural Networks",
        "text": "The developed multitask polymer property predictors are made available as a part of the Polymer Genome project at https://PolymerGenome.org."
    },
    {
        "title": "Combinatorial Pure Exploration with Full-bandit Feedback and Beyond:   Solving Combinatorial Optimization under Uncertainty with Limited Observation",
        "text": "Combinatorial optimization is one of the fundamental research fields that has been extensively studied in theoretical computer science and operations research. When developing an algorithm for combinatorial optimization, it is commonly assumed that paramet"
    },
    {
        "title": "Combinatorial Pure Exploration with Full-bandit Feedback and Beyond:   Solving Combinatorial Optimization under Uncertainty with Limited Observation",
        "text": "ers such as edge weights are exactly known as inputs. However, this assumption may not be fulfilled since input parameters are often uncertain or initially unknown in many applications such as recommender systems, crowdsourcing, communication networks, and"
    },
    {
        "title": "Combinatorial Pure Exploration with Full-bandit Feedback and Beyond:   Solving Combinatorial Optimization under Uncertainty with Limited Observation",
        "text": " online advertisement. To resolve such uncertainty, the problem of combinatorial pure exploration of multi-armed bandits (CPE) and its variants have recieved increasing attention. Earlier work on CPE has studied the semi-bandit feedback or assumed that the"
    },
    {
        "title": "Combinatorial Pure Exploration with Full-bandit Feedback and Beyond:   Solving Combinatorial Optimization under Uncertainty with Limited Observation",
        "text": " outcome from each individual edge is always accessible at all rounds. However, due to practical constraints such as a budget ceiling or privacy concern, such strong feedback is not always available in recent applications. In this article, we review recent"
    },
    {
        "title": "Combinatorial Pure Exploration with Full-bandit Feedback and Beyond:   Solving Combinatorial Optimization under Uncertainty with Limited Observation",
        "text": "ly proposed techniques for combinatorial pure exploration problems with limited feedback."
    },
    {
        "title": "CodedReduce: A Fast and Robust Framework for Gradient Aggregation in   Distributed Learning",
        "text": "We focus on the commonly used synchronous Gradient Descent paradigm for large-scale distributed learning, for which there has been a growing interest to develop efficient and robust gradient aggregation strategies that overcome two key system bottlenecks: "
    },
    {
        "title": "CodedReduce: A Fast and Robust Framework for Gradient Aggregation in   Distributed Learning",
        "text": "communication bandwidth and stragglers' delays. In particular, Ring-AllReduce (RAR) design has been proposed to avoid bandwidth bottleneck at any particular node by allowing each worker to only communicate with its neighbors that are arranged in a logical "
    },
    {
        "title": "CodedReduce: A Fast and Robust Framework for Gradient Aggregation in   Distributed Learning",
        "text": "ring. On the other hand, Gradient Coding (GC) has been recently proposed to mitigate stragglers in a master-worker topology by allowing carefully designed redundant allocation of the data set to the workers. We propose a joint communication topology design"
    },
    {
        "title": "CodedReduce: A Fast and Robust Framework for Gradient Aggregation in   Distributed Learning",
        "text": " and data set allocation strategy, named CodedReduce (CR), that combines the best of both RAR and GC. That is, it parallelizes the communications over a tree topology leading to efficient bandwidth utilization, and carefully designs a redundant data set al"
    },
    {
        "title": "CodedReduce: A Fast and Robust Framework for Gradient Aggregation in   Distributed Learning",
        "text": "location and coding strategy at the nodes to make the proposed gradient aggregation scheme robust to stragglers. In particular, we quantify the communication parallelization gain and resiliency of the proposed CR scheme, and prove its optimality when the c"
    },
    {
        "title": "CodedReduce: A Fast and Robust Framework for Gradient Aggregation in   Distributed Learning",
        "text": "ommunication topology is a regular tree. Moreover, we characterize the expected run-time of CR and show order-wise speedups compared to the benchmark schemes. Finally, we empirically evaluate the performance of our proposed CR design over Amazon EC2 and de"
    },
    {
        "title": "CodedReduce: A Fast and Robust Framework for Gradient Aggregation in   Distributed Learning",
        "text": "monstrate that it achieves speedups of up to 27.2x and 7.0x, respectively over the benchmarks GC and RAR."
    },
    {
        "title": "A Family of Pairwise Multi-Marginal Optimal Transports that Define a   Generalized Metric",
        "text": "The Optimal transport (OT) problem is rapidly finding its way into machine learning. Favoring its use are its metric properties. Many problems admit solutions with guarantees only for objects embedded in metric spaces, and the use of non-metrics can compli"
    },
    {
        "title": "A Family of Pairwise Multi-Marginal Optimal Transports that Define a   Generalized Metric",
        "text": "cate solving them. Multi-marginal OT (MMOT) generalizes OT to simultaneously transporting multiple distributions. It captures important relations that are missed if the transport only involves two distributions. Research on MMOT, however, has been focused "
    },
    {
        "title": "A Family of Pairwise Multi-Marginal Optimal Transports that Define a   Generalized Metric",
        "text": "on its existence, uniqueness, practical algorithms, and the choice of cost functions. There is a lack of discussion on the metric properties of MMOT, which limits its theoretical and practical use. Here, we prove new generalized metric properties for a new"
    },
    {
        "title": "A Family of Pairwise Multi-Marginal Optimal Transports that Define a   Generalized Metric",
        "text": " family of MMOTs. We first explain the difficulty of proving this via two negative results. Afterward, we prove the MMOTs' metric properties. Finally, we show that the generalized triangle inequality of this family of MMOTs cannot be improved. We illustrat"
    },
    {
        "title": "A Family of Pairwise Multi-Marginal Optimal Transports that Define a   Generalized Metric",
        "text": "e the superiority of our MMOTs over other generalized metrics, and over non-metrics in both synthetic and real tasks."
    },
    {
        "title": "Dynamic Radar Network of UAVs: A Joint Navigation and Tracking Approach",
        "text": "Nowadays there is a growing research interest on the possibility of enriching small flying robots with autonomous sensing and online navigation capabilities. This will enable a large number of applications spanning from remote surveillance to logistics, sm"
    },
    {
        "title": "Dynamic Radar Network of UAVs: A Joint Navigation and Tracking Approach",
        "text": "arter cities and emergency aid in hazardous environments. In this context, an emerging problem is to track unauthorized small unmanned aerial vehicles (UAVs) hiding behind buildings or concealing in large UAV networks. In contrast with current solutions ma"
    },
    {
        "title": "Dynamic Radar Network of UAVs: A Joint Navigation and Tracking Approach",
        "text": "inly based on static and on-ground radars, this paper proposes the idea of a dynamic radar network of UAVs for real-time and high-accuracy tracking of malicious targets. To this end, we describe a solution for real-time navigation of UAVs to track a dynami"
    },
    {
        "title": "Dynamic Radar Network of UAVs: A Joint Navigation and Tracking Approach",
        "text": "c target using heterogeneously sensed information. Such information is shared by the UAVs with their neighbors via multi-hops, allowing tracking the target by a local Bayesian estimator running at each agent. Since not all the paths are equal in terms of i"
    },
    {
        "title": "Dynamic Radar Network of UAVs: A Joint Navigation and Tracking Approach",
        "text": "nformation gathering point-of-view, the UAVs plan their own trajectory by minimizing the posterior covariance matrix of the target state under UAV kinematic and anti-collision constraints. Our results show how a dynamic network of radars attains better loc"
    },
    {
        "title": "Dynamic Radar Network of UAVs: A Joint Navigation and Tracking Approach",
        "text": "alization results compared to a fixed configuration and how the on-board sensor technology impacts the accuracy in tracking a target with different radar cross sections, especially in non line-of-sight (NLOS) situations."
    },
    {
        "title": "Z-GCNETs: Time Zigzags at Graph Convolutional Networks for Time Series   Forecasting",
        "text": "There recently has been a surge of interest in developing a new class of deep learning (DL) architectures that integrate an explicit time dimension as a fundamental building block of learning and representation mechanisms. In turn, many recent results show"
    },
    {
        "title": "Z-GCNETs: Time Zigzags at Graph Convolutional Networks for Time Series   Forecasting",
        "text": " that topological descriptors of the observed data, encoding information on the shape of the dataset in a topological space at different scales, that is, persistent homology of the data, may contain important complementary information, improving both perfo"
    },
    {
        "title": "Z-GCNETs: Time Zigzags at Graph Convolutional Networks for Time Series   Forecasting",
        "text": "rmance and robustness of DL. As convergence of these two emerging ideas, we propose to enhance DL architectures with the most salient time-conditioned topological information of the data and introduce the concept of zigzag persistence into time-aware graph"
    },
    {
        "title": "Z-GCNETs: Time Zigzags at Graph Convolutional Networks for Time Series   Forecasting",
        "text": " convolutional networks (GCNs). Zigzag persistence provides a systematic and mathematically rigorous framework to track the most important topological features of the observed data that tend to manifest themselves over time. To integrate the extracted time"
    },
    {
        "title": "Z-GCNETs: Time Zigzags at Graph Convolutional Networks for Time Series   Forecasting",
        "text": "-conditioned topological descriptors into DL, we develop a new topological summary, zigzag persistence image, and derive its theoretical stability guarantees. We validate the new GCNs with a time-aware zigzag topological layer (Z-GCNETs), in application to"
    },
    {
        "title": "Z-GCNETs: Time Zigzags at Graph Convolutional Networks for Time Series   Forecasting",
        "text": " traffic forecasting and Ethereum blockchain price prediction. Our results indicate that Z-GCNET outperforms 13 state-of-the-art methods on 4 time series datasets."
    },
    {
        "title": "Between-Domain Instance Transition Via the Process of Gibbs Sampling in   RBM",
        "text": "In this paper, we present a new idea for Transfer Learning (TL) based on Gibbs Sampling. Gibbs sampling is an algorithm in which instances are likely to transfer to a new state with a higher possibility with respect to a probability distribution. We find t"
    },
    {
        "title": "Between-Domain Instance Transition Via the Process of Gibbs Sampling in   RBM",
        "text": "hat such an algorithm can be employed to transfer instances between domains. Restricted Boltzmann Machine (RBM) is an energy based model that is very feasible for being trained to represent a data distribution and also for performing Gibbs sampling. We use"
    },
    {
        "title": "Between-Domain Instance Transition Via the Process of Gibbs Sampling in   RBM",
        "text": "d RBM to capture data distribution of the source domain and use it in order to cast target instances into new data with a distribution similar to the distribution of source data. Using datasets that are commonly used for evaluation of TL methods, we show t"
    },
    {
        "title": "Between-Domain Instance Transition Via the Process of Gibbs Sampling in   RBM",
        "text": "hat our method can successfully enhance target classification by a considerable ratio. Additionally, the proposed method has the advantage over common DA methods that it needs no target data during the process of training of models."
    },
    {
        "title": "Approximating the Permanent by Sampling from Adaptive Partitions",
        "text": "Computing the permanent of a non-negative matrix is a core problem with practical applications ranging from target tracking to statistical thermodynamics. However, this problem is also #P-complete, which leaves little hope for finding an exact solution tha"
    },
    {
        "title": "Approximating the Permanent by Sampling from Adaptive Partitions",
        "text": "t can be computed efficiently. While the problem admits a fully polynomial randomized approximation scheme, this method has seen little use because it is both inefficient in practice and difficult to implement. We present AdaPart, a simple and efficient me"
    },
    {
        "title": "Approximating the Permanent by Sampling from Adaptive Partitions",
        "text": "thod for drawing exact samples from an unnormalized distribution. Using AdaPart, we show how to construct tight bounds on the permanent which hold with high probability, with guaranteed polynomial runtime for dense matrices. We find that AdaPart can provid"
    },
    {
        "title": "Approximating the Permanent by Sampling from Adaptive Partitions",
        "text": "e empirical speedups exceeding 25x over prior sampling methods on matrices that are challenging for variational based approaches. Finally, in the context of multi-target tracking, exact sampling from the distribution defined by the matrix permanent allows "
    },
    {
        "title": "Approximating the Permanent by Sampling from Adaptive Partitions",
        "text": "us to use the optimal proposal distribution during particle filtering. Using AdaPart, we show that this leads to improved tracking performance using an order of magnitude fewer samples."
    },
    {
        "title": "Deep Learning for Spatio-Temporal Data Mining: A Survey",
        "text": "With the fast development of various positioning techniques such as Global Position System (GPS), mobile devices and remote sensing, spatio-temporal data has become increasingly available nowadays. Mining valuable knowledge from spatio-temporal data is cri"
    },
    {
        "title": "Deep Learning for Spatio-Temporal Data Mining: A Survey",
        "text": "tically important to many real world applications including human mobility understanding, smart transportation, urban planning, public safety, health care and environmental management. As the number, volume and resolution of spatio-temporal datasets increa"
    },
    {
        "title": "Deep Learning for Spatio-Temporal Data Mining: A Survey",
        "text": "se rapidly, traditional data mining methods, especially statistics based methods for dealing with such data are becoming overwhelmed. Recently, with the advances of deep learning techniques, deep leaning models such as convolutional neural network (CNN) an"
    },
    {
        "title": "Deep Learning for Spatio-Temporal Data Mining: A Survey",
        "text": "d recurrent neural network (RNN) have enjoyed considerable success in various machine learning tasks due to their powerful hierarchical feature learning ability in both spatial and temporal domains, and have been widely applied in various spatio-temporal d"
    },
    {
        "title": "Deep Learning for Spatio-Temporal Data Mining: A Survey",
        "text": "ata mining (STDM) tasks such as predictive learning, representation learning, anomaly detection and classification. In this paper, we provide a comprehensive survey on recent progress in applying deep learning techniques for STDM. We first categorize the t"
    },
    {
        "title": "Deep Learning for Spatio-Temporal Data Mining: A Survey",
        "text": "ypes of spatio-temporal data and briefly introduce the popular deep learning models that are used in STDM. Then a framework is introduced to show a general pipeline of the utilization of deep learning models for STDM. Next we classify existing literatures "
    },
    {
        "title": "Deep Learning for Spatio-Temporal Data Mining: A Survey",
        "text": "based on the types of ST data, the data mining tasks, and the deep learning models, followed by the applications of deep learning for STDM in different domains including transportation, climate science, human mobility, location based social network, crime "
    },
    {
        "title": "Deep Learning for Spatio-Temporal Data Mining: A Survey",
        "text": "analysis, and neuroscience. Finally, we conclude the limitations of current research and point out future research directions."
    },
    {
        "title": "Shampoo: Preconditioned Stochastic Tensor Optimization",
        "text": "Preconditioned gradient methods are among the most general and powerful tools in optimization. However, preconditioning requires storing and manipulating prohibitively large matrices. We describe and analyze a new structure-aware preconditioning algorithm,"
    },
    {
        "title": "Shampoo: Preconditioned Stochastic Tensor Optimization",
        "text": " called Shampoo, for stochastic optimization over tensor spaces. Shampoo maintains a set of preconditioning matrices, each of which operates on a single dimension, contracting over the remaining dimensions. We establish convergence guarantees in the stocha"
    },
    {
        "title": "Shampoo: Preconditioned Stochastic Tensor Optimization",
        "text": "stic convex setting, the proof of which builds upon matrix trace inequalities. Our experiments with state-of-the-art deep learning models show that Shampoo is capable of converging considerably faster than commonly used optimizers. Although it involves a m"
    },
    {
        "title": "Shampoo: Preconditioned Stochastic Tensor Optimization",
        "text": "ore complex update rule, Shampoo's runtime per step is comparable to that of simple gradient methods such as SGD, AdaGrad, and Adam."
    },
    {
        "title": "SynFi: Automatic Synthetic Fingerprint Generation",
        "text": "Authentication and identification methods based on human fingerprints are ubiquitous in several systems ranging from government organizations to consumer products. The performance and reliability of such systems directly rely on the volume of data on which"
    },
    {
        "title": "SynFi: Automatic Synthetic Fingerprint Generation",
        "text": " they have been verified. Unfortunately, a large volume of fingerprint databases is not publicly available due to many privacy and security concerns.   In this paper, we introduce a new approach to automatically generate high-fidelity synthetic fingerprint"
    },
    {
        "title": "SynFi: Automatic Synthetic Fingerprint Generation",
        "text": "s at scale. Our approach relies on (i) Generative Adversarial Networks to estimate the probability distribution of human fingerprints and (ii) Super-Resolution methods to synthesize fine-grained textures. We rigorously test our system and show that our met"
    },
    {
        "title": "SynFi: Automatic Synthetic Fingerprint Generation",
        "text": "hodology is the first to generate fingerprints that are computationally indistinguishable from real ones, a task that prior art could not accomplish."
    },
    {
        "title": "A DNN Based Post-Filter to Enhance the Quality of Coded Speech in MDCT   Domain",
        "text": "Frequency domain processing, and in particular the use of Modified Discrete Cosine Transform (MDCT), is the most widespread approach to audio coding. However, at low bitrates, audio quality, especially for speech, degrades drastically due to the lack of av"
    },
    {
        "title": "A DNN Based Post-Filter to Enhance the Quality of Coded Speech in MDCT   Domain",
        "text": "ailable bits to directly code the transform coefficients. Traditionally, post-filtering has been used to mitigate artefacts in the coded speech by exploiting a-priori information of the source and extra transmitted parameters. Recently, data-driven post-fi"
    },
    {
        "title": "A DNN Based Post-Filter to Enhance the Quality of Coded Speech in MDCT   Domain",
        "text": "lters have shown better results, but at the cost of significant additional complexity and delay. In this work, we propose a mask-based post-filter operating directly in MDCT domain of the codec, inducing no extra delay. The real-valued mask is applied to t"
    },
    {
        "title": "A DNN Based Post-Filter to Enhance the Quality of Coded Speech in MDCT   Domain",
        "text": "he quantized MDCT coefficients and is estimated from a relatively lightweight convolutional encoder-decoder network. Our solution is tested on the recently standardized low-delay, low-complexity codec (LC3) at lowest possible bitrate of 16 kbps. Objective "
    },
    {
        "title": "A DNN Based Post-Filter to Enhance the Quality of Coded Speech in MDCT   Domain",
        "text": "and subjective assessments clearly show the advantage of this approach over the conventional post-filter, with an average improvement of 10 MUSHRA points over the LC3 coded speech."
    },
    {
        "title": "Root Mean Square Layer Normalization",
        "text": "Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However"
    },
    {
        "title": "Root Mean Square Layer Normalization",
        "text": ", the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network, e.g. RNN in particular. In this paper, we hypothesize that re-centering invariance in LayerNorm is dispensable and propo"
    },
    {
        "title": "Root Mean Square Layer Normalization",
        "text": "se root mean square layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm "
    },
    {
        "title": "Root Mean Square Layer Normalization",
        "text": "is computationally simpler and thus more efficient than LayerNorm. We also present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of the summed inputs without breaking the above properties. Extensive experiments on several tasks using dive"
    },
    {
        "title": "Root Mean Square Layer Normalization",
        "text": "rse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7%~64% on different models. Source code is available at https://github.com/bzhangGo/rmsnorm."
    },
    {
        "title": "The Implicit Bias of AdaGrad on Separable Data",
        "text": "We study the implicit bias of AdaGrad on separable linear classification problems. We show that AdaGrad converges to a direction that can be characterized as the solution of a quadratic optimization problem with the same feasible set as the hard SVM proble"
    },
    {
        "title": "The Implicit Bias of AdaGrad on Separable Data",
        "text": "m. We also give a discussion about how different choices of the hyperparameters of AdaGrad might impact this direction. This provides a deeper understanding of why adaptive methods do not seem to have the generalization ability as good as gradient descent "
    },
    {
        "title": "The Implicit Bias of AdaGrad on Separable Data",
        "text": "does in practice."
    },
    {
        "title": "Comparison of Syntactic and Semantic Representations of Programs in   Neural Embeddings",
        "text": "Neural approaches to program synthesis and understanding have proliferated widely in the last few years; at the same time graph based neural networks have become a promising new tool. This work aims to be the first empirical study comparing the effectivene"
    },
    {
        "title": "Comparison of Syntactic and Semantic Representations of Programs in   Neural Embeddings",
        "text": "ss of natural language models and static analysis graph based models in representing programs in deep learning systems. It compares graph convolutional networks using different graph representations in the task of program embedding. It shows that the spars"
    },
    {
        "title": "Comparison of Syntactic and Semantic Representations of Programs in   Neural Embeddings",
        "text": "ity of control flow graphs and the implicit aggregation of graph convolutional networks cause these models to perform worse than naive models. Therefore it concludes that simply augmenting purely linguistic or statistical models with formal information doe"
    },
    {
        "title": "Comparison of Syntactic and Semantic Representations of Programs in   Neural Embeddings",
        "text": "s not perform well due to the nuanced nature of formal properties introducing more noise than structure for graph convolutional networks."
    },
    {
        "title": "Self-paced Principal Component Analysis",
        "text": "Principal Component Analysis (PCA) has been widely used for dimensionality reduction and feature extraction. Robust PCA (RPCA), under different robust distance metrics, such as l1-norm and l2, p-norm, can deal with noise or outliers to some extent. However"
    },
    {
        "title": "Self-paced Principal Component Analysis",
        "text": ", real-world data may display structures that can not be fully captured by these simple functions. In addition, existing methods treat complex and simple samples equally. By contrast, a learning pattern typically adopted by human beings is to learn from si"
    },
    {
        "title": "Self-paced Principal Component Analysis",
        "text": "mple to complex and less to more. Based on this principle, we propose a novel method called Self-paced PCA (SPCA) to further reduce the effect of noise and outliers. Notably, the complexity of each sample is calculated at the beginning of each iteration in"
    },
    {
        "title": "Self-paced Principal Component Analysis",
        "text": " order to integrate samples from simple to more complex into training. Based on an alternating optimization, SPCA finds an optimal projection matrix and filters out outliers iteratively. Theoretical analysis is presented to show the rationality of SPCA. Ex"
    },
    {
        "title": "Self-paced Principal Component Analysis",
        "text": "tensive experiments on popular data sets demonstrate that the proposed method can improve the state of-the-art results considerably."
    },
    {
        "title": "Unsupervised Feature Learning for Environmental Sound Classification   Using Weighted Cycle-Consistent Generative Adversarial Network",
        "text": "In this paper we propose a novel environmental sound classification approach incorporating unsupervised feature learning from codebook via spherical $K$-Means++ algorithm and a new architecture for high-level data augmentation. The audio signal is transfor"
    },
    {
        "title": "Unsupervised Feature Learning for Environmental Sound Classification   Using Weighted Cycle-Consistent Generative Adversarial Network",
        "text": "med into a 2D representation using a discrete wavelet transform (DWT). The DWT spectrograms are then augmented by a novel architecture for cycle-consistent generative adversarial network. This high-level augmentation bootstraps generated spectrograms in bo"
    },
    {
        "title": "Unsupervised Feature Learning for Environmental Sound Classification   Using Weighted Cycle-Consistent Generative Adversarial Network",
        "text": "th intra and inter class manners by translating structural features from sample to sample. A codebook is built by coding the DWT spectrograms with the speeded-up robust feature detector (SURF) and the K-Means++ algorithm. The Random Forest is our final lea"
    },
    {
        "title": "Unsupervised Feature Learning for Environmental Sound Classification   Using Weighted Cycle-Consistent Generative Adversarial Network",
        "text": "rning algorithm which learns the environmental sound classification task from the clustered codewords in the codebook. Experimental results in four benchmarking environmental sound datasets (ESC-10, ESC-50, UrbanSound8k, and DCASE-2017) have shown that the"
    },
    {
        "title": "Unsupervised Feature Learning for Environmental Sound Classification   Using Weighted Cycle-Consistent Generative Adversarial Network",
        "text": " proposed classification approach outperforms the state-of-the-art classifiers in the scope, including advanced and dense convolutional neural networks such as AlexNet and GoogLeNet, improving the classification rate between 3.51% and 14.34%, depending on "
    },
    {
        "title": "Unsupervised Feature Learning for Environmental Sound Classification   Using Weighted Cycle-Consistent Generative Adversarial Network",
        "text": "the dataset."
    },
    {
        "title": "Federated Active Learning (F-AL): an Efficient Annotation Strategy for   Federated Learning",
        "text": "Federated learning (FL) has been intensively investigated in terms of communication efficiency, privacy, and fairness. However, efficient annotation, which is a pain point in real-world FL applications, is less studied. In this project, we propose to apply"
    },
    {
        "title": "Federated Active Learning (F-AL): an Efficient Annotation Strategy for   Federated Learning",
        "text": " active learning (AL) and sampling strategy into the FL framework to reduce the annotation workload. We expect that the AL and FL can improve the performance of each other complementarily. In our proposed federated active learning (F-AL) method, the client"
    },
    {
        "title": "Federated Active Learning (F-AL): an Efficient Annotation Strategy for   Federated Learning",
        "text": "s collaboratively implement the AL to obtain the instances which are considered as informative to FL in a distributed optimization manner. We compare the test accuracies of the global FL models using the conventional random sampling strategy, client-level "
    },
    {
        "title": "Federated Active Learning (F-AL): an Efficient Annotation Strategy for   Federated Learning",
        "text": "separate AL (S-AL), and the proposed F-AL. We empirically demonstrate that the F-AL outperforms baseline methods in image classification tasks."
    },
    {
        "title": "Ensemble Machine Learning Methods for Modeling COVID19 Deaths",
        "text": "Using a hybrid of machine learning and epidemiological approaches, we propose a novel data-driven approach in predicting US COVID-19 deaths at a county level. The model gives a more complete description of the daily death distribution, outputting quantile-"
    },
    {
        "title": "Ensemble Machine Learning Methods for Modeling COVID19 Deaths",
        "text": "estimates instead of mean deaths, where the model's objective is to minimize the pinball loss on deaths reported by the New York Times coronavirus county dataset. The resulting quantile estimates accurately forecast deaths at an individual-county level for"
    },
    {
        "title": "Ensemble Machine Learning Methods for Modeling COVID19 Deaths",
        "text": " a variable-length forecast period, and the approach generalizes well across different forecast period lengths. We won the Caltech-run modeling competition out of 50+ teams, and our aggregate is competitive with the best COVID-19 modeling systems (on root "
    },
    {
        "title": "Ensemble Machine Learning Methods for Modeling COVID19 Deaths",
        "text": "mean squared error)."
    },
    {
        "title": "Making Neural Programming Architectures Generalize via Recursion",
        "text": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to a"
    },
    {
        "title": "Making Neural Programming Architectures Generalize via Recursion",
        "text": "ddress these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort,"
    },
    {
        "title": "Making Neural Programming Architectures Generalize via Recursion",
        "text": " and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to"
    },
    {
        "title": "Making Neural Programming Architectures Generalize via Recursion",
        "text": " prove guarantees about the overall system's behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion."
    },
    {
        "title": "Stacking Models for Nearly Optimal Link Prediction in Complex Networks",
        "text": "Most real-world networks are incompletely observed. Algorithms that can accurately predict which links are missing can dramatically speedup the collection of network data and improve the validity of network models. Many algorithms now exist for predicting "
    },
    {
        "title": "Stacking Models for Nearly Optimal Link Prediction in Complex Networks",
        "text": "missing links, given a partially observed network, but it has remained unknown whether a single best predictor exists, how link predictability varies across methods and networks from different domains, and how close to optimality current methods are. We an"
    },
    {
        "title": "Stacking Models for Nearly Optimal Link Prediction in Complex Networks",
        "text": "swer these questions by systematically evaluating 203 individual link predictor algorithms, representing three popular families of methods, applied to a large corpus of 548 structurally diverse networks from six scientific domains. We first show that indiv"
    },
    {
        "title": "Stacking Models for Nearly Optimal Link Prediction in Complex Networks",
        "text": "idual algorithms exhibit a broad diversity of prediction errors, such that no one predictor or family is best, or worst, across all realistic inputs. We then exploit this diversity via meta-learning to construct a series of \"stacked\" models that combine pr"
    },
    {
        "title": "Stacking Models for Nearly Optimal Link Prediction in Complex Networks",
        "text": "edictors into a single algorithm. Applied to a broad range of synthetic networks, for which we may analytically calculate optimal performance, these stacked models achieve optimal or nearly optimal levels of accuracy. Applied to real-world networks, stacke"
    },
    {
        "title": "Stacking Models for Nearly Optimal Link Prediction in Complex Networks",
        "text": "d models are also superior, but their accuracy varies strongly by domain, suggesting that link prediction may be fundamentally easier in social networks than in biological or technological networks. These results indicate that the state-of-the-art for link"
    },
    {
        "title": "Stacking Models for Nearly Optimal Link Prediction in Complex Networks",
        "text": " prediction comes from combining individual algorithms, which achieves nearly optimal predictions. We close with a brief discussion of limitations and opportunities for further improvement of these results."
    },
    {
        "title": "Neural Volumetric Object Selection",
        "text": "We introduce an approach for selecting objects in neural volumetric 3D representations, such as multi-plane images (MPI) and neural radiance fields (NeRF). Our approach takes a set of foreground and background 2D user scribbles in one view and automaticall"
    },
    {
        "title": "Neural Volumetric Object Selection",
        "text": "y estimates a 3D segmentation of the desired object, which can be rendered into novel views. To achieve this result, we propose a novel voxel feature embedding that incorporates the neural volumetric 3D representation and multi-view image features from all"
    },
    {
        "title": "Neural Volumetric Object Selection",
        "text": " input views. To evaluate our approach, we introduce a new dataset of human-provided segmentation masks for depicted objects in real-world multi-view scene captures. We show that our approach out-performs strong baselines, including 2D segmentation and 3D "
    },
    {
        "title": "Neural Volumetric Object Selection",
        "text": "segmentation approaches adapted to our task."
    },
    {
        "title": "Orthogonal Wasserstein GANs",
        "text": "Wasserstein-GANs have been introduced to address the deficiencies of generative adversarial networks (GANs) regarding the problems of vanishing gradients and mode collapse during the training, leading to improved convergence behaviour and improved image qu"
    },
    {
        "title": "Orthogonal Wasserstein GANs",
        "text": "ality. However, Wasserstein-GANs require the discriminator to be Lipschitz continuous. In current state-of-the-art Wasserstein-GANs this constraint is enforced via gradient norm regularization. In this paper, we demonstrate that this regularization does no"
    },
    {
        "title": "Orthogonal Wasserstein GANs",
        "text": "t encourage a broad distribution of spectral-values in the discriminator weights, hence resulting in less fidelity in the learned distribution. We therefore investigate the possibility of substituting this Lipschitz constraint with an orthogonality constra"
    },
    {
        "title": "Orthogonal Wasserstein GANs",
        "text": "int on the weight matrices. We compare three different weight orthogonalization techniques with regards to their convergence properties, their ability to ensure the Lipschitz condition and the achieved quality of the learned distribution. In addition, we p"
    },
    {
        "title": "Orthogonal Wasserstein GANs",
        "text": "rovide a comparison to Wasserstein-GANs trained with current state-of-the-art methods, where we demonstrate the potential of solely using orthogonality-based regularization. In this context, we propose an improved training procedure for Wasserstein-GANs wh"
    },
    {
        "title": "Orthogonal Wasserstein GANs",
        "text": "ich utilizes orthogonalization to further increase its generalization capability. Finally, we provide a novel metric to evaluate the generalization capabilities of the discriminators of different Wasserstein-GANs."
    },
    {
        "title": "End-to-End Neural Segmental Models for Speech Recognition",
        "text": "Segmental models are an alternative to frame-based models for sequence prediction, where hypothesized path weights are based on entire segment scores rather than a single frame at a time. Neural segmental models are segmental models that use neural network"
    },
    {
        "title": "End-to-End Neural Segmental Models for Speech Recognition",
        "text": "-based weight functions. Neural segmental models have achieved competitive results for speech recognition, and their end-to-end training has been explored in several studies. In this work, we review neural segmental models, which can be viewed as consistin"
    },
    {
        "title": "End-to-End Neural Segmental Models for Speech Recognition",
        "text": "g of a neural network-based acoustic encoder and a finite-state transducer decoder. We study end-to-end segmental models with different weight functions, including ones based on frame-level neural classifiers and on segmental recurrent neural networks. We "
    },
    {
        "title": "End-to-End Neural Segmental Models for Speech Recognition",
        "text": "study how reducing the search space size impacts performance under different weight functions. We also compare several loss functions for end-to-end training. Finally, we explore training approaches, including multi-stage vs. end-to-end training and multit"
    },
    {
        "title": "End-to-End Neural Segmental Models for Speech Recognition",
        "text": "ask training that combines segmental and frame-level losses."
    },
    {
        "title": "Learning Hidden Structures with Relational Models by Adequately   Involving Rich Information in A Network",
        "text": "Effectively modelling hidden structures in a network is very practical but theoretically challenging. Existing relational models only involve very limited information, namely the binary directional link data, embedded in a network to learn hidden networkin"
    },
    {
        "title": "Learning Hidden Structures with Relational Models by Adequately   Involving Rich Information in A Network",
        "text": "g structures. There is other rich and meaningful information (e.g., various attributes of entities and more granular information than binary elements such as \"like\" or \"dislike\") missed, which play a critical role in forming and understanding relations in "
    },
    {
        "title": "Learning Hidden Structures with Relational Models by Adequately   Involving Rich Information in A Network",
        "text": "a network. In this work, we propose an informative relational model (InfRM) framework to adequately involve rich information and its granularity in a network, including metadata information about each entity and various forms of link data. Firstly, an effe"
    },
    {
        "title": "Learning Hidden Structures with Relational Models by Adequately   Involving Rich Information in A Network",
        "text": "ctive metadata information incorporation method is employed on the prior information from relational models MMSB and LFRM. This is to encourage the entities with similar metadata information to have similar hidden structures. Secondly, we propose various s"
    },
    {
        "title": "Learning Hidden Structures with Relational Models by Adequately   Involving Rich Information in A Network",
        "text": "olutions to cater for alternative forms of link data. Substantial efforts have been made towards modelling appropriateness and efficiency, for example, using conjugate priors. We evaluate our framework and its inference algorithms in different datasets, wh"
    },
    {
        "title": "Learning Hidden Structures with Relational Models by Adequately   Involving Rich Information in A Network",
        "text": "ich shows the generality and effectiveness of our models in capturing implicit structures in networks."
    },
    {
        "title": "Learning Bounds for Open-Set Learning",
        "text": "Traditional supervised learning aims to train a classifier in the closed-set world, where training and test samples share the same label space. In this paper, we target a more challenging and realistic setting: open-set learning (OSL), where there exist te"
    },
    {
        "title": "Learning Bounds for Open-Set Learning",
        "text": "st samples from the classes that are unseen during training. Although researchers have designed many methods from the algorithmic perspectives, there are few methods that provide generalization guarantees on their ability to achieve consistent performance "
    },
    {
        "title": "Learning Bounds for Open-Set Learning",
        "text": "on different training samples drawn from the same distribution. Motivated by the transfer learning and probably approximate correct (PAC) theory, we make a bold attempt to study OSL by proving its generalization error-given training samples with size n, th"
    },
    {
        "title": "Learning Bounds for Open-Set Learning",
        "text": "e estimation error will get close to order O_p(1/\\sqrt{n}). This is the first study to provide a generalization bound for OSL, which we do by theoretically investigating the risk of the target classifier on unknown classes. According to our theory, a novel"
    },
    {
        "title": "Learning Bounds for Open-Set Learning",
        "text": " algorithm, called auxiliary open-set risk (AOSR) is proposed to address the OSL problem. Experiments verify the efficacy of AOSR. The code is available at github.com/Anjin-Liu/Openset_Learning_AOSR."
    },
    {
        "title": "Mind-proofing Your Phone: Navigating the Digital Minefield with   GreaseTerminator",
        "text": "Digital harms are widespread in the mobile ecosystem. As these devices gain ever more prominence in our daily lives, so too increases the potential for malicious attacks against individuals. The last line of defense against a range of digital harms - inclu"
    },
    {
        "title": "Mind-proofing Your Phone: Navigating the Digital Minefield with   GreaseTerminator",
        "text": "ding digital distraction, political polarisation through hate speech, and children being exposed to damaging material - is the user interface. This work introduces GreaseTerminator to enable researchers to develop, deploy, and test interventions against th"
    },
    {
        "title": "Mind-proofing Your Phone: Navigating the Digital Minefield with   GreaseTerminator",
        "text": "ese harms with end-users. We demonstrate the ease of intervention development and deployment, as well as the broad range of harms potentially covered with GreaseTerminator in five in-depth case studies."
    },
    {
        "title": "From Species to Cultivar: Soybean Cultivar Recognition using Multiscale   Sliding Chord Matching of Leaf Images",
        "text": "Leaf image recognition techniques have been actively researched for plant species identification. However it remains unclear whether leaf patterns can provide sufficient information for cultivar recognition. This paper reports the first attempt on soybean "
    },
    {
        "title": "From Species to Cultivar: Soybean Cultivar Recognition using Multiscale   Sliding Chord Matching of Leaf Images",
        "text": "cultivar recognition from plant leaves which is not only a challenging research problem but also important for soybean cultivar evaluation, selection and production in agriculture. In this paper, we propose a novel multiscale sliding chord matching (MSCM) "
    },
    {
        "title": "From Species to Cultivar: Soybean Cultivar Recognition using Multiscale   Sliding Chord Matching of Leaf Images",
        "text": "approach to extract leaf patterns that are distinctive for soybean cultivar identification. A chord is defined to slide along the contour for measuring the synchronised patterns of exterior shape and interior appearance of soybean leaf images. A multiscale"
    },
    {
        "title": "From Species to Cultivar: Soybean Cultivar Recognition using Multiscale   Sliding Chord Matching of Leaf Images",
        "text": " sliding chord strategy is developed to extract features in a coarse-to-fine hierarchical order. A joint description that integrates the leaf descriptors from different parts of a soybean plant is proposed for further enhancing the discriminative power of "
    },
    {
        "title": "From Species to Cultivar: Soybean Cultivar Recognition using Multiscale   Sliding Chord Matching of Leaf Images",
        "text": "cultivar description. We built a cultivar leaf image database, SoyCultivar, consisting of 1200 sample leaf images from 200 soybean cultivars for performance evaluation. Encouraging experimental results of the proposed method in comparison to the state-of-t"
    },
    {
        "title": "From Species to Cultivar: Soybean Cultivar Recognition using Multiscale   Sliding Chord Matching of Leaf Images",
        "text": "he-art leaf species recognition methods demonstrate the availability of cultivar information in soybean leaves and effectiveness of the proposed MSCM for soybean cultivar identification, which may advance the research in leaf recognition from species to cu"
    },
    {
        "title": "From Species to Cultivar: Soybean Cultivar Recognition using Multiscale   Sliding Chord Matching of Leaf Images",
        "text": "ltivar."
    },
    {
        "title": "Residual-Concatenate Neural Network with Deep Regularization Layers for Binary Classification",
        "text": "Many complex Deep Learning models are used with different variations for various prognostication tasks. The higher learning parameters not necessarily ensure great accuracy. This can be solved by considering changes in very deep models with many regulariza"
    },
    {
        "title": "Residual-Concatenate Neural Network with Deep Regularization Layers for Binary Classification",
        "text": "tion based techniques. In this paper we train a deep neural network that uses many regularization layers with residual and concatenation process for best fit with Polycystic Ovary Syndrome Diagnosis prognostication. The network was built with improvements "
    },
    {
        "title": "Residual-Concatenate Neural Network with Deep Regularization Layers for Binary Classification",
        "text": "from every step of failure to meet the needs of the data and achieves an accuracy of 99.3% seamlessly."
    },
    {
        "title": "Fundamental Tradeoffs in Distributionally Adversarial Training",
        "text": "Adversarial training is among the most effective techniques to improve the robustness of models against adversarial perturbations. However, the full effect of this approach on models is not well understood. For example, while adversarial training can reduc"
    },
    {
        "title": "Fundamental Tradeoffs in Distributionally Adversarial Training",
        "text": "e the adversarial risk (prediction error against an adversary), it sometimes increase standard risk (generalization error when there is no adversary). Even more, such behavior is impacted by various elements of the learning problem, including the size and "
    },
    {
        "title": "Fundamental Tradeoffs in Distributionally Adversarial Training",
        "text": "quality of training data, specific forms of adversarial perturbations in the input, model overparameterization, and adversary's power, among others. In this paper, we focus on \\emph{distribution perturbing} adversary framework wherein the adversary can cha"
    },
    {
        "title": "Fundamental Tradeoffs in Distributionally Adversarial Training",
        "text": "nge the test distribution within a neighborhood of the training data distribution. The neighborhood is defined via Wasserstein distance between distributions and the radius of the neighborhood is a measure of adversary's manipulative power. We study the tr"
    },
    {
        "title": "Fundamental Tradeoffs in Distributionally Adversarial Training",
        "text": "adeoff between standard risk and adversarial risk and derive the Pareto-optimal tradeoff, achievable over specific classes of models, in the infinite data limit with features dimension kept fixed. We consider three learning settings: 1) Regression with the"
    },
    {
        "title": "Fundamental Tradeoffs in Distributionally Adversarial Training",
        "text": " class of linear models; 2) Binary classification under the Gaussian mixtures data model, with the class of linear classifiers; 3) Regression with the class of random features model (which can be equivalently represented as two-layer neural network with ra"
    },
    {
        "title": "Fundamental Tradeoffs in Distributionally Adversarial Training",
        "text": "ndom first-layer weights). We show that a tradeoff between standard and adversarial risk is manifested in all three settings. We further characterize the Pareto-optimal tradeoff curves and discuss how a variety of factors, such as features correlation, adv"
    },
    {
        "title": "Fundamental Tradeoffs in Distributionally Adversarial Training",
        "text": "ersary's power or the width of two-layer neural network would affect this tradeoff."
    },
    {
        "title": "Towards Crowdsourced Training of Large Neural Networks using   Decentralized Mixture-of-Experts",
        "text": "Many recent breakthroughs in deep learning were achieved by training increasingly larger models on massive datasets. However, training such models can be prohibitively expensive. For instance, the cluster used to train GPT-3 costs over \\$250 million. As a "
    },
    {
        "title": "Towards Crowdsourced Training of Large Neural Networks using   Decentralized Mixture-of-Experts",
        "text": "result, most researchers cannot afford to train state of the art models and contribute to their development. Hypothetically, a researcher could crowdsource the training of large neural networks with thousands of regular PCs provided by volunteers. The raw "
    },
    {
        "title": "Towards Crowdsourced Training of Large Neural Networks using   Decentralized Mixture-of-Experts",
        "text": "computing power of a hundred thousand \\$2500 desktops dwarfs that of a \\$250M server pod, but one cannot utilize that power efficiently with conventional distributed training methods. In this work, we propose Learning@home: a novel neural network training "
    },
    {
        "title": "Towards Crowdsourced Training of Large Neural Networks using   Decentralized Mixture-of-Experts",
        "text": "paradigm designed to handle large amounts of poorly connected participants. We analyze the performance, reliability, and architectural constraints of this paradigm and compare it against existing distributed training techniques."
    },
    {
        "title": "Accelerating Perturbed Stochastic Iterates in Asynchronous Lock-Free   Optimization",
        "text": "We show that stochastic acceleration can be achieved under the perturbed iterate framework (Mania et al., 2017) in asynchronous lock-free optimization, which leads to the optimal incremental gradient complexity for finite-sum objectives. We prove that our "
    },
    {
        "title": "Accelerating Perturbed Stochastic Iterates in Asynchronous Lock-Free   Optimization",
        "text": "new accelerated method requires the same linear speed-up condition as the existing non-accelerated methods. Our core algorithmic discovery is a new accelerated SVRG variant with sparse updates. Empirical results are presented to verify our theoretical find"
    },
    {
        "title": "Accelerating Perturbed Stochastic Iterates in Asynchronous Lock-Free   Optimization",
        "text": "ings."
    },
    {
        "title": "Learning About Objects by Learning to Interact with Them",
        "text": "Much of the remarkable progress in computer vision has been focused around fully supervised learning mechanisms relying on highly curated datasets for a variety of tasks. In contrast, humans often learn about their world with little to no external supervis"
    },
    {
        "title": "Learning About Objects by Learning to Interact with Them",
        "text": "ion. Taking inspiration from infants learning from their environment through play and interaction, we present a computational framework to discover objects and learn their physical properties along this paradigm of Learning from Interaction. Our agent, whe"
    },
    {
        "title": "Learning About Objects by Learning to Interact with Them",
        "text": "n placed within the near photo-realistic and physics-enabled AI2-THOR environment, interacts with its world and learns about objects, their geometric extents and relative masses, without any external guidance. Our experiments reveal that this agent learns "
    },
    {
        "title": "Learning About Objects by Learning to Interact with Them",
        "text": "efficiently and effectively; not just for objects it has interacted with before, but also for novel instances from seen categories as well as novel object categories."
    },
    {
        "title": "Fast Convergence of Natural Gradient Descent for Overparameterized   Neural Networks",
        "text": "Natural gradient descent has proven effective at mitigating the effects of pathological curvature in neural network optimization, but little is known theoretically about its convergence properties, especially for \\emph{nonlinear} networks. In this work, we"
    },
    {
        "title": "Fast Convergence of Natural Gradient Descent for Overparameterized   Neural Networks",
        "text": " analyze for the first time the speed of convergence of natural gradient descent on nonlinear neural networks with squared-error loss. We identify two conditions which guarantee efficient convergence from random initializations: (1) the Jacobian matrix (of"
    },
    {
        "title": "Fast Convergence of Natural Gradient Descent for Overparameterized   Neural Networks",
        "text": " network's output for all training cases with respect to the parameters) has full row rank, and (2) the Jacobian matrix is stable for small perturbations around the initialization. For two-layer ReLU neural networks, we prove that these two conditions do i"
    },
    {
        "title": "Fast Convergence of Natural Gradient Descent for Overparameterized   Neural Networks",
        "text": "n fact hold throughout the training, under the assumptions of nondegenerate inputs and overparameterization. We further extend our analysis to more general loss functions. Lastly, we show that K-FAC, an approximate natural gradient descent method, also con"
    },
    {
        "title": "Fast Convergence of Natural Gradient Descent for Overparameterized   Neural Networks",
        "text": "verges to global minima under the same assumptions, and we give a bound on the rate of this convergence."
    },
    {
        "title": "Model Degradation Hinders Deep Graph Neural Networks",
        "text": "Graph Neural Networks (GNNs) have achieved great success in various graph mining tasks.However, drastic performance degradation is always observed when a GNN is stacked with many layers. As a result, most GNNs only have shallow architectures, which limits "
    },
    {
        "title": "Model Degradation Hinders Deep Graph Neural Networks",
        "text": "their expressive power and exploitation of deep neighborhoods.Most recent studies attribute the performance degradation of deep GNNs to the \\textit{over-smoothing} issue. In this paper, we disentangle the conventional graph convolution operation into two i"
    },
    {
        "title": "Model Degradation Hinders Deep Graph Neural Networks",
        "text": "ndependent operations: \\textit{Propagation} (\\textbf{P}) and \\textit{Transformation} (\\textbf{T}).Following this, the depth of a GNN can be split into the propagation depth ($D_p$) and the transformation depth ($D_t$). Through extensive experiments, we fin"
    },
    {
        "title": "Model Degradation Hinders Deep Graph Neural Networks",
        "text": "d that the major cause for the performance degradation of deep GNNs is the \\textit{model degradation} issue caused by large $D_t$ rather than the \\textit{over-smoothing} issue mainly caused by large $D_p$. Further, we present \\textit{Adaptive Initial Resid"
    },
    {
        "title": "Model Degradation Hinders Deep Graph Neural Networks",
        "text": "ual} (AIR), a plug-and-play module compatible with all kinds of GNN architectures, to alleviate the \\textit{model degradation} issue and the \\textit{over-smoothing} issue simultaneously. Experimental results on six real-world datasets demonstrate that GNNs"
    },
    {
        "title": "Model Degradation Hinders Deep Graph Neural Networks",
        "text": " equipped with AIR outperform most GNNs with shallow architectures owing to the benefits of both large $D_p$ and $D_t$, while the time costs associated with AIR can be ignored."
    },
    {
        "title": "Robust Finite Mixture Regression for Heterogeneous Targets",
        "text": "Finite Mixture Regression (FMR) refers to the mixture modeling scheme which learns multiple regression models from the training data set. Each of them is in charge of a subset. FMR is an effective scheme for handling sample heterogeneity, where a single re"
    },
    {
        "title": "Robust Finite Mixture Regression for Heterogeneous Targets",
        "text": "gression model is not enough for capturing the complexities of the conditional distribution of the observed samples given the features. In this paper, we propose an FMR model that 1) finds sample clusters and jointly models multiple incomplete mixed-type t"
    },
    {
        "title": "Robust Finite Mixture Regression for Heterogeneous Targets",
        "text": "argets simultaneously, 2) achieves shared feature selection among tasks and cluster components, and 3) detects anomaly tasks or clustered structure among tasks, and accommodates outlier samples. We provide non-asymptotic oracle performance bounds for our m"
    },
    {
        "title": "Robust Finite Mixture Regression for Heterogeneous Targets",
        "text": "odel under a high-dimensional learning framework. The proposed model is evaluated on both synthetic and real-world data sets. The results show that our model can achieve state-of-the-art performance."
    },
    {
        "title": "Robust Meta-learning with Sampling Noise and Label Noise via   Eigen-Reptile",
        "text": "Recent years have seen a surge of interest in meta-learning techniques for tackling the few-shot learning (FSL) problem. However, the meta-learner is prone to overfitting since there are only a few available samples, which can be identified as sampling noi"
    },
    {
        "title": "Robust Meta-learning with Sampling Noise and Label Noise via   Eigen-Reptile",
        "text": "se on a clean dataset. Moreover, when handling the data with noisy labels, the meta-learner could be extremely sensitive to label noise on a corrupted dataset. To address these two challenges, we present Eigen-Reptile (ER) that updates the meta-parameters "
    },
    {
        "title": "Robust Meta-learning with Sampling Noise and Label Noise via   Eigen-Reptile",
        "text": "with the main direction of historical task-specific parameters to alleviate sampling and label noise. Specifically, the main direction is computed in a fast way, where the scale of the calculated matrix is related to the number of gradient steps instead of"
    },
    {
        "title": "Robust Meta-learning with Sampling Noise and Label Noise via   Eigen-Reptile",
        "text": " the number of parameters. Furthermore, to obtain a more accurate main direction for Eigen-Reptile in the presence of many noisy labels, we further propose Introspective Self-paced Learning (ISPL). We have theoretically and experimentally demonstrated the "
    },
    {
        "title": "Robust Meta-learning with Sampling Noise and Label Noise via   Eigen-Reptile",
        "text": "soundness and effectiveness of the proposed Eigen-Reptile and ISPL. Particularly, our experiments on different tasks show that the proposed method is able to outperform or achieve highly competitive performance compared with other gradient-based methods wi"
    },
    {
        "title": "Robust Meta-learning with Sampling Noise and Label Noise via   Eigen-Reptile",
        "text": "th or without noisy labels. The code and data for the proposed method are provided for research purposes https://github.com/Anfeather/Eigen-Reptile."
    },
    {
        "title": "Learning Implicit Text Generation via Feature Matching",
        "text": "Generative feature matching network (GFMN) is an approach for training implicit generative models for images by performing moment matching on features from pre-trained neural networks. In this paper, we present new GFMN formulations that are effective for "
    },
    {
        "title": "Learning Implicit Text Generation via Feature Matching",
        "text": "sequential data. Our experimental results show the effectiveness of the proposed method, SeqGFMN, for three distinct generation tasks in English: unconditional text generation, class-conditional text generation, and unsupervised text style transfer. SeqGFM"
    },
    {
        "title": "Learning Implicit Text Generation via Feature Matching",
        "text": "N is stable to train and outperforms various adversarial approaches for text generation and text style transfer."
    },
    {
        "title": "Identity-aware Graph Neural Networks",
        "text": "Message passing Graph Neural Networks (GNNs) provide a powerful modeling framework for relational data. However, the expressive power of existing GNNs is upper-bounded by the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test, which means GNNs that are not "
    },
    {
        "title": "Identity-aware Graph Neural Networks",
        "text": "able to predict node clustering coefficients and shortest path distances, and cannot differentiate between different d-regular graphs. Here we develop a class of message passing GNNs, named Identity-aware Graph Neural Networks (ID-GNNs), with greater expre"
    },
    {
        "title": "Identity-aware Graph Neural Networks",
        "text": "ssive power than the 1-WL test. ID-GNN offers a minimal but powerful solution to limitations of existing GNNs. ID-GNN extends existing GNN architectures by inductively considering nodes' identities during message passing. To embed a given node, ID-GNN firs"
    },
    {
        "title": "Identity-aware Graph Neural Networks",
        "text": "t extracts the ego network centered at the node, then conducts rounds of heterogeneous message passing, where different sets of parameters are applied to the center node than to other surrounding nodes in the ego network. We further propose a simplified bu"
    },
    {
        "title": "Identity-aware Graph Neural Networks",
        "text": "t faster version of ID-GNN that injects node identity information as augmented node features. Altogether, both versions of ID-GNN represent general extensions of message passing GNNs, where experiments show that transforming existing GNNs to ID-GNNs yields"
    },
    {
        "title": "Identity-aware Graph Neural Networks",
        "text": " on average 40% accuracy improvement on challenging node, edge, and graph property prediction tasks; 3% accuracy improvement on node and graph classification benchmarks; and 15% ROC AUC improvement on real-world link prediction tasks. Additionally, ID-GNNs"
    },
    {
        "title": "Identity-aware Graph Neural Networks",
        "text": " demonstrate improved or comparable performance over other task-specific graph networks."
    },
    {
        "title": "Asymptotically Optimal Algorithms for Budgeted Multiple Play Bandits",
        "text": "We study a generalization of the multi-armed bandit problem with multiple plays where there is a cost associated with pulling each arm and the agent has a budget at each time that dictates how much she can expect to spend. We derive an asymptotic regret lo"
    },
    {
        "title": "Asymptotically Optimal Algorithms for Budgeted Multiple Play Bandits",
        "text": "wer bound for any uniformly efficient algorithm in our setting. We then study a variant of Thompson sampling for Bernoulli rewards and a variant of KL-UCB for both single-parameter exponential families and bounded, finitely supported rewards. We show these"
    },
    {
        "title": "Asymptotically Optimal Algorithms for Budgeted Multiple Play Bandits",
        "text": " algorithms are asymptotically optimal, both in rateand leading problem-dependent constants, including in the thick margin setting where multiple arms fall on the decision boundary."
    },
    {
        "title": "The Generalization Ability of Online Algorithms for Dependent Data",
        "text": "We study the generalization performance of online learning algorithms trained on samples coming from a dependent source of data. We show that the generalization error of any stable online algorithm concentrates around its regret--an easily computable stati"
    },
    {
        "title": "The Generalization Ability of Online Algorithms for Dependent Data",
        "text": "stic of the online performance of the algorithm--when the underlying ergodic process is $\\beta$- or $\\phi$-mixing. We show high probability error bounds assuming the loss function is convex, and we also establish sharp convergence rates and deviation bound"
    },
    {
        "title": "The Generalization Ability of Online Algorithms for Dependent Data",
        "text": "s for strongly convex losses and several linear prediction problems such as linear and logistic regression, least-squares SVM, and boosting on dependent data. In addition, our results have straightforward applications to stochastic optimization with depend"
    },
    {
        "title": "The Generalization Ability of Online Algorithms for Dependent Data",
        "text": "ent data, and our analysis requires only martingale convergence arguments; we need not rely on more powerful statistical tools such as empirical process theory."
    },
    {
        "title": "Masking by Moving: Learning Distraction-Free Radar Odometry from Pose   Information",
        "text": "This paper presents an end-to-end radar odometry system which delivers robust, real-time pose estimates based on a learned embedding space free of sensing artefacts and distractor objects. The system deploys a fully differentiable, correlation-based radar "
    },
    {
        "title": "Masking by Moving: Learning Distraction-Free Radar Odometry from Pose   Information",
        "text": "matching approach. This provides the same level of interpretability as established scan-matching methods and allows for a principled derivation of uncertainty estimates. The system is trained in a (self-)supervised way using only previously obtained pose i"
    },
    {
        "title": "Masking by Moving: Learning Distraction-Free Radar Odometry from Pose   Information",
        "text": "nformation as a training signal. Using 280km of urban driving data, we demonstrate that our approach outperforms the previous state-of-the-art in radar odometry by reducing errors by up 68% whilst running an order of magnitude faster."
    },
    {
        "title": "Graphite: Iterative Generative Modeling of Graphs",
        "text": "Graphs are a fundamental abstraction for modeling relational data. However, graphs are discrete and combinatorial in nature, and learning representations suitable for machine learning tasks poses statistical and computational challenges. In this work, we p"
    },
    {
        "title": "Graphite: Iterative Generative Modeling of Graphs",
        "text": "ropose Graphite, an algorithmic framework for unsupervised learning of representations over nodes in large graphs using deep latent variable generative models. Our model parameterizes variational autoencoders (VAE) with graph neural networks, and uses a no"
    },
    {
        "title": "Graphite: Iterative Generative Modeling of Graphs",
        "text": "vel iterative graph refinement strategy inspired by low-rank approximations for decoding. On a wide variety of synthetic and benchmark datasets, Graphite outperforms competing approaches for the tasks of density estimation, link prediction, and node classi"
    },
    {
        "title": "Graphite: Iterative Generative Modeling of Graphs",
        "text": "fication. Finally, we derive a theoretical connection between message passing in graph neural networks and mean-field variational inference."
    },
    {
        "title": "Predicting and Explaining Mobile UI Tappability with Vision Modeling and   Saliency Analysis",
        "text": "We use a deep learning based approach to predict whether a selected element in a mobile UI screenshot will be perceived by users as tappable, based on pixels only instead of view hierarchies required by previous work. To help designers better understand mo"
    },
    {
        "title": "Predicting and Explaining Mobile UI Tappability with Vision Modeling and   Saliency Analysis",
        "text": "del predictions and to provide more actionable design feedback than predictions alone, we additionally use ML interpretability techniques to help explain the output of our model. We use XRAI to highlight areas in the input screenshot that most strongly inf"
    },
    {
        "title": "Predicting and Explaining Mobile UI Tappability with Vision Modeling and   Saliency Analysis",
        "text": "luence the tappability prediction for the selected region, and use k-Nearest Neighbors to present the most similar mobile UIs from the dataset with opposing influences on tappability perception."
    },
    {
        "title": "Fully Convolutional Networks for Monocular Retinal Depth Estimation and   Optic Disc-Cup Segmentation",
        "text": "Glaucoma is a serious ocular disorder for which the screening and diagnosis are carried out by the examination of the optic nerve head (ONH). The color fundus image (CFI) is the most common modality used for ocular screening. In CFI, the central r"
    },
    {
        "title": "Local Neighbor Propagation Embedding",
        "text": "Manifold Learning occupies a vital role in the field of nonlinear dimensionality reduction and its ideas also serve for other relevant methods. Graph-based methods such as Graph Convolutional Networks (GCN) show ideas in common with manifold learning, alth"
    },
    {
        "title": "Local Neighbor Propagation Embedding",
        "text": "ough they belong to different fields. Inspired by GCN, we introduce neighbor propagation into LLE and propose Local Neighbor Propagation Embedding (LNPE). With linear computational complexity increase compared with LLE, LNPE enhances the local connections "
    },
    {
        "title": "Local Neighbor Propagation Embedding",
        "text": "and interactions between neighborhoods by extending $1$-hop neighbors into $n$-hop neighbors. The experimental results show that LNPE could obtain more faithful and robust embeddings with better topological and geometrical properties."
    },
    {
        "title": "Measuring Mother-Infant Emotions By Audio Sensing",
        "text": "It has been suggested in developmental psychology literature that the communication of affect between mothers and their infants correlates with the socioemotional and cognitive development of infants. In this study, we obtained day-long audio recordings of"
    },
    {
        "title": "Measuring Mother-Infant Emotions By Audio Sensing",
        "text": " 10 mother-infant pairs in order to study their affect communication in speech with a focus on mother's speech. In order to build a model for speech emotion detection, we used the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) and tra"
    },
    {
        "title": "Measuring Mother-Infant Emotions By Audio Sensing",
        "text": "ined a Convolutional Neural Nets model which is able to classify 6 different emotions at 70% accuracy. We applied our model to mother's speech and found the dominant emotions were angry and sad, which were not true. Based on our own observations, we conclu"
    },
    {
        "title": "Measuring Mother-Infant Emotions By Audio Sensing",
        "text": "ded that emotional speech databases made with the help of actors cannot generalize well to real-life settings, suggesting an active learning or unsupervised approach in the future."
    },
    {
        "title": "Learning to Create Better Ads: Generation and Ranking Approaches for Ad   Creative Refinement",
        "text": "In the online advertising industry, the process of designing an ad creative (i.e., ad text and image) requires manual labor. Typically, each advertiser launches multiple creatives via online A/B tests to infer effective creatives for the target audience, t"
    },
    {
        "title": "Learning to Create Better Ads: Generation and Ranking Approaches for Ad   Creative Refinement",
        "text": "hat are then refined further in an iterative fashion. Due to the manual nature of this process, it is time-consuming to learn, refine, and deploy the modified creatives. Since major ad platforms typically run A/B tests for multiple advertisers in parallel,"
    },
    {
        "title": "Learning to Create Better Ads: Generation and Ranking Approaches for Ad   Creative Refinement",
        "text": " we explore the possibility of collaboratively learning ad creative refinement via A/B tests of multiple advertisers. In particular, given an input ad creative, we study approaches to refine the given ad text and image by: (i) generating new ad text, (ii) "
    },
    {
        "title": "Learning to Create Better Ads: Generation and Ranking Approaches for Ad   Creative Refinement",
        "text": "recommending keyphrases for new ad text, and (iii) recommending image tags (objects in image) to select new ad image. Based on A/B tests conducted by multiple advertisers, we form pairwise examples of inferior and superior ad creatives, and use such pairs "
    },
    {
        "title": "Learning to Create Better Ads: Generation and Ranking Approaches for Ad   Creative Refinement",
        "text": "to train models for the above tasks. For generating new ad text, we demonstrate the efficacy of an encoder-decoder architecture with copy mechanism, which allows some words from the (inferior) input text to be copied to the output while incorporating new w"
    },
    {
        "title": "Learning to Create Better Ads: Generation and Ranking Approaches for Ad   Creative Refinement",
        "text": "ords associated with higher click-through-rate. For the keyphrase and image tag recommendation task, we demonstrate the efficacy of a deep relevance matching model, as well as the relative robustness of ranking approaches compared to ad text generation in "
    },
    {
        "title": "Learning to Create Better Ads: Generation and Ranking Approaches for Ad   Creative Refinement",
        "text": "cold-start scenarios with unseen advertisers. We also share broadly applicable insights from our experiments using data from the Yahoo Gemini ad platform."
    },
    {
        "title": "Prediction with Corrupted Expert Advice",
        "text": "We revisit the fundamental problem of prediction with expert advice, in a setting where the environment is benign and generates losses stochastically, but the feedback observed by the learner is subject to a moderate adversarial corruption. We prove that a"
    },
    {
        "title": "Prediction with Corrupted Expert Advice",
        "text": " variant of the classical Multiplicative Weights algorithm with decreasing step sizes achieves constant regret in this setting and performs optimally in a wide range of environments, regardless of the magnitude of the injected corruption. Our results revea"
    },
    {
        "title": "Prediction with Corrupted Expert Advice",
        "text": "l a surprising disparity between the often comparable Follow the Regularized Leader (FTRL) and Online Mirror Descent (OMD) frameworks: we show that for experts in the corrupted stochastic regime, the regret performance of OMD is in fact strictly inferior t"
    },
    {
        "title": "Prediction with Corrupted Expert Advice",
        "text": "o that of FTRL."
    },
    {
        "title": "Batch size-invariance for policy optimization",
        "text": "We say an algorithm is batch size-invariant if changes to the batch size can largely be compensated for by changes to other hyperparameters. Stochastic gradient descent is well-known to have this property at small batch sizes, via the learning rate. Howeve"
    },
    {
        "title": "Batch size-invariance for policy optimization",
        "text": "r, some policy optimization algorithms (such as PPO) do not have this property, because of how they control the size of policy updates. In this work we show how to make these algorithms batch size-invariant. Our key insight is to decouple the proximal poli"
    },
    {
        "title": "Batch size-invariance for policy optimization",
        "text": "cy (used for controlling policy updates) from the behavior policy (used for off-policy corrections). Our experiments help explain why these algorithms work, and additionally show how they can make more efficient use of stale data."
    },
    {
        "title": "A Progressive Batching L-BFGS Method for Machine Learning",
        "text": "The standard L-BFGS method relies on gradient approximations that are not dominated by noise, so that search directions are descent directions, the line search is reliable, and quasi-Newton updating yields useful quadratic models of the objective function."
    },
    {
        "title": "A Progressive Batching L-BFGS Method for Machine Learning",
        "text": " All of this appears to call for a full batch approach, but since small batch sizes give rise to faster algorithms with better generalization properties, L-BFGS is currently not considered an algorithm of choice for large-scale machine learning application"
    },
    {
        "title": "A Progressive Batching L-BFGS Method for Machine Learning",
        "text": "s. One need not, however, choose between the two extremes represented by the full batch or highly stochastic regimes, and may instead follow a progressive batching approach in which the sample size increases during the course of the optimization. In this p"
    },
    {
        "title": "A Progressive Batching L-BFGS Method for Machine Learning",
        "text": "aper, we present a new version of the L-BFGS algorithm that combines three basic components - progressive batching, a stochastic line search, and stable quasi-Newton updating - and that performs well on training logistic regression and deep neural networks"
    },
    {
        "title": "A Progressive Batching L-BFGS Method for Machine Learning",
        "text": ". We provide supporting convergence theory for the method."
    },
    {
        "title": "Convolutional Tensor-Train LSTM for Spatio-temporal Learning",
        "text": "Learning from spatio-temporal data has numerous applications such as human-behavior analysis, object tracking, video compression, and physics simulation.However, existing methods still perform poorly on challenging video tasks such as long-term forecasting"
    },
    {
        "title": "Convolutional Tensor-Train LSTM for Spatio-temporal Learning",
        "text": ". This is because these kinds of challenging tasks require learning long-term spatio-temporal correlations in the video sequence. In this paper, we propose a higher-order convolutional LSTM model that can efficiently learn these correlations, along with a "
    },
    {
        "title": "Convolutional Tensor-Train LSTM for Spatio-temporal Learning",
        "text": "succinct representations of the history. This is accomplished through a novel tensor train module that performs prediction by combining convolutional features across time. To make this feasible in terms of computation and memory requirements, we propose a "
    },
    {
        "title": "Convolutional Tensor-Train LSTM for Spatio-temporal Learning",
        "text": "novel convolutional tensor-train decomposition of the higher-order model. This decomposition reduces the model complexity by jointly approximating a sequence of convolutional kernels asa low-rank tensor-train factorization. As a result, our model outperfor"
    },
    {
        "title": "Convolutional Tensor-Train LSTM for Spatio-temporal Learning",
        "text": "ms existing approaches, but uses only a fraction of parameters, including the baseline models.Our results achieve state-of-the-art performance in a wide range of applications and datasets, including the multi-steps video prediction on the Moving-MNIST-2and"
    },
    {
        "title": "Convolutional Tensor-Train LSTM for Spatio-temporal Learning",
        "text": " KTH action datasets as well as early activity recognition on the Something-Something V2 dataset."
    },
    {
        "title": "Killing four birds with one Gaussian process: the relation between   different test-time attacks",
        "text": "In machine learning (ML) security, attacks like evasion, model stealing or membership inference are generally studied in individually. Previous work has also shown a relationship between some attacks and decision function curvature of the targeted model. C"
    },
    {
        "title": "Killing four birds with one Gaussian process: the relation between   different test-time attacks",
        "text": "onsequently, we study an ML model allowing direct control over the decision surface curvature: Gaussian Process classifiers (GPCs). For evasion, we find that changing GPC's curvature to be robust against one attack algorithm boils down to enabling a differ"
    },
    {
        "title": "Killing four birds with one Gaussian process: the relation between   different test-time attacks",
        "text": "ent norm or attack algorithm to succeed. This is backed up by our formal analysis showing that static security guarantees are opposed to learning. Concerning intellectual property, we show formally that lazy learning does not necessarily leak all informati"
    },
    {
        "title": "Killing four birds with one Gaussian process: the relation between   different test-time attacks",
        "text": "on when applied. In practice, often a seemingly secure curvature can be found. For example, we are able to secure GPC against empirical membership inference by proper configuration. In this configuration, however, the GPC's hyper-parameters are leaked, e.g"
    },
    {
        "title": "Killing four birds with one Gaussian process: the relation between   different test-time attacks",
        "text": ". model reverse engineering succeeds. We conclude that attacks on classification should not be studied in isolation, but in relation to each other."
    },
    {
        "title": "A Learning Approach for Joint Design of Event-triggered Control and Power-Efficient Resource Allocation",
        "text": "In emerging Industrial Cyber-Physical Systems (ICPSs), the joint design of communication and control sub-systems is essential, as these sub-systems are interconnected. In this paper, we study the joint design problem of an event-triggered control and an en"
    },
    {
        "title": "A Learning Approach for Joint Design of Event-triggered Control and Power-Efficient Resource Allocation",
        "text": "ergy-efficient resource allocation in a fifth generation (5G) wireless network. We formally state the problem as a multi-objective optimization one, aiming to minimize the number of updates on the actuators' input and the power consumption in the downlink "
    },
    {
        "title": "A Learning Approach for Joint Design of Event-triggered Control and Power-Efficient Resource Allocation",
        "text": "transmission. To address the problem, we propose a model-free hierarchical reinforcement learning approach \\textcolor{blue}{with uniformly ultimate boundedness stability guarantee} that learns four policies simultaneously. These policies contain an update "
    },
    {
        "title": "A Learning Approach for Joint Design of Event-triggered Control and Power-Efficient Resource Allocation",
        "text": "time policy on the actuators' input, a control policy, and energy-efficient sub-carrier and power allocation policies. Our simulation results show that the proposed approach can properly control a simulated ICPS and significantly decrease the number of upd"
    },
    {
        "title": "A Learning Approach for Joint Design of Event-triggered Control and Power-Efficient Resource Allocation",
        "text": "ates on the actuators' input as well as the downlink power consumption."
    },
    {
        "title": "D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using   Differential Analysis",
        "text": "Static analysis tools are widely used for vulnerability detection as they understand programs with complex behavior and millions of lines of code. Despite their popularity, static analysis tools are known to generate an excess of false positives. The recen"
    },
    {
        "title": "D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using   Differential Analysis",
        "text": "t ability of Machine Learning models to understand programming languages opens new possibilities when applied to static analysis. However, existing datasets to train models for vulnerability identification suffer from multiple limitations such as limited b"
    },
    {
        "title": "D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using   Differential Analysis",
        "text": "ug context, limited size, and synthetic and unrealistic source code. We propose D2A, a differential analysis based approach to label issues reported by static analysis tools. The D2A dataset is built by analyzing version pairs from multiple open source pro"
    },
    {
        "title": "D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using   Differential Analysis",
        "text": "jects. From each project, we select bug fixing commits and we run static analysis on the versions before and after such commits. If some issues detected in a before-commit version disappear in the corresponding after-commit version, they are very likely to"
    },
    {
        "title": "D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using   Differential Analysis",
        "text": " be real bugs that got fixed by the commit. We use D2A to generate a large labeled dataset to train models for vulnerability identification. We show that the dataset can be used to build a classifier to identify possible false alarms among the issues repor"
    },
    {
        "title": "D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using   Differential Analysis",
        "text": "ted by static analysis, hence helping developers prioritize and investigate potential true positives first."
    },
    {
        "title": "The Lipschitz Constant of Self-Attention",
        "text": "Lipschitz constants of neural networks have been explored in various contexts in deep learning, such as provable adversarial robustness, estimating Wasserstein distance, stabilising training of GANs, and formulating invertible neural networks. Such works h"
    },
    {
        "title": "The Lipschitz Constant of Self-Attention",
        "text": "ave focused on bounding the Lipschitz constant of fully connected or convolutional networks, composed of linear maps and pointwise non-linearities. In this paper, we investigate the Lipschitz constant of self-attention, a non-linear neural network module w"
    },
    {
        "title": "The Lipschitz Constant of Self-Attention",
        "text": "idely used in sequence modelling. We prove that the standard dot-product self-attention is not Lipschitz for unbounded input domain, and propose an alternative L2 self-attention that is Lipschitz. We derive an upper bound on the Lipschitz constant of L2 se"
    },
    {
        "title": "The Lipschitz Constant of Self-Attention",
        "text": "lf-attention and provide empirical evidence for its asymptotic tightness. To demonstrate the practical relevance of our theoretical work, we formulate invertible self-attention and use it in a Transformer-based architecture for a character-level language m"
    },
    {
        "title": "The Lipschitz Constant of Self-Attention",
        "text": "odelling task."
    },
    {
        "title": "An Effective Graph Learning based Approach for Temporal Link Prediction:   The First Place of WSDM Cup 2022",
        "text": "Temporal link prediction, as one of the most crucial work in temporal graphs, has attracted lots of attention from the research area. The WSDM Cup 2022 seeks for solutions that predict the existence probabilities of edges within time spans over temporal gr"
    },
    {
        "title": "An Effective Graph Learning based Approach for Temporal Link Prediction:   The First Place of WSDM Cup 2022",
        "text": "aph. This paper introduces the solution of AntGraph, which wins the 1st place in the competition. We first analysis the theoretical upper-bound of the performance by removing temporal information, which implies that only structure and attribute information"
    },
    {
        "title": "An Effective Graph Learning based Approach for Temporal Link Prediction:   The First Place of WSDM Cup 2022",
        "text": " on the graph could achieve great performance. Based on this hypothesis, then we introduce several well-designed features. Finally, experiments conducted on the competition datasets show the superiority of our proposal, which achieved AUC score of 0.666 on"
    },
    {
        "title": "An Effective Graph Learning based Approach for Temporal Link Prediction:   The First Place of WSDM Cup 2022",
        "text": " dataset A and 0.902 on dataset B, the ablation studies also prove the efficiency of each feature. Code is publicly available at https://github.com/im0qianqian/WSDM2022TGP-AntGraph."
    },
    {
        "title": "Differentially Private $\\ell_1$-norm Linear Regression with Heavy-tailed   Data",
        "text": "We study the problem of Differentially Private Stochastic Convex Optimization (DP-SCO) with heavy-tailed data. Specifically, we focus on the $\\ell_1$-norm linear regression in the $\\epsilon$-DP model. While most of the previous work focuses on the case whe"
    },
    {
        "title": "Differentially Private $\\ell_1$-norm Linear Regression with Heavy-tailed   Data",
        "text": "re the loss function is Lipschitz, here we only need to assume the variates has bounded moments. Firstly, we study the case where the $\\ell_2$ norm of data has bounded second order moment. We propose an algorithm which is based on the exponential mechanism"
    },
    {
        "title": "Differentially Private $\\ell_1$-norm Linear Regression with Heavy-tailed   Data",
        "text": " and show that it is possible to achieve an upper bound of $\\tilde{O}(\\sqrt{\\frac{d}{n\\epsilon}})$ (with high probability). Next, we relax the assumption to bounded $\\theta$-th order moment with some $\\theta\\in (1, 2)$ and show that it is possible to achie"
    },
    {
        "title": "Differentially Private $\\ell_1$-norm Linear Regression with Heavy-tailed   Data",
        "text": "ve an upper bound of $\\tilde{O}(({\\frac{d}{n\\epsilon}})^\\frac{\\theta-1}{\\theta})$. Our algorithms can also be extended to more relaxed cases where only each coordinate of the data has bounded moments, and we can get an upper bound of $\\tilde{O}({\\frac{d}{\\"
    },
    {
        "title": "Differentially Private $\\ell_1$-norm Linear Regression with Heavy-tailed   Data",
        "text": "sqrt{n\\epsilon}}})$ and $\\tilde{O}({\\frac{d}{({n\\epsilon})^\\frac{\\theta-1}{\\theta}}})$ in the second and $\\theta$-th moment case respectively."
    },
    {
        "title": "Beyond Localized Graph Neural Networks: An Attributed Motif   Regularization Framework",
        "text": "We present InfoMotif, a new semi-supervised, motif-regularized, learning framework over graphs. We overcome two key limitations of message passing in popular graph neural networks (GNNs): localization (a k-layer GNN cannot utilize features outside the k-ho"
    },
    {
        "title": "Beyond Localized Graph Neural Networks: An Attributed Motif   Regularization Framework",
        "text": "p neighborhood of the labeled training nodes) and over-smoothed (structurally indistinguishable) representations. We propose the concept of attributed structural roles of nodes based on their occurrence in different network motifs, independent of network p"
    },
    {
        "title": "Beyond Localized Graph Neural Networks: An Attributed Motif   Regularization Framework",
        "text": "roximity. Two nodes share attributed structural roles if they participate in topologically similar motif instances over co-varying sets of attributes. Further, InfoMotif achieves architecture independence by regularizing the node representations of arbitra"
    },
    {
        "title": "Beyond Localized Graph Neural Networks: An Attributed Motif   Regularization Framework",
        "text": "ry GNNs via mutual information maximization. Our training curriculum dynamically prioritizes multiple motifs in the learning process without relying on distributional assumptions in the underlying graph or the learning task. We integrate three state-of-the"
    },
    {
        "title": "Beyond Localized Graph Neural Networks: An Attributed Motif   Regularization Framework",
        "text": "-art GNNs in our framework, to show significant gains (3-10% accuracy) across six diverse, real-world datasets. We see stronger gains for nodes with sparse training labels and diverse attributes in local neighborhood structures."
    },
    {
        "title": "Explicitizing an Implicit Bias of the Frequency Principle in Two-layer   Neural Networks",
        "text": "It remains a puzzle that why deep neural networks (DNNs), with more parameters than samples, often generalize well. An attempt of understanding this puzzle is to discover implicit biases underlying the training process of DNNs, such as the Frequency Princi"
    },
    {
        "title": "Explicitizing an Implicit Bias of the Frequency Principle in Two-layer   Neural Networks",
        "text": "ple (F-Principle), i.e., DNNs often fit target functions from low to high frequencies. Inspired by the F-Principle, we propose an effective model of linear F-Principle (LFP) dynamics which accurately predicts the learning results of two-layer ReLU neural n"
    },
    {
        "title": "Explicitizing an Implicit Bias of the Frequency Principle in Two-layer   Neural Networks",
        "text": "etworks (NNs) of large widths. This LFP dynamics is rationalized by a linearized mean field residual dynamics of NNs. Importantly, the long-time limit solution of this LFP dynamics is equivalent to the solution of a constrained optimization problem explici"
    },
    {
        "title": "Explicitizing an Implicit Bias of the Frequency Principle in Two-layer   Neural Networks",
        "text": "tly minimizing an FP-norm, in which higher frequencies of feasible solutions are more heavily penalized. Using this optimization formulation, an a priori estimate of the generalization error bound is provided, revealing that a higher FP-norm of the target "
    },
    {
        "title": "Explicitizing an Implicit Bias of the Frequency Principle in Two-layer   Neural Networks",
        "text": "function increases the generalization error. Overall, by explicitizing the implicit bias of the F-Principle as an explicit penalty for two-layer NNs, our work makes a step towards a quantitative understanding of the learning and generalization of general D"
    },
    {
        "title": "Explicitizing an Implicit Bias of the Frequency Principle in Two-layer   Neural Networks",
        "text": "NNs."
    },
    {
        "title": "ImgSensingNet: UAV Vision Guided Aerial-Ground Air Quality Sensing   System",
        "text": "Given the increasingly serious air pollution problem, the monitoring of air quality index (AQI) in urban areas has drawn considerable attention. This paper presents ImgSensingNet, a vision guided aerial-ground sensing system, for fine-grained air quality m"
    },
    {
        "title": "ImgSensingNet: UAV Vision Guided Aerial-Ground Air Quality Sensing   System",
        "text": "onitoring and forecasting using the fusion of haze images taken by the unmanned-aerial-vehicle (UAV) and the AQI data collected by an on-ground three-dimensional (3D) wireless sensor network (WSN). Specifically, ImgSensingNet first leverages the computer v"
    },
    {
        "title": "ImgSensingNet: UAV Vision Guided Aerial-Ground Air Quality Sensing   System",
        "text": "ision technique to tell the AQI scale in different regions from the taken haze images, where haze-relevant features and a deep convolutional neural network (CNN) are designed for direct learning between haze images and corresponding AQI scale. Based on the"
    },
    {
        "title": "ImgSensingNet: UAV Vision Guided Aerial-Ground Air Quality Sensing   System",
        "text": " learnt AQI scale, ImgSensingNet determines whether to wake up on-ground wireless sensors for small-scale AQI monitoring and inference, which can greatly reduce the energy consumption of the system. An entropy-based model is employed for accurate real-time"
    },
    {
        "title": "ImgSensingNet: UAV Vision Guided Aerial-Ground Air Quality Sensing   System",
        "text": " AQI inference at unmeasured locations and future air quality distribution forecasting. We implement and evaluate ImgSensingNet on two university campuses since Feb. 2018, and has collected 17,630 photos and 2.6 millions of AQI data samples. Experimental r"
    },
    {
        "title": "ImgSensingNet: UAV Vision Guided Aerial-Ground Air Quality Sensing   System",
        "text": "esults confirm that ImgSensingNet can achieve higher inference accuracy while greatly reduce the energy consumption, compared to state-of-the-art AQI monitoring approaches."
    },
    {
        "title": "Kernel Density Estimation by Stagewise Algorithm with a Simple   Dictionary",
        "text": "This study proposes multivariate kernel density estimation by stagewise minimization algorithm based on $U$-divergence and a simple dictionary. The dictionary consists of an appropriate scalar bandwidth matrix and a part of the original data. The resulting"
    },
    {
        "title": "Kernel Density Estimation by Stagewise Algorithm with a Simple   Dictionary",
        "text": " estimator brings us data-adaptive weighting parameters and bandwidth matrices, and realizes a sparse representation of kernel density estimation. We develop the non-asymptotic error bound of estimator obtained via the proposed stagewise minimization algor"
    },
    {
        "title": "Kernel Density Estimation by Stagewise Algorithm with a Simple   Dictionary",
        "text": "ithm. It is confirmed from simulation studies that the proposed estimator performs competitive to or sometime better than other well-known density estimators."
    },
    {
        "title": "Convex Optimization on Functionals of Probability Densities",
        "text": "In information theory, some optimization problems result in convex optimization problems on strictly convex functionals of probability densities. In this note, we study these problems and show conditions of minimizers and the uniqueness of the minimizer if"
    },
    {
        "title": "Convex Optimization on Functionals of Probability Densities",
        "text": " there exist a minimizer."
    },
    {
        "title": "Regression with Uncertainty Quantification in Large Scale Complex Data",
        "text": "While several methods for predicting uncertainty on deep networks have been recently proposed, they do not readily translate to large and complex datasets. In this paper we utilize a simplified form of the Mixture Density Networks (MDNs) to produce a one-s"
    },
    {
        "title": "Regression with Uncertainty Quantification in Large Scale Complex Data",
        "text": "hot approach to quantify uncertainty in regression problems. We show that our uncertainty bounds are on-par or better than other reported existing methods. When applied to standard regression benchmark datasets, we show an improvement in predictive log-lik"
    },
    {
        "title": "Regression with Uncertainty Quantification in Large Scale Complex Data",
        "text": "elihood and root-mean-square-error when compared to existing state-of-the-art methods. We also demonstrate this method's efficacy on stochastic, highly volatile time-series data where stock prices are predicted for the next time interval. The resulting unc"
    },
    {
        "title": "Regression with Uncertainty Quantification in Large Scale Complex Data",
        "text": "ertainty graph summarizes significant anomalies in the stock price chart. Furthermore, we apply this method to the task of age estimation from the challenging IMDb-Wiki dataset of half a million face images. We successfully predict the uncertainties associ"
    },
    {
        "title": "Regression with Uncertainty Quantification in Large Scale Complex Data",
        "text": "ated with the prediction and empirically analyze the underlying causes of the uncertainties. This uncertainty quantification can be used to pre-process low quality datasets and further enable learning."
    },
    {
        "title": "A Latent Variable Approach to Gaussian Process Modeling with Qualitative   and Quantitative Factors",
        "text": "Computer simulations often involve both qualitative and numerical inputs. Existing Gaussian process (GP) methods for handling this mainly assume a different response surface for each combination of levels of the qualitative factors and relate them via a mu"
    },
    {
        "title": "A Latent Variable Approach to Gaussian Process Modeling with Qualitative   and Quantitative Factors",
        "text": "ltiresponse cross-covariance matrix. We introduce a substantially different approach that maps each qualitative factor to an underlying numerical latent variable (LV), with the mapped value for each level estimated similarly to the correlation parameters. "
    },
    {
        "title": "A Latent Variable Approach to Gaussian Process Modeling with Qualitative   and Quantitative Factors",
        "text": "This provides a parsimonious GP parameterization that treats qualitative factors the same as numerical variables and views them as effecting the response via similar physical mechanisms. This has strong physical justification, as the effects of a qualitati"
    },
    {
        "title": "A Latent Variable Approach to Gaussian Process Modeling with Qualitative   and Quantitative Factors",
        "text": "ve factor in any physics-based simulation model must always be due to some underlying numerical variables. Even when the underlying variables are many, sufficient dimension reduction arguments imply that their effects can be represented by a low-dimensiona"
    },
    {
        "title": "A Latent Variable Approach to Gaussian Process Modeling with Qualitative   and Quantitative Factors",
        "text": "l LV. This conjecture is supported by the superior predictive performance observed across a variety of examples. Moreover, the mapped LVs provide substantial insight into the nature and effects of the qualitative factors."
    },
    {
        "title": "Gradient Hyperalignment for multi-subject fMRI data alignment",
        "text": "Multi-subject fMRI data analysis is an interesting and challenging problem in human brain decoding studies. The inherent anatomical and functional variability across subjects make it necessary to do both anatomical and functional alignment before classific"
    },
    {
        "title": "Gradient Hyperalignment for multi-subject fMRI data alignment",
        "text": "ation analysis. Besides, when it comes to big data, time complexity becomes a problem that cannot be ignored. This paper proposes Gradient Hyperalignment (Gradient-HA) as a gradient-based functional alignment method that is suitable for multi-subject fMRI "
    },
    {
        "title": "Gradient Hyperalignment for multi-subject fMRI data alignment",
        "text": "datasets with large amounts of samples and voxels. The advantage of Gradient-HA is that it can solve independence and high dimension problems by using Independent Component Analysis (ICA) and Stochastic Gradient Ascent (SGA). Validation using multi-classif"
    },
    {
        "title": "Gradient Hyperalignment for multi-subject fMRI data alignment",
        "text": "ication tasks on big data demonstrates that Gradient-HA method has less time complexity and better or comparable performance compared with other state-of-the-art functional alignment methods."
    },
    {
        "title": "Revisiting Paraphrase Question Generator using Pairwise Discriminator",
        "text": "In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of securing word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in "
    },
    {
        "title": "Revisiting Paraphrase Question Generator using Pairwise Discriminator",
        "text": "the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating paraphrase, we would like the generated paraphrase to be semantically close to the original sentence. One way to ensure this is by adding con"
    },
    {
        "title": "Revisiting Paraphrase Question Generator using Pairwise Discriminator",
        "text": "straints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the encoder that is trained with a suitable loss fu"
    },
    {
        "title": "Revisiting Paraphrase Question Generator using Pairwise Discriminator",
        "text": "nction. Our loss function penalizes paraphrase sentence embedding distances from being too large. This loss is used in combination with a sequential encoder-decoder network. We also validated our method by evaluating the obtained embeddings for a sentiment"
    },
    {
        "title": "Revisiting Paraphrase Question Generator using Pairwise Discriminator",
        "text": " analysis task. The proposed method results in semantic embeddings and outperforms the state-of-the-art on the paraphrase generation and sentiment analysis task on standard datasets. These results are also shown to be statistically significant."
    },
    {
        "title": "Self-Supervised Bug Detection and Repair",
        "text": "Machine learning-based program analyses have recently shown the promise of integrating formal and probabilistic reasoning towards aiding software development. However, in the absence of large annotated corpora, training these analyses is challenging. Towar"
    },
    {
        "title": "Self-Supervised Bug Detection and Repair",
        "text": "ds addressing this, we present BugLab, an approach for self-supervised learning of bug detection and repair. BugLab co-trains two models: (1) a detector model that learns to detect and repair bugs in code, (2) a selector model that learns to create buggy c"
    },
    {
        "title": "Self-Supervised Bug Detection and Repair",
        "text": "ode for the detector to use as training data. A Python implementation of BugLab improves by up to 30% upon baseline methods on a test dataset of 2374 real-life bugs and finds 19 previously unknown bugs in open-source software."
    },
    {
        "title": "Relational representation learning with spike trains",
        "text": "Relational representation learning has lately received an increase in interest due to its flexibility in modeling a variety of systems like interacting particles, materials and industrial projects for, e.g., the design of spacecraft. A prominent method for"
    },
    {
        "title": "Relational representation learning with spike trains",
        "text": " dealing with relational data are knowledge graph embedding algorithms, where entities and relations of a knowledge graph are mapped to a low-dimensional vector space while preserving its semantic structure. Recently, a graph embedding method has been prop"
    },
    {
        "title": "Relational representation learning with spike trains",
        "text": "osed that maps graph elements to the temporal domain of spiking neural networks. However, it relies on encoding graph elements through populations of neurons that only spike once. Here, we present a model that allows us to learn spike train-based embedding"
    },
    {
        "title": "Relational representation learning with spike trains",
        "text": "s of knowledge graphs, requiring only one neuron per graph element by fully utilizing the temporal domain of spike patterns. This coding scheme can be implemented with arbitrary spiking neuron models as long as gradients with respect to spike times can be "
    },
    {
        "title": "Relational representation learning with spike trains",
        "text": "calculated, which we demonstrate for the integrate-and-fire neuron model. In general, the presented results show how relational knowledge can be integrated into spike-based systems, opening up the possibility of merging event-based computing and relational"
    },
    {
        "title": "Relational representation learning with spike trains",
        "text": " data to build powerful and energy efficient artificial intelligence applications and reasoning systems."
    },
    {
        "title": "TinyRadarNN: Combining Spatial and Temporal Convolutional Neural   Networks for Embedded Gesture Recognition with Short Range Radars",
        "text": "This work proposes a low-power high-accuracy embedded hand-gesture recognition algorithm targeting battery-operated wearable devices using low power short-range RADAR sensors. A 2D Convolutional Neural Network (CNN) using range frequency Doppler features i"
    },
    {
        "title": "TinyRadarNN: Combining Spatial and Temporal Convolutional Neural   Networks for Embedded Gesture Recognition with Short Range Radars",
        "text": "s combined with a Temporal Convolutional Neural Network (TCN) for time sequence prediction. The final algorithm has a model size of only 46 thousand parameters, yielding a memory footprint of only 92 KB. Two datasets containing 11 challenging hand gestures"
    },
    {
        "title": "TinyRadarNN: Combining Spatial and Temporal Convolutional Neural   Networks for Embedded Gesture Recognition with Short Range Radars",
        "text": " performed by 26 different people have been recorded containing a total of 20,210 gesture instances. On the 11 hand gesture dataset, accuracies of 86.6% (26 users) and 92.4% (single user) have been achieved, which are comparable to the state-of-the-art, wh"
    },
    {
        "title": "TinyRadarNN: Combining Spatial and Temporal Convolutional Neural   Networks for Embedded Gesture Recognition with Short Range Radars",
        "text": "ich achieves 87% (10 users) and 94% (single user), while using a TCN-based network that is 7500x smaller than the state-of-the-art. Furthermore, the gesture recognition classifier has been implemented on a Parallel Ultra-Low Power Processor, demonstrating "
    },
    {
        "title": "TinyRadarNN: Combining Spatial and Temporal Convolutional Neural   Networks for Embedded Gesture Recognition with Short Range Radars",
        "text": "that real-time prediction is feasible with only 21 mW of power consumption for the full TCN sequence prediction network, while a system-level power consumption of less than 100 mW is achieved. We provide open-source access to all the code and data collecte"
    },
    {
        "title": "TinyRadarNN: Combining Spatial and Temporal Convolutional Neural   Networks for Embedded Gesture Recognition with Short Range Radars",
        "text": "d and used in this work on tinyradar.ethz.ch."
    },
    {
        "title": "Connectivity Learning in Multi-Branch Networks",
        "text": "While much of the work in the design of convolutional networks over the last five years has revolved around the empirical investigation of the importance of depth, filter sizes, and number of feature channels, recent studies have shown that branching, i.e."
    },
    {
        "title": "Connectivity Learning in Multi-Branch Networks",
        "text": ", splitting the computation along parallel but distinct threads and then aggregating their outputs, represents a new promising dimension for significant improvements in performance. To combat the complexity of design choices in multi-branch architectures, "
    },
    {
        "title": "Connectivity Learning in Multi-Branch Networks",
        "text": "prior work has adopted simple strategies, such as a fixed branching factor, the same input being fed to all parallel branches, and an additive combination of the outputs produced by all branches at aggregation points.   In this work we remove these predefi"
    },
    {
        "title": "Connectivity Learning in Multi-Branch Networks",
        "text": "ned choices and propose an algorithm to learn the connections between branches in the network. Instead of being chosen a priori by the human designer, the multi-branch connectivity is learned simultaneously with the weights of the network by optimizing a s"
    },
    {
        "title": "Connectivity Learning in Multi-Branch Networks",
        "text": "ingle loss function defined with respect to the end task. We demonstrate our approach on the problem of multi-class image classification using three different datasets where it yields consistently higher accuracy compared to the state-of-the-art \"ResNeXt\" "
    },
    {
        "title": "Connectivity Learning in Multi-Branch Networks",
        "text": "multi-branch network given the same learning capacity."
    },
    {
        "title": "Clustered Hierarchical Anomaly and Outlier Detection Algorithms",
        "text": "Anomaly and outlier detection is a long-standing problem in machine learning. In some cases, anomaly detection is easy, such as when data are drawn from well-characterized distributions such as the Gaussian. However, when data occupy high-dimensional space"
    },
    {
        "title": "Clustered Hierarchical Anomaly and Outlier Detection Algorithms",
        "text": "s, anomaly detection becomes more difficult. We present CLAM (Clustered Learning of Approximate Manifolds), a manifold mapping technique in any metric space. CLAM begins with a fast hierarchical clustering technique and then induces a graph from the cluste"
    },
    {
        "title": "Clustered Hierarchical Anomaly and Outlier Detection Algorithms",
        "text": "r tree, based on overlapping clusters as selected using several geometric and topological features. Using these graphs, we implement CHAODA (Clustered Hierarchical Anomaly and Outlier Detection Algorithms), exploring various properties of the graphs and th"
    },
    {
        "title": "Clustered Hierarchical Anomaly and Outlier Detection Algorithms",
        "text": "eir constituent clusters to find outliers. CHAODA employs a form of transfer learning based on a training set of datasets, and applies this knowledge to a separate test set of datasets of different cardinalities, dimensionalities, and domains. On 24 public"
    },
    {
        "title": "Clustered Hierarchical Anomaly and Outlier Detection Algorithms",
        "text": "ly available datasets, we compare CHAODA (by measure of ROC AUC) to a variety of state-of-the-art unsupervised anomaly-detection algorithms. Six of the datasets are used for training. CHAODA outperforms other approaches on 16 of the remaining 18 datasets. "
    },
    {
        "title": "Clustered Hierarchical Anomaly and Outlier Detection Algorithms",
        "text": "CLAM and CHAODA scale to large, high-dimensional \"big data\" anomaly-detection problems, and generalize across datasets and distance functions. Source code to CLAM and CHAODA are freely available on GitHub at https://github.com/URI-ABD/clam."
    },
    {
        "title": "Federated Learning for UAV Swarms Under Class Imbalance and Power   Consumption Constraints",
        "text": "The usage of unmanned aerial vehicles (UAVs) in civil and military applications continues to increase due to the numerous advantages that they provide over conventional approaches. Despite the abundance of such advantages, it is imperative to investigate t"
    },
    {
        "title": "Federated Learning for UAV Swarms Under Class Imbalance and Power   Consumption Constraints",
        "text": "he performance of UAV utilization while considering their design limitations. This paper investigates the deployment of UAV swarms when each UAV carries a machine learning classification task. To avoid data exchange with ground-based processing nodes, a fe"
    },
    {
        "title": "Federated Learning for UAV Swarms Under Class Imbalance and Power   Consumption Constraints",
        "text": "derated learning approach is adopted between a UAV leader and the swarm members to improve the local learning model while avoiding excessive air-to-ground and ground-to-air communications. Moreover, the proposed deployment framework considers the stringent"
    },
    {
        "title": "Federated Learning for UAV Swarms Under Class Imbalance and Power   Consumption Constraints",
        "text": " energy constraints of UAVs and the problem of class imbalance, where we show that considering these design parameters significantly improves the performances of the UAV swarm in terms of classification accuracy, energy consumption and availability of UAVs"
    },
    {
        "title": "Federated Learning for UAV Swarms Under Class Imbalance and Power   Consumption Constraints",
        "text": " when compared with several baseline algorithms."
    },
    {
        "title": "Deep Representation for Connected Health: Semi-supervised Learning for   Analysing the Risk of Urinary Tract Infections in People with Dementia",
        "text": "Machine learning techniques combined with in-home monitoring technologies provide a unique opportunity to automate diagnosis and early detection of adverse health conditions in long-term conditions such as dementia. However, accessing sufficient labelled t"
    },
    {
        "title": "Deep Representation for Connected Health: Semi-supervised Learning for   Analysing the Risk of Urinary Tract Infections in People with Dementia",
        "text": "raining samples and integrating high-quality, routinely collected data from heterogeneous in-home monitoring technologies are main obstacles hindered utilising these technologies in real-world medicine. This work presents a semi-supervised model that can c"
    },
    {
        "title": "Deep Representation for Connected Health: Semi-supervised Learning for   Analysing the Risk of Urinary Tract Infections in People with Dementia",
        "text": "ontinuously learn from routinely collected in-home observation and measurement data. We show how our model can process highly imbalanced and dynamic data to make robust predictions in analysing the risk of Urinary Tract Infections (UTIs) in dementia. UTIs "
    },
    {
        "title": "Deep Representation for Connected Health: Semi-supervised Learning for   Analysing the Risk of Urinary Tract Infections in People with Dementia",
        "text": "are common in older adults and constitute one of the main causes of avoidable hospital admissions in people with dementia (PwD). Health-related conditions, such as UTI, have a lower prevalence in individuals, which classifies them as sporadic cases (i.e. r"
    },
    {
        "title": "Deep Representation for Connected Health: Semi-supervised Learning for   Analysing the Risk of Urinary Tract Infections in People with Dementia",
        "text": "are or scattered, yet important events). This limits the access to sufficient training data, without which the supervised learning models risk becoming overfitted or biased. We introduce a probabilistic semi-supervised learning framework to address these i"
    },
    {
        "title": "Deep Representation for Connected Health: Semi-supervised Learning for   Analysing the Risk of Urinary Tract Infections in People with Dementia",
        "text": "ssues. The proposed method produces a risk analysis score for UTIs using routinely collected data by in-home sensing technologies."
    },
    {
        "title": "The Role of \"Live\" in Livestreaming Markets: Evidence Using Orthogonal   Random Forest",
        "text": "The common belief about the growing medium of livestreaming is that its value lies in its \"live\" component. In this paper, we leverage data from a large livestreaming platform to examine this belief. We are able to do this as this platform also allows view"
    },
    {
        "title": "The Role of \"Live\" in Livestreaming Markets: Evidence Using Orthogonal   Random Forest",
        "text": "ers to purchase the recorded version of the livestream. We summarize the value of livestreaming content by estimating how demand responds to price before, on the day of, and after the livestream. We do this by proposing a generalized Orthogonal Random Fore"
    },
    {
        "title": "The Role of \"Live\" in Livestreaming Markets: Evidence Using Orthogonal   Random Forest",
        "text": "st framework. This framework allows us to estimate heterogeneous treatment effects in the presence of high-dimensional confounders whose relationships with the treatment policy (i.e., price) are complex but partially known. We find significant dynamics in "
    },
    {
        "title": "The Role of \"Live\" in Livestreaming Markets: Evidence Using Orthogonal   Random Forest",
        "text": "the price elasticity of demand over the temporal distance to the scheduled livestreaming day and after. Specifically, demand gradually becomes less price sensitive over time to the livestreaming day and is inelastic on the livestreaming day. Over the post-"
    },
    {
        "title": "The Role of \"Live\" in Livestreaming Markets: Evidence Using Orthogonal   Random Forest",
        "text": "livestream period, demand is still sensitive to price, but much less than the pre-livestream period. This indicates that the vlaue of livestreaming persists beyond the live component. Finally, we provide suggestive evidence for the likely mechanisms drivin"
    },
    {
        "title": "The Role of \"Live\" in Livestreaming Markets: Evidence Using Orthogonal   Random Forest",
        "text": "g our results. These are quality uncertainty reduction for the patterns pre- and post-livestream and the potential of real-time interaction with the creator on the day of the livestream."
    },
    {
        "title": "A note on the sample complexity of the Er-SpUD algorithm by Spielman,   Wang and Wright for exact recovery of sparsely used dictionaries",
        "text": "We consider the problem of recovering an invertible $n \\times n$ matrix $A$ and a sparse $n \\times p$ random matrix $X$ based on the observation of $Y = AX$ (up to a scaling and permutation of columns of $A$ and rows of $X$). Using only elementary tools fr"
    },
    {
        "title": "A note on the sample complexity of the Er-SpUD algorithm by Spielman,   Wang and Wright for exact recovery of sparsely used dictionaries",
        "text": "om the theory of empirical processes we show that a version of the Er-SpUD algorithm by Spielman, Wang and Wright with high probability recovers $A$ and $X$ exactly, provided that $p \\ge Cn\\log n$, which is optimal up to the constant $C$."
    },
    {
        "title": "Mixed Precision DNNs: All you need is a good parametrization",
        "text": "Efficient deep neural network (DNN) inference on mobile or embedded devices typically involves quantization of the network parameters and activations. In particular, mixed precision networks achieve better performance than networks with homogeneous bitwidt"
    },
    {
        "title": "Mixed Precision DNNs: All you need is a good parametrization",
        "text": "h for the same size constraint. Since choosing the optimal bitwidths is not straight forward, training methods, which can learn them, are desirable. Differentiable quantization with straight-through gradients allows to learn the quantizer's parameters usin"
    },
    {
        "title": "Mixed Precision DNNs: All you need is a good parametrization",
        "text": "g gradient methods. We show that a suited parametrization of the quantizer is the key to achieve a stable training and a good final performance. Specifically, we propose to parametrize the quantizer with the step size and dynamic range. The bitwidth can th"
    },
    {
        "title": "Mixed Precision DNNs: All you need is a good parametrization",
        "text": "en be inferred from them. Other parametrizations, which explicitly use the bitwidth, consistently perform worse. We confirm our findings with experiments on CIFAR-10 and ImageNet and we obtain mixed precision DNNs with learned quantization parameters, achi"
    },
    {
        "title": "Mixed Precision DNNs: All you need is a good parametrization",
        "text": "eving state-of-the-art performance."
    },
    {
        "title": "Maximum Entropy competes with Maximum Likelihood",
        "text": "Maximum entropy (MAXENT) method has a large number of applications in theoretical and applied machine learning, since it provides a convenient non-parametric tool for estimating unknown probabilities. The method is a major contribution of statistical physi"
    },
    {
        "title": "Maximum Entropy competes with Maximum Likelihood",
        "text": "cs to probabilistic inference. However, a systematic approach towards its validity limits is currently missing. Here we study MAXENT in a Bayesian decision theory set-up, i.e. assuming that there exists a well-defined prior Dirichlet density for unknown pr"
    },
    {
        "title": "Maximum Entropy competes with Maximum Likelihood",
        "text": "obabilities, and that the average Kullback-Leibler (KL) distance can be employed for deciding on the quality and applicability of various estimators. These allow to evaluate the relevance of various MAXENT constraints, check its general applicability, and "
    },
    {
        "title": "Maximum Entropy competes with Maximum Likelihood",
        "text": "compare MAXENT with estimators having various degrees of dependence on the prior, viz. the regularized maximum likelihood (ML) and the Bayesian estimators. We show that MAXENT applies in sparse data regimes, but needs specific types of prior information. I"
    },
    {
        "title": "Maximum Entropy competes with Maximum Likelihood",
        "text": "n particular, MAXENT can outperform the optimally regularized ML provided that there are prior rank correlations between the estimated random quantity and its probabilities."
    },
    {
        "title": "Routing Networks: Adaptive Selection of Non-linear Functions for   Multi-Task Learning",
        "text": "Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a nove"
    },
    {
        "title": "Routing Networks: Adaptive Selection of Non-linear Functions for   Multi-Task Learning",
        "text": "l neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network - for example a fully-connect"
    },
    {
        "title": "Routing Networks: Adaptive Selection of Non-linear Functions for   Multi-Task Learning",
        "text": "ed or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network "
    },
    {
        "title": "Routing Networks: Adaptive Selection of Non-linear Functions for   Multi-Task Learning",
        "text": "dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-laye"
    },
    {
        "title": "Routing Networks: Adaptive Selection of Non-linear Functions for   Multi-Task Learning",
        "text": "r baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost w"
    },
    {
        "title": "Routing Networks: Adaptive Selection of Non-linear Functions for   Multi-Task Learning",
        "text": "hile cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we obtain cross-stitch performance levels with an 85% reduction in training time."
    },
    {
        "title": "Task-group Relatedness and Generalization Bounds for Regularized   Multi-task Learning",
        "text": "In this paper, we study the generalization performance of regularized multi-task learning (RMTL) in a vector-valued framework, where MTL is considered as a learning process for vector-valued functions. We are mainly concerned with two theoretical questions"
    },
    {
        "title": "Task-group Relatedness and Generalization Bounds for Regularized   Multi-task Learning",
        "text": ": 1) under what conditions does RMTL perform better with a smaller task sample size than STL? 2) under what conditions is RMTL generalizable and can guarantee the consistency of each task during simultaneous learning?   In particular, we investigate two ty"
    },
    {
        "title": "Task-group Relatedness and Generalization Bounds for Regularized   Multi-task Learning",
        "text": "pes of task-group relatedness: the observed discrepancy-dependence measure (ODDM) and the empirical discrepancy-dependence measure (EDDM), both of which detect the dependence between two groups of multiple related tasks (MRTs). We then introduce the Cartes"
    },
    {
        "title": "Task-group Relatedness and Generalization Bounds for Regularized   Multi-task Learning",
        "text": "ian product-based uniform entropy number (CPUEN) to measure the complexities of vector-valued function classes. By applying the specific deviation and the symmetrization inequalities to the vector-valued framework, we obtain the generalization bound for RM"
    },
    {
        "title": "Task-group Relatedness and Generalization Bounds for Regularized   Multi-task Learning",
        "text": "TL, which is the upper bound of the joint probability of the event that there is at least one task with a large empirical discrepancy between the expected and empirical risks. Finally, we present a sufficient condition to guarantee the consistency of each "
    },
    {
        "title": "Task-group Relatedness and Generalization Bounds for Regularized   Multi-task Learning",
        "text": "task in the simultaneous learning process, and we discuss how task relatedness affects the generalization performance of RMTL. Our theoretical findings answer the aforementioned two questions."
    },
    {
        "title": "Intelligent Credit Limit Management in Consumer Loans Based on Causal   Inference",
        "text": "Nowadays consumer loan plays an important role in promoting the economic growth, and credit cards are the most popular consumer loan. One of the most essential parts in credit cards is the credit limit management. Traditionally, credit limits are adjusted "
    },
    {
        "title": "Intelligent Credit Limit Management in Consumer Loans Based on Causal   Inference",
        "text": "based on limited heuristic strategies, which are developed by experienced professionals. In this paper, we present a data-driven approach to manage the credit limit intelligently. Firstly, a conditional independence testing is conducted to acquire the data"
    },
    {
        "title": "Intelligent Credit Limit Management in Consumer Loans Based on Causal   Inference",
        "text": " for building models. Based on these testing data, a response model is then built to measure the heterogeneous treatment effect of increasing credit limits (i.e. treatments) for different customers, who are depicted by several control variables (i.e. featu"
    },
    {
        "title": "Intelligent Credit Limit Management in Consumer Loans Based on Causal   Inference",
        "text": "res). In order to incorporate the diminishing marginal effect, a carefully selected log transformation is introduced to the treatment variable. Moreover, the model's capability can be further enhanced by applying a non-linear transformation on features via"
    },
    {
        "title": "Intelligent Credit Limit Management in Consumer Loans Based on Causal   Inference",
        "text": " GBDT encoding. Finally, a well-designed metric is proposed to properly measure the performances of compared methods. The experimental results demonstrate the effectiveness of the proposed approach."
    },
    {
        "title": "Uncertainty Quantification Using Neural Networks for Molecular Property   Prediction",
        "text": "Uncertainty quantification (UQ) is an important component of molecular property prediction, particularly for drug discovery applications where model predictions direct experimental design and where unanticipated imprecision wastes valuable time and resourc"
    },
    {
        "title": "Uncertainty Quantification Using Neural Networks for Molecular Property   Prediction",
        "text": "es. The need for UQ is especially acute for neural models, which are becoming increasingly standard yet are challenging to interpret. While several approaches to UQ have been proposed in the literature, there is no clear consensus on the comparative perfor"
    },
    {
        "title": "Uncertainty Quantification Using Neural Networks for Molecular Property   Prediction",
        "text": "mance of these models. In this paper, we study this question in the context of regression tasks. We systematically evaluate several methods on five benchmark datasets using multiple complementary performance metrics. Our experiments show that none of the m"
    },
    {
        "title": "Uncertainty Quantification Using Neural Networks for Molecular Property   Prediction",
        "text": "ethods we tested is unequivocally superior to all others, and none produces a particularly reliable ranking of errors across multiple datasets. While we believe these results show that existing UQ methods are not sufficient for all common use-cases and dem"
    },
    {
        "title": "Uncertainty Quantification Using Neural Networks for Molecular Property   Prediction",
        "text": "onstrate the benefits of further research, we conclude with a practical recommendation as to which existing techniques seem to perform well relative to others."
    },
    {
        "title": "SpinNet: Learning a General Surface Descriptor for 3D Point Cloud   Registration",
        "text": "Extracting robust and general 3D local features is key to downstream tasks such as point cloud registration and reconstruction. Existing learning-based local descriptors are either sensitive to rotation transformations, or rely on classical handcrafted fea"
    },
    {
        "title": "SpinNet: Learning a General Surface Descriptor for 3D Point Cloud   Registration",
        "text": "tures which are neither general nor representative. In this paper, we introduce a new, yet conceptually simple, neural architecture, termed SpinNet, to extract local features which are rotationally invariant whilst sufficiently informative to enable accura"
    },
    {
        "title": "SpinNet: Learning a General Surface Descriptor for 3D Point Cloud   Registration",
        "text": "te registration. A Spatial Point Transformer is first introduced to map the input local surface into a carefully designed cylindrical space, enabling end-to-end optimization with SO(2) equivariant representation. A Neural Feature Extractor which leverages "
    },
    {
        "title": "SpinNet: Learning a General Surface Descriptor for 3D Point Cloud   Registration",
        "text": "the powerful point-based and 3D cylindrical convolutional neural layers is then utilized to derive a compact and representative descriptor for matching. Extensive experiments on both indoor and outdoor datasets demonstrate that SpinNet outperforms existing"
    },
    {
        "title": "SpinNet: Learning a General Surface Descriptor for 3D Point Cloud   Registration",
        "text": " state-of-the-art techniques by a large margin. More critically, it has the best generalization ability across unseen scenarios with different sensor modalities. The code is available at https://github.com/QingyongHu/SpinNet."
    },
    {
        "title": "Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive   Patterns in Vowel Acoustics",
        "text": "Most of the previous approaches to lyrics-to-audio alignment used a pre-developed automatic speech recognition (ASR) system that innately suffered from several difficulties to adapt the speech model to individual singers. A significant aspect missing in pr"
    },
    {
        "title": "Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive   Patterns in Vowel Acoustics",
        "text": "evious works is the self-learnability of repetitive vowel patterns in the singing voice, where the vowel part used is more consistent than the consonant part. Based on this, our system first learns a discriminative subspace of vowel sequences, based on wei"
    },
    {
        "title": "Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive   Patterns in Vowel Acoustics",
        "text": "ghted symmetric non-negative matrix factorization (WS-NMF), by taking the self-similarity of a standard acoustic feature as an input. Then, we make use of canonical time warping (CTW), derived from a recent computer vision technique, to find an optimal spa"
    },
    {
        "title": "Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive   Patterns in Vowel Acoustics",
        "text": "tiotemporal transformation between the text and the acoustic sequences. Experiments with Korean and English data sets showed that deploying this method after a pre-developed, unsupervised, singing source separation achieved more promising results than othe"
    },
    {
        "title": "Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive   Patterns in Vowel Acoustics",
        "text": "r state-of-the-art unsupervised approaches and an existing ASR-based system."
    },
    {
        "title": "Modularity in Query-Based Concept Learning",
        "text": "We define and study the problem of modular concept learning, that is, learning a concept that is a cross product of component concepts. If an element's membership in a concept depends solely on it's membership in the components, learning the concept as a w"
    },
    {
        "title": "Modularity in Query-Based Concept Learning",
        "text": "hole can be reduced to learning the components. We analyze this problem with respect to different types of oracle interfaces, defining different sets of queries. If a given oracle interface cannot answer questions about the components, learning can be diff"
    },
    {
        "title": "Modularity in Query-Based Concept Learning",
        "text": "icult, even when the components are easy to learn with the same type of oracle queries. While learning from superset queries is easy, learning from membership, equivalence, or subset queries is harder. However, we show that these problems become tractable "
    },
    {
        "title": "Modularity in Query-Based Concept Learning",
        "text": "when oracles are given a positive example and are allowed to ask membership queries."
    },
    {
        "title": "MLPs to Find Extrema of Functionals",
        "text": "Multilayer perceptron (MLP) is a class of networks composed of multiple layers of perceptrons, and it is essentially a mathematical function. Based on MLP, we develop a new numerical method to find the extrema of functionals. As demonstrations, we present "
    },
    {
        "title": "MLPs to Find Extrema of Functionals",
        "text": "our solutions in three physic scenes. Ideally, the same method is applicable to any cases where the objective curve/surface can be fitted by second-order differentiable functions. This method can also be extended to cases where there are a finite number of"
    },
    {
        "title": "MLPs to Find Extrema of Functionals",
        "text": " non-differentiable (but continuous) points/surfaces."
    },
    {
        "title": "Modeling Images using Transformed Indian Buffet Processes",
        "text": "Latent feature models are attractive for image modeling, since images generally contain multiple objects. However, many latent feature models ignore that objects can appear at different locations or require pre-segmentation of images. While the transformed"
    },
    {
        "title": "Modeling Images using Transformed Indian Buffet Processes",
        "text": " Indian buffet process (tIBP) provides a method for modeling transformation-invariant features in unsegmented binary images, its current form is inappropriate for real images because of its computational cost and modeling assumptions. We combine the tIBP w"
    },
    {
        "title": "Modeling Images using Transformed Indian Buffet Processes",
        "text": "ith likelihoods appropriate for real images and develop an efficient inference, using the cross-correlation between images and features, that is theoretically and empirically faster than existing inference techniques. Our method discovers reasonable compon"
    },
    {
        "title": "Modeling Images using Transformed Indian Buffet Processes",
        "text": "ents and achieve effective image reconstruction in natural images."
    },
    {
        "title": "Nearly Minimax Optimal Reinforcement Learning with Linear Function Approximation",
        "text": "We study reinforcement learning with linear function approximation where the transition probability and reward functions are linear with respect to a feature mapping $\\boldsymbol{\\phi}(s,a)$. Specifically, we consider the episodic inhomogeneous linear Mark"
    },
    {
        "title": "Nearly Minimax Optimal Reinforcement Learning with Linear Function Approximation",
        "text": "ov Decision Process (MDP), and propose a novel computation-efficient algorithm, LSVI-UCB$^+$, which achieves an $\\widetilde{O}(Hd\\sqrt{T})$ regret bound where $H$ is the episode length, $d$ is the feature dimension, and $T$ is the number of steps. LSVI-UCB"
    },
    {
        "title": "Nearly Minimax Optimal Reinforcement Learning with Linear Function Approximation",
        "text": "$^+$ builds on weighted ridge regression and upper confidence value iteration with a Bernstein-type exploration bonus. Our statistical results are obtained with novel analytical tools, including a new Bernstein self-normalized bound with conservatism on el"
    },
    {
        "title": "Nearly Minimax Optimal Reinforcement Learning with Linear Function Approximation",
        "text": "liptical potentials, and refined analysis of the correction term. To the best of our knowledge, this is the first minimax optimal algorithm for linear MDPs up to logarithmic factors, which closes the $\\sqrt{Hd}$ gap between the best known upper bound of $\\"
    },
    {
        "title": "Nearly Minimax Optimal Reinforcement Learning with Linear Function Approximation",
        "text": "widetilde{O}(\\sqrt{H^3d^3T})$ in \\cite{jin2020provably} and lower bound of $\\Omega(Hd\\sqrt{T})$ for linear MDPs."
    },
    {
        "title": "Which Minimizer Does My Neural Network Converge To?",
        "text": "The loss surface of an overparameterized neural network (NN) possesses many global minima of zero training error. We explain how common variants of the standard NN training procedure change the minimizer obtained. First, we make explicit how the size of th"
    },
    {
        "title": "Which Minimizer Does My Neural Network Converge To?",
        "text": "e initialization of a strongly overparameterized NN affects the minimizer and can deteriorate its final test performance. We propose a strategy to limit this effect. Then, we demonstrate that for adaptive optimization such as AdaGrad, the obtained minimize"
    },
    {
        "title": "Which Minimizer Does My Neural Network Converge To?",
        "text": "r generally differs from the gradient descent (GD) minimizer. This adaptive minimizer is changed further by stochastic mini-batch training, even though in the non-adaptive case GD and stochastic GD result in essentially the same minimizer. Lastly, we expla"
    },
    {
        "title": "Which Minimizer Does My Neural Network Converge To?",
        "text": "in that these effects remain relevant for less overparameterized NNs. While overparameterization has its benefits, our work highlights that it induces sources of error absent from underparameterized models, some of which can be challenging to control."
    },
    {
        "title": "Intention-Based Lane Changing and Lane Keeping Haptic Guidance Steering   System",
        "text": "Haptic guidance in a shared steering assistance system has drawn significant attention in intelligent vehicle fields, owing to its mutual communication ability for vehicle control. By exerting continuous torque on the steering wheel, both the driver and su"
    },
    {
        "title": "Intention-Based Lane Changing and Lane Keeping Haptic Guidance Steering   System",
        "text": "pport system can share lateral control of the vehicle. However, current haptic guidance steering systems demonstrate some deficiencies in assisting lane changing. This study explored a new steering interaction method, including the design and evaluation of"
    },
    {
        "title": "Intention-Based Lane Changing and Lane Keeping Haptic Guidance Steering   System",
        "text": " an intention-based haptic shared steering system. Such an intention-based method can support both lane keeping and lane changing assistance, by detecting a driver lane change intention. By using a deep learning-based method to model a driver decision timi"
    },
    {
        "title": "Intention-Based Lane Changing and Lane Keeping Haptic Guidance Steering   System",
        "text": "ng regarding lane crossing, an adaptive gain control method was proposed for realizing a steering control system. An intention consistency method was proposed to detect whether the driver and the system were acting towards the same target trajectories and "
    },
    {
        "title": "Intention-Based Lane Changing and Lane Keeping Haptic Guidance Steering   System",
        "text": "to accurately capture the driver intention. A driving simulator experiment was conducted to test the system performance. Participants were required to perform six trials with assistive methods and one trial without assistance. The results demonstrated that"
    },
    {
        "title": "Intention-Based Lane Changing and Lane Keeping Haptic Guidance Steering   System",
        "text": " the supporting system decreased the lane departure risk in the lane keeping tasks and could support a fast and stable lane changing maneuver."
    },
    {
        "title": "Evaluating Nonlinear Decision Trees for Binary Classification Tasks with   Other Existing Methods",
        "text": "Classification of datasets into two or more distinct classes is an important machine learning task. Many methods are able to classify binary classification tasks with a very high accuracy on test data, but cannot provide any easily interpretable explanatio"
    },
    {
        "title": "Evaluating Nonlinear Decision Trees for Binary Classification Tasks with   Other Existing Methods",
        "text": "n for users to have a deeper understanding of reasons for the split of data into two classes. In this paper, we highlight and evaluate a recently proposed nonlinear decision tree approach with a number of commonly used classification methods on a number of"
    },
    {
        "title": "Evaluating Nonlinear Decision Trees for Binary Classification Tasks with   Other Existing Methods",
        "text": " datasets involving a few to a large number of features. The study reveals key issues such as effect of classification on the method's parameter values, complexity of the classifier versus achieved accuracy, and interpretability of resulting classifiers."
    },
    {
        "title": "The PRIMPing Routine -- Tiling through Proximal Alternating Linearized   Minimization",
        "text": "Mining and exploring databases should provide users with knowledge and new insights. Tiles of data strive to unveil true underlying structure and distinguish valuable information from various kinds of noise. We propose a novel Boolean matrix factorization "
    },
    {
        "title": "The PRIMPing Routine -- Tiling through Proximal Alternating Linearized   Minimization",
        "text": "algorithm to solve the tiling problem, based on recent results from optimization theory. In contrast to existing work, the new algorithm minimizes the description length of the resulting factorization. This approach is well known for model selection and da"
    },
    {
        "title": "The PRIMPing Routine -- Tiling through Proximal Alternating Linearized   Minimization",
        "text": "ta compression, but not for finding suitable factorizations via numerical optimization. We demonstrate the superior robustness of the new approach in the presence of several kinds of noise and types of underlying structure. Moreover, our general framework "
    },
    {
        "title": "The PRIMPing Routine -- Tiling through Proximal Alternating Linearized   Minimization",
        "text": "can work with any cost measure having a suitable real-valued relaxation. Thereby, no convexity assumptions have to be met. The experimental results on synthetic data and image data show that the new method identifies interpretable patterns which explain th"
    },
    {
        "title": "The PRIMPing Routine -- Tiling through Proximal Alternating Linearized   Minimization",
        "text": "e data almost always better than the competing algorithms."
    },
    {
        "title": "Strategic Classification is Causal Modeling in Disguise",
        "text": "Consequential decision-making incentivizes individuals to strategically adapt their behavior to the specifics of the decision rule. While a long line of work has viewed strategic adaptation as gaming and attempted to mitigate its effects, recent work has i"
    },
    {
        "title": "Strategic Classification is Causal Modeling in Disguise",
        "text": "nstead sought to design classifiers that incentivize individuals to improve a desired quality. Key to both accounts is a cost function that dictates which adaptations are rational to undertake. In this work, we develop a causal framework for strategic adap"
    },
    {
        "title": "Strategic Classification is Causal Modeling in Disguise",
        "text": "tation. Our causal perspective clearly distinguishes between gaming and improvement and reveals an important obstacle to incentive design. We prove any procedure for designing classifiers that incentivize improvement must inevitably solve a non-trivial cau"
    },
    {
        "title": "Strategic Classification is Causal Modeling in Disguise",
        "text": "sal inference problem. Moreover, we show a similar result holds for designing cost functions that satisfy the requirements of previous work. With the benefit of hindsight, our results show much of the prior work on strategic classification is causal modeli"
    },
    {
        "title": "Strategic Classification is Causal Modeling in Disguise",
        "text": "ng in disguise."
    },
    {
        "title": "Scalable Scene Flow from Point Clouds in the Real World",
        "text": "Autonomous vehicles operate in highly dynamic environments necessitating an accurate assessment of which aspects of a scene are moving and where they are moving to. A popular approach to 3D motion estimation, termed scene flow, is to employ 3D point cloud "
    },
    {
        "title": "Scalable Scene Flow from Point Clouds in the Real World",
        "text": "data from consecutive LiDAR scans, although such approaches have been limited by the small size of real-world, annotated LiDAR data. In this work, we introduce a new large-scale dataset for scene flow estimation derived from corresponding tracked 3D object"
    },
    {
        "title": "Scalable Scene Flow from Point Clouds in the Real World",
        "text": "s, which is $\\sim$1,000$\\times$ larger than previous real-world datasets in terms of the number of annotated frames. We demonstrate how previous works were bounded based on the amount of real LiDAR data available, suggesting that larger datasets are requir"
    },
    {
        "title": "Scalable Scene Flow from Point Clouds in the Real World",
        "text": "ed to achieve state-of-the-art predictive performance. Furthermore, we show how previous heuristics for operating on point clouds such as down-sampling heavily degrade performance, motivating a new class of models that are tractable on the full point cloud"
    },
    {
        "title": "Scalable Scene Flow from Point Clouds in the Real World",
        "text": ". To address this issue, we introduce the FastFlow3D architecture which provides real time inference on the full point cloud. Additionally, we design human-interpretable metrics that better capture real world aspects by accounting for ego-motion and provid"
    },
    {
        "title": "Scalable Scene Flow from Point Clouds in the Real World",
        "text": "ing breakdowns per object type. We hope that this dataset may provide new opportunities for developing real world scene flow systems."
    },
    {
        "title": "Random Machines: A bagged-weighted support vector model with free kernel   choice",
        "text": "Improvement of statistical learning models in order to increase efficiency in solving classification or regression problems is still a goal pursued by the scientific community. In this way, the support vector machine model is one of the most successful and"
    },
    {
        "title": "Random Machines: A bagged-weighted support vector model with free kernel   choice",
        "text": " powerful algorithms for those tasks. However, its performance depends directly from the choice of the kernel function and their hyperparameters. The traditional choice of them, actually, can be computationally expensive to do the kernel choice and the tun"
    },
    {
        "title": "Random Machines: A bagged-weighted support vector model with free kernel   choice",
        "text": "ing processes. In this article, it is proposed a novel framework to deal with the kernel function selection called Random Machines. The results improved accuracy and reduced computational time. The data study was performed in simulated data and over 27 rea"
    },
    {
        "title": "Random Machines: A bagged-weighted support vector model with free kernel   choice",
        "text": "l benchmarking datasets."
    },
    {
        "title": "Towards Quantized Model Parallelism for Graph-Augmented MLPs Based on   Gradient-Free ADMM framework",
        "text": "The Graph Augmented Multi-layer Perceptron (GA-MLP) model is an attractive alternative to Graph Neural Networks (GNNs). This is because it is resistant to the over-smoothing problem, and deeper GA-MLP models yield better performance. GA-MLP models are trad"
    },
    {
        "title": "Towards Quantized Model Parallelism for Graph-Augmented MLPs Based on   Gradient-Free ADMM framework",
        "text": "itionally optimized by the Stochastic Gradient Descent (SGD). However, SGD suffers from the layer dependency problem, which prevents the gradients of different layers of GA-MLP models from being calculated in parallel. In this paper, we propose a parallel "
    },
    {
        "title": "Towards Quantized Model Parallelism for Graph-Augmented MLPs Based on   Gradient-Free ADMM framework",
        "text": "deep learning Alternating Direction Method of Multipliers (pdADMM) framework to achieve model parallelism: parameters in each layer of GA-MLP models can be updated in parallel. The extended pdADMM-Q algorithm reduces communication cost by utilizing the qua"
    },
    {
        "title": "Towards Quantized Model Parallelism for Graph-Augmented MLPs Based on   Gradient-Free ADMM framework",
        "text": "ntization technique. Theoretical convergence to a critical point of the pdADMM algorithm and the pdADMM-Q algorithm is provided with a sublinear convergence rate $o(1/k)$. Extensive experiments in six benchmark datasets demonstrate that the pdADMM can lead"
    },
    {
        "title": "Towards Quantized Model Parallelism for Graph-Augmented MLPs Based on   Gradient-Free ADMM framework",
        "text": " to high speedup, and outperforms all the existing state-of-the-art comparison methods."
    },
    {
        "title": "Fine-Grained Semantic Segmentation of Motion Capture Data using Dilated   Temporal Fully-Convolutional Networks",
        "text": "Human motion capture data has been widely used in data-driven character animation. In order to generate realistic, natural-looking motions, most data-driven approaches require considerable efforts of pre-processing, including motion segmentation and annota"
    },
    {
        "title": "Fine-Grained Semantic Segmentation of Motion Capture Data using Dilated   Temporal Fully-Convolutional Networks",
        "text": "tion. Existing (semi-) automatic solutions either require hand-crafted features for motion segmentation or do not produce the semantic annotations required for motion synthesis and building large-scale motion databases. In addition, human labeled annotatio"
    },
    {
        "title": "Fine-Grained Semantic Segmentation of Motion Capture Data using Dilated   Temporal Fully-Convolutional Networks",
        "text": "n data suffers from inter- and intra-labeler inconsistencies by design. We propose a semi-automatic framework for semantic segmentation of motion capture data based on supervised machine learning techniques. It first transforms a motion capture sequence in"
    },
    {
        "title": "Fine-Grained Semantic Segmentation of Motion Capture Data using Dilated   Temporal Fully-Convolutional Networks",
        "text": "to a ``motion image'' and applies a convolutional neural network for image segmentation. Dilated temporal convolutions enable the extraction of temporal information from a large receptive field. Our model outperforms two state-of-the-art models for action "
    },
    {
        "title": "Fine-Grained Semantic Segmentation of Motion Capture Data using Dilated   Temporal Fully-Convolutional Networks",
        "text": "segmentation, as well as a popular network for sequence modeling. Most of all, our method is very robust under noisy and inaccurate training labels and thus can handle human errors during the labeling process."
    },
    {
        "title": "Learning Equivariant Representations",
        "text": "State-of-the-art deep learning systems often require large amounts of data and computation. For this reason, leveraging known or unknown structure of the data is paramount. Convolutional neural networks (CNNs) are successful examples of this principle, the"
    },
    {
        "title": "Learning Equivariant Representations",
        "text": "ir defining characteristic being the shift-equivariance. By sliding a filter over the input, when the input shifts, the response shifts by the same amount, exploiting the structure of natural images where semantic content is independent of absolute pixel p"
    },
    {
        "title": "Learning Equivariant Representations",
        "text": "ositions. This property is essential to the success of CNNs in audio, image and video recognition tasks. In this thesis, we extend equivariance to other kinds of transformations, such as rotation and scaling. We propose equivariant models for different tra"
    },
    {
        "title": "Learning Equivariant Representations",
        "text": "nsformations defined by groups of symmetries. The main contributions are (i) polar transformer networks, achieving equivariance to the group of similarities on the plane, (ii) equivariant multi-view networks, achieving equivariance to the group of symmetri"
    },
    {
        "title": "Learning Equivariant Representations",
        "text": "es of the icosahedron, (iii) spherical CNNs, achieving equivariance to the continuous 3D rotation group, (iv) cross-domain image embeddings, achieving equivariance to 3D rotations for 2D inputs, and (v) spin-weighted spherical CNNs, generalizing the spheri"
    },
    {
        "title": "Learning Equivariant Representations",
        "text": "cal CNNs and achieving equivariance to 3D rotations for spherical vector fields. Applications include image classification, 3D shape classification and retrieval, panoramic image classification and segmentation, shape alignment and pose estimation. What th"
    },
    {
        "title": "Learning Equivariant Representations",
        "text": "ese models have in common is that they leverage symmetries in the data to reduce sample and model complexity and improve generalization performance. The advantages are more significant on (but not limited to) challenging tasks where data is limited or inpu"
    },
    {
        "title": "Learning Equivariant Representations",
        "text": "t perturbations such as arbitrary rotations are present."
    },
    {
        "title": "Optimal diagnostic tests for sporadic Creutzfeldt-Jakob disease based on   support vector machine classification of RT-QuIC data",
        "text": "In this work we study numerical construction of optimal clinical diagnostic tests for detecting sporadic Creutzfeldt-Jakob disease (sCJD). A cerebrospinal fluid sample (CSF) from a suspected sCJD patient is subjected to a process which initiates the aggreg"
    },
    {
        "title": "Optimal diagnostic tests for sporadic Creutzfeldt-Jakob disease based on   support vector machine classification of RT-QuIC data",
        "text": "ation of a protein present only in cases of sCJD. This aggregation is indirectly observed in real-time at regular intervals, so that a longitudinal set of data is constructed that is then analysed for evidence of this aggregation. The best existing test is"
    },
    {
        "title": "Optimal diagnostic tests for sporadic Creutzfeldt-Jakob disease based on   support vector machine classification of RT-QuIC data",
        "text": " based solely on the final value of this set of data, which is compared against a threshold to conclude whether or not aggregation, and thus sCJD, is present. This test criterion was decided upon by analysing data from a total of 108 sCJD and non-sCJD samp"
    },
    {
        "title": "Optimal diagnostic tests for sporadic Creutzfeldt-Jakob disease based on   support vector machine classification of RT-QuIC data",
        "text": "les, but this was done subjectively and there is no supporting mathematical analysis declaring this criterion to be exploiting the available data optimally. This paper addresses this deficiency, seeking to validate or improve the test primarily via support"
    },
    {
        "title": "Optimal diagnostic tests for sporadic Creutzfeldt-Jakob disease based on   support vector machine classification of RT-QuIC data",
        "text": " vector machine (SVM) classification. Besides this, we address a number of additional issues such as i) early stopping of the measurement process, ii) the possibility of detecting the particular type of sCJD and iii) the incorporation of additional patient"
    },
    {
        "title": "Optimal diagnostic tests for sporadic Creutzfeldt-Jakob disease based on   support vector machine classification of RT-QuIC data",
        "text": " data such as age, sex, disease duration and timing of CSF sampling into the construction of the test."
    },
    {
        "title": "Learning Action-Transferable Policy with Action Embedding",
        "text": "Transfer learning (TL) is a promising way to improve the sample efficiency of reinforcement learning. However, how to efficiently transfer knowledge across tasks with different state-action spaces is investigated at an early stage. Most previous studies on"
    },
    {
        "title": "Learning Action-Transferable Policy with Action Embedding",
        "text": "ly addressed the inconsistency across different state spaces by learning a common feature space, without considering that similar actions in different action spaces of related tasks share similar semantics. In this paper, we propose a method to learning ac"
    },
    {
        "title": "Learning Action-Transferable Policy with Action Embedding",
        "text": "tion embeddings by leveraging this idea, and a framework that learns both state embeddings and action embeddings to transfer policy across tasks with different state and action spaces. Our experimental results on various tasks show that the proposed method"
    },
    {
        "title": "Learning Action-Transferable Policy with Action Embedding",
        "text": " can not only learn informative action embeddings but accelerate policy learning."
    },
    {
        "title": "Adaptive Gradient Method with Resilience and Momentum",
        "text": "Several variants of stochastic gradient descent (SGD) have been proposed to improve the learning effectiveness and efficiency when training deep neural networks, among which some recent influential attempts would like to adaptively control the parameter-wi"
    },
    {
        "title": "Adaptive Gradient Method with Resilience and Momentum",
        "text": "se learning rate (e.g., Adam and RMSProp). Although they show a large improvement in convergence speed, most adaptive learning rate methods suffer from compromised generalization compared with SGD. In this paper, we proposed an Adaptive Gradient Method wit"
    },
    {
        "title": "Adaptive Gradient Method with Resilience and Momentum",
        "text": "h Resilience and Momentum (AdaRem), motivated by the observation that the oscillations of network parameters slow the training, and give a theoretical proof of convergence. For each parameter, AdaRem adjusts the parameter-wise learning rate according to wh"
    },
    {
        "title": "Adaptive Gradient Method with Resilience and Momentum",
        "text": "ether the direction of one parameter changes in the past is aligned with the direction of the current gradient, and thus encourages long-term consistent parameter updating with much fewer oscillations. Comprehensive experiments have been conducted to verif"
    },
    {
        "title": "Adaptive Gradient Method with Resilience and Momentum",
        "text": "y the effectiveness of AdaRem when training various models on a large-scale image recognition dataset, e.g., ImageNet, which also demonstrate that our method outperforms previous adaptive learning rate-based algorithms in terms of the training speed and th"
    },
    {
        "title": "Adaptive Gradient Method with Resilience and Momentum",
        "text": "e test error, respectively."
    },
    {
        "title": "Hyperbolic Deep Learning for Chinese Natural Language Understanding",
        "text": "Recently hyperbolic geometry has proven to be effective in building embeddings that encode hierarchical and entailment information. This makes it particularly suited to modelling the complex asymmetrical relationships between Chinese characters and words. "
    },
    {
        "title": "Hyperbolic Deep Learning for Chinese Natural Language Understanding",
        "text": "In this paper we first train a large scale hyperboloid skip-gram model on a Chinese corpus, then apply the character embeddings to a downstream hyperbolic Transformer model derived from the principles of gyrovector space for Poincare disk model. In our exp"
    },
    {
        "title": "Hyperbolic Deep Learning for Chinese Natural Language Understanding",
        "text": "eriments the character-based Transformer outperformed its word-based Euclidean equivalent. To the best of our knowledge, this is the first time in Chinese NLP that a character-based model outperformed its word-based counterpart, allowing the circumvention "
    },
    {
        "title": "Hyperbolic Deep Learning for Chinese Natural Language Understanding",
        "text": "of the challenging and domain-dependent task of Chinese Word Segmentation (CWS)."
    },
    {
        "title": "Decentralized adaptive clustering of deep nets is beneficial for client collaboration",
        "text": "We study the problem of training personalized deep learning models in a decentralized peer-to-peer setting, focusing on the setting where data distributions differ between the clients and where different clients have different local learning tasks. We stud"
    },
    {
        "title": "Decentralized adaptive clustering of deep nets is beneficial for client collaboration",
        "text": "y both covariate and label shift, and our contribution is an algorithm which for each client finds beneficial collaborations based on a similarity estimate for the local task. Our method does not rely on hyperparameters which are hard to estimate, such as "
    },
    {
        "title": "Decentralized adaptive clustering of deep nets is beneficial for client collaboration",
        "text": "the number of client clusters, but rather continuously adapts to the network topology using soft cluster assignment based on a novel adaptive gossip algorithm. We test the proposed method in various settings where data is not independent and identically di"
    },
    {
        "title": "Decentralized adaptive clustering of deep nets is beneficial for client collaboration",
        "text": "stributed among the clients. The experimental evaluation shows that the proposed method performs better than previous state-of-the-art algorithms for this problem setting, and handles situations well where previous methods fail."
    },
    {
        "title": "BS-NAS: Broadening-and-Shrinking One-Shot NAS with Searchable Numbers of   Channels",
        "text": "One-Shot methods have evolved into one of the most popular methods in Neural Architecture Search (NAS) due to weight sharing and single training of a supernet. However, existing methods generally suffer from two issues: predetermined number of channels in "
    },
    {
        "title": "BS-NAS: Broadening-and-Shrinking One-Shot NAS with Searchable Numbers of   Channels",
        "text": "each layer which is suboptimal; and model averaging effects and poor ranking correlation caused by weight coupling and continuously expanding search space. To explicitly address these issues, in this paper, a Broadening-and-Shrinking One-Shot NAS (BS-NAS) "
    },
    {
        "title": "BS-NAS: Broadening-and-Shrinking One-Shot NAS with Searchable Numbers of   Channels",
        "text": "framework is proposed, in which `broadening' refers to broadening the search space with a spring block enabling search for numbers of channels during training of the supernet; while `shrinking' refers to a novel shrinking strategy gradually turning off tho"
    },
    {
        "title": "BS-NAS: Broadening-and-Shrinking One-Shot NAS with Searchable Numbers of   Channels",
        "text": "se underperforming operations. The above innovations broaden the search space for wider representation and then shrink it by gradually removing underperforming operations, followed by an evolutionary algorithm to efficiently search for the optimal architec"
    },
    {
        "title": "BS-NAS: Broadening-and-Shrinking One-Shot NAS with Searchable Numbers of   Channels",
        "text": "ture. Extensive experiments on ImageNet illustrate the effectiveness of the proposed BS-NAS as well as the state-of-the-art performance."
    },
    {
        "title": "GATCluster: Self-Supervised Gaussian-Attention Network for Image   Clustering",
        "text": "We propose a self-supervised Gaussian ATtention network for image Clustering (GATCluster). Rather than extracting intermediate features first and then performing the traditional clustering algorithm, GATCluster directly outputs semantic cluster labels with"
    },
    {
        "title": "GATCluster: Self-Supervised Gaussian-Attention Network for Image   Clustering",
        "text": "out further post-processing. Theoretically, we give a Label Feature Theorem to guarantee the learned features are one-hot encoded vectors, and the trivial solutions are avoided. To train the GATCluster in a completely unsupervised manner, we design four se"
    },
    {
        "title": "GATCluster: Self-Supervised Gaussian-Attention Network for Image   Clustering",
        "text": "lf-learning tasks with the constraints of transformation invariance, separability maximization, entropy analysis, and attention mapping. Specifically, the transformation invariance and separability maximization tasks learn the relationships between sample "
    },
    {
        "title": "GATCluster: Self-Supervised Gaussian-Attention Network for Image   Clustering",
        "text": "pairs. The entropy analysis task aims to avoid trivial solutions. To capture the object-oriented semantics, we design a self-supervised attention mechanism that includes a parameterized attention module and a soft-attention loss. All the guiding signals fo"
    },
    {
        "title": "GATCluster: Self-Supervised Gaussian-Attention Network for Image   Clustering",
        "text": "r clustering are self-generated during the training process. Moreover, we develop a two-step learning algorithm that is memory-efficient for clustering large-size images. Extensive experiments demonstrate the superiority of our proposed method in compariso"
    },
    {
        "title": "GATCluster: Self-Supervised Gaussian-Attention Network for Image   Clustering",
        "text": "n with the state-of-the-art image clustering benchmarks. Our code has been made publicly available at https://github.com/niuchuangnn/GATCluster."
    },
    {
        "title": "Random Projection in Neural Episodic Control",
        "text": "End-to-end deep reinforcement learning has enabled agents to learn with little preprocessing by humans. However, it is still difficult to learn stably and efficiently because the learning method usually uses a nonlinear function approximation. Neural Episo"
    },
    {
        "title": "Random Projection in Neural Episodic Control",
        "text": "dic Control (NEC), which has been proposed in order to improve sample efficiency, is able to learn stably by estimating action values using a non-parametric method. In this paper, we propose an architecture that incorporates random projection into NEC to t"
    },
    {
        "title": "Random Projection in Neural Episodic Control",
        "text": "rain with more stability. In addition, we verify the effectiveness of our architecture by Atari's five games. The main idea is to reduce the number of parameters that have to learn by replacing neural networks with random projection in order to reduce dime"
    },
    {
        "title": "Random Projection in Neural Episodic Control",
        "text": "nsions while keeping the learning end-to-end."
    },
    {
        "title": "Learning Models for Actionable Recourse",
        "text": "As machine learning models are increasingly deployed in high-stakes domains such as legal and financial decision-making, there has been growing interest in post-hoc methods for generating counterfactual explanations. Such explanations provide individuals a"
    },
    {
        "title": "Learning Models for Actionable Recourse",
        "text": "dversely impacted by predicted outcomes (e.g., an applicant denied a loan) with recourse -- i.e., a description of how they can change their features to obtain a positive outcome. We propose a novel algorithm that leverages adversarial training and PAC con"
    },
    {
        "title": "Learning Models for Actionable Recourse",
        "text": "fidence sets to learn models that theoretically guarantee recourse to affected individuals with high probability without sacrificing accuracy. We demonstrate the efficacy of our approach via extensive experiments on real data."
    },
    {
        "title": "Online Optimization : Competing with Dynamic Comparators",
        "text": "Recent literature on online learning has focused on developing adaptive algorithms that take advantage of a regularity of the sequence of observations, yet retain worst-case performance guarantees. A complementary direction is to develop prediction methods"
    },
    {
        "title": "Online Optimization : Competing with Dynamic Comparators",
        "text": " that perform well against complex benchmarks. In this paper, we address these two directions together. We present a fully adaptive method that competes with dynamic benchmarks in which regret guarantee scales with regularity of the sequence of cost functi"
    },
    {
        "title": "Online Optimization : Competing with Dynamic Comparators",
        "text": "ons and comparators. Notably, the regret bound adapts to the smaller complexity measure in the problem environment. Finally, we apply our results to drifting zero-sum, two-player games where both players achieve no regret guarantees against best sequences "
    },
    {
        "title": "Online Optimization : Competing with Dynamic Comparators",
        "text": "of actions in hindsight."
    },
    {
        "title": "Labelling unlabelled videos from scratch with multi-modal   self-supervision",
        "text": "A large part of the current success of deep learning lies in the effectiveness of data -- more precisely: labelled data. Yet, labelling a dataset with human annotation continues to carry high costs, especially for videos. While in the image domain, recent "
    },
    {
        "title": "Labelling unlabelled videos from scratch with multi-modal   self-supervision",
        "text": "methods have allowed to generate meaningful (pseudo-) labels for unlabelled datasets without supervision, this development is missing for the video domain where learning feature representations is the current focus. In this work, we a) show that unsupervis"
    },
    {
        "title": "Labelling unlabelled videos from scratch with multi-modal   self-supervision",
        "text": "ed labelling of a video dataset does not come for free from strong feature encoders and b) propose a novel clustering method that allows pseudo-labelling of a video dataset without any human annotations, by leveraging the natural correspondence between the"
    },
    {
        "title": "Labelling unlabelled videos from scratch with multi-modal   self-supervision",
        "text": " audio and visual modalities. An extensive analysis shows that the resulting clusters have high semantic overlap to ground truth human labels. We further introduce the first benchmarking results on unsupervised labelling of common video datasets Kinetics, "
    },
    {
        "title": "Labelling unlabelled videos from scratch with multi-modal   self-supervision",
        "text": "Kinetics-Sound, VGG-Sound and AVE."
    },
    {
        "title": "Simeon -- Secure Federated Machine Learning Through Iterative Filtering",
        "text": "Federated learning enables a global machine learning model to be trained collaboratively by distributed, mutually non-trusting learning agents who desire to maintain the privacy of their training data and their hardware. A global model is distributed to cl"
    },
    {
        "title": "Simeon -- Secure Federated Machine Learning Through Iterative Filtering",
        "text": "ients, who perform training, and submit their newly-trained model to be aggregated into a superior model. However, federated learning systems are vulnerable to interference from malicious learning agents who may desire to prevent training or induce targete"
    },
    {
        "title": "Simeon -- Secure Federated Machine Learning Through Iterative Filtering",
        "text": "d misclassification in the resulting global model. A class of Byzantine-tolerant aggregation algorithms has emerged, offering varying degrees of robustness against these attacks, often with the caveat that the number of attackers is bounded by some quantit"
    },
    {
        "title": "Simeon -- Secure Federated Machine Learning Through Iterative Filtering",
        "text": "y known prior to training. This paper presents Simeon: a novel approach to aggregation that applies a reputation-based iterative filtering technique to achieve robustness even in the presence of attackers who can exhibit arbitrary behaviour. We compare Sim"
    },
    {
        "title": "Simeon -- Secure Federated Machine Learning Through Iterative Filtering",
        "text": "eon to state-of-the-art aggregation techniques and find that Simeon achieves comparable or superior robustness to a variety of attacks. Notably, we show that Simeon is tolerant to sybil attacks, where other algorithms are not, presenting a key advantage of"
    },
    {
        "title": "Simeon -- Secure Federated Machine Learning Through Iterative Filtering",
        "text": " our approach."
    },
    {
        "title": "A Non-convex One-Pass Framework for Generalized Factorization Machine   and Rank-One Matrix Sensing",
        "text": "We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from $d$ dimensional random Gaussian vectors and the target second ord"
    },
    {
        "title": "A Non-convex One-Pass Framework for Generalized Factorization Machine   and Rank-One Matrix Sensing",
        "text": "er coefficient matrix in gFM is of rank $k$, our algorithm converges linearly, achieves $O(\\epsilon)$ recovery error after retrieving $O(k^{3}d\\log(1/\\epsilon))$ training instances, consumes $O(kd)$ memory in one-pass of dataset and only requires matrix-ve"
    },
    {
        "title": "A Non-convex One-Pass Framework for Generalized Factorization Machine   and Rank-One Matrix Sensing",
        "text": "ctor product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to s"
    },
    {
        "title": "A Non-convex One-Pass Framework for Generalized Factorization Machine   and Rank-One Matrix Sensing",
        "text": "ymmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval."
    },
    {
        "title": "Moment-Based Variational Inference for Stochastic Differential Equations",
        "text": "Existing deterministic variational inference approaches for diffusion processes use simple proposals and target the marginal density of the posterior. We construct the variational process as a controlled version of the prior process and approximate the pos"
    },
    {
        "title": "Moment-Based Variational Inference for Stochastic Differential Equations",
        "text": "terior by a set of moment functions. In combination with moment closure, the smoothing problem is reduced to a deterministic optimal control problem. Exploiting the path-wise Fisher information, we propose an optimization procedure that corresponds to a na"
    },
    {
        "title": "Moment-Based Variational Inference for Stochastic Differential Equations",
        "text": "tural gradient descent in the variational parameters. Our approach allows for richer variational approximations that extend to state-dependent diffusion terms. The classical Gaussian process approximation is recovered as a special case."
    },
    {
        "title": "ResNorm: Tackling Long-tailed Degree Distribution Issue in Graph Neural   Networks via Normalization",
        "text": "Graph Neural Networks (GNNs) have attracted much attention due to their ability in learning representations from graph-structured data. Despite the successful applications of GNNs in many domains, the optimization of GNNs is less well studied, and the perf"
    },
    {
        "title": "ResNorm: Tackling Long-tailed Degree Distribution Issue in Graph Neural   Networks via Normalization",
        "text": "ormance on node classification heavily suffers from the long-tailed node degree distribution. This paper focuses on improving the performance of GNNs via normalization.   In detail, by studying the long-tailed distribution of node degrees in the graph, we "
    },
    {
        "title": "ResNorm: Tackling Long-tailed Degree Distribution Issue in Graph Neural   Networks via Normalization",
        "text": "propose a novel normalization method for GNNs, which is termed ResNorm (\\textbf{Res}haping the long-tailed distribution into a normal-like distribution via \\textbf{norm}alization). The $scale$ operation of ResNorm reshapes the node-wise standard deviation "
    },
    {
        "title": "ResNorm: Tackling Long-tailed Degree Distribution Issue in Graph Neural   Networks via Normalization",
        "text": "(NStd) distribution so as to improve the accuracy of tail nodes (\\textit{i}.\\textit{e}., low-degree nodes). We provide a theoretical interpretation and empirical evidence for understanding the mechanism of the above $scale$. In addition to the long-tailed "
    },
    {
        "title": "ResNorm: Tackling Long-tailed Degree Distribution Issue in Graph Neural   Networks via Normalization",
        "text": "distribution issue, over-smoothing is also a fundamental issue plaguing the community. To this end, we analyze the behavior of the standard shift and prove that the standard shift serves as a preconditioner on the weight matrix, increasing the risk of over"
    },
    {
        "title": "ResNorm: Tackling Long-tailed Degree Distribution Issue in Graph Neural   Networks via Normalization",
        "text": "-smoothing. With the over-smoothing issue in mind, we design a $shift$ operation for ResNorm that simulates the degree-specific parameter strategy in a low-cost manner. Extensive experiments have validated the effectiveness of ResNorm on several node class"
    },
    {
        "title": "ResNorm: Tackling Long-tailed Degree Distribution Issue in Graph Neural   Networks via Normalization",
        "text": "ification benchmark datasets."
    },
    {
        "title": "JBFnet -- Low Dose CT Denoising by Trainable Joint Bilateral Filtering",
        "text": "Deep neural networks have shown great success in low dose CT denoising. However, most of these deep neural networks have several hundred thousand trainable parameters. This, combined with the inherent non-linearity of the neural network, makes the deep neu"
    },
    {
        "title": "JBFnet -- Low Dose CT Denoising by Trainable Joint Bilateral Filtering",
        "text": "ral network diffcult to understand with low accountability. In this study we introduce JBFnet, a neural network for low dose CT denoising. The architecture of JBFnet implements iterative bilateral filtering. The filter functions of the Joint Bilateral Filt"
    },
    {
        "title": "JBFnet -- Low Dose CT Denoising by Trainable Joint Bilateral Filtering",
        "text": "er (JBF) are learned via shallow convolutional networks. The guidance image is estimated by a deep neural network. JBFnet is split into four filtering blocks, each of which performs Joint Bilateral Filtering. Each JBF block consists of 112 trainable parame"
    },
    {
        "title": "JBFnet -- Low Dose CT Denoising by Trainable Joint Bilateral Filtering",
        "text": "ters, making the noise removal process comprehendable. The Noise Map (NM) is added after filtering to preserve high level features. We train JBFnet with the data from the body scans of 10 patients, and test it on the AAPM low dose CT Grand Challenge datase"
    },
    {
        "title": "JBFnet -- Low Dose CT Denoising by Trainable Joint Bilateral Filtering",
        "text": "t. We compare JBFnet with state-of-the-art deep learning networks. JBFnet outperforms CPCE3D, GAN and deep GFnet on the test dataset in terms of noise removal while preserving structures. We conduct several ablation studies to test the performance of our n"
    },
    {
        "title": "JBFnet -- Low Dose CT Denoising by Trainable Joint Bilateral Filtering",
        "text": "etwork architecture and training method. Our current setup achieves the best performance, while still maintaining behavioural accountability."
    },
    {
        "title": "A Unified Benchmark for the Unknown Detection Capability of Deep Neural   Networks",
        "text": "Deep neural networks have achieved outstanding performance over various tasks, but they have a critical issue: over-confident predictions even for completely unknown samples. Many studies have been proposed to successfully filter out these unknown samples,"
    },
    {
        "title": "A Unified Benchmark for the Unknown Detection Capability of Deep Neural   Networks",
        "text": " but they only considered narrow and specific tasks, referred to as misclassification detection, open-set recognition, or out-of-distribution detection. In this work, we argue that these tasks should be treated as fundamentally an identical problem because"
    },
    {
        "title": "A Unified Benchmark for the Unknown Detection Capability of Deep Neural   Networks",
        "text": " an ideal model should possess detection capability for all those tasks. Therefore, we introduce the unknown detection task, an integration of previous individual tasks, for a rigorous examination of the detection capability of deep neural networks on a wi"
    },
    {
        "title": "A Unified Benchmark for the Unknown Detection Capability of Deep Neural   Networks",
        "text": "de spectrum of unknown samples. To this end, unified benchmark datasets on different scales were constructed and the unknown detection capabilities of existing popular methods were subject to comparison. We found that Deep Ensemble consistently outperforms"
    },
    {
        "title": "A Unified Benchmark for the Unknown Detection Capability of Deep Neural   Networks",
        "text": " the other approaches in detecting unknowns; however, all methods are only successful for a specific type of unknown. The reproducible code and benchmark datasets are available at https://github.com/daintlab/unknown-detection-benchmarks ."
    },
    {
        "title": "A survey on datasets for fairness-aware machine learning",
        "text": "As decision-making increasingly relies on Machine Learning (ML) and (big) data, the issue of fairness in data-driven Artificial Intelligence (AI) systems is receiving increasing attention from both research and industry. A large variety of fairness-aware m"
    },
    {
        "title": "A survey on datasets for fairness-aware machine learning",
        "text": "achine learning solutions have been proposed which involve fairness-related interventions in the data, learning algorithms and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that rep"
    },
    {
        "title": "A survey on datasets for fairness-aware machine learning",
        "text": "resent realistic and diverse settings. Therefore, in this paper, we overview real-world datasets used for fairness-aware machine learning. We focus on tabular data as the most common data representation for fairness-aware machine learning. We start our ana"
    },
    {
        "title": "A survey on datasets for fairness-aware machine learning",
        "text": "lysis by identifying relationships between the different attributes, particularly w.r.t. protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate the interesting relationships us"
    },
    {
        "title": "A survey on datasets for fairness-aware machine learning",
        "text": "ing exploratory analysis."
    },
    {
        "title": "NovGrid: A Flexible Grid World for Evaluating Agent Response to Novelty",
        "text": "A robust body of reinforcement learning techniques have been developed to solve complex sequential decision making problems. However, these methods assume that train and evaluation tasks come from similarly or identically distributed environments. This ass"
    },
    {
        "title": "NovGrid: A Flexible Grid World for Evaluating Agent Response to Novelty",
        "text": "umption does not hold in real life where small novel changes to the environment can make a previously learned policy fail or introduce simpler solutions that might never be found. To that end we explore the concept of {\\em novelty}, defined in this work as"
    },
    {
        "title": "NovGrid: A Flexible Grid World for Evaluating Agent Response to Novelty",
        "text": " the sudden change to the mechanics or properties of environment. We provide an ontology of for novelties most relevant to sequential decision making, which distinguishes between novelties that affect objects versus actions, unary properties versus non-una"
    },
    {
        "title": "NovGrid: A Flexible Grid World for Evaluating Agent Response to Novelty",
        "text": "ry relations, and the distribution of solutions to a task. We introduce NovGrid, a novelty generation framework built on MiniGrid, acting as a toolkit for rapidly developing and evaluating novelty-adaptation-enabled reinforcement learning techniques. Along"
    },
    {
        "title": "NovGrid: A Flexible Grid World for Evaluating Agent Response to Novelty",
        "text": " with the core NovGrid we provide exemplar novelties aligned with our ontology and instantiate them as novelty templates that can be applied to many MiniGrid-compliant environments. Finally, we present a set of metrics built into our framework for the eval"
    },
    {
        "title": "NovGrid: A Flexible Grid World for Evaluating Agent Response to Novelty",
        "text": "uation of novelty-adaptation-enabled machine-learning techniques, and show characteristics of a baseline RL model using these metrics."
    },
    {
        "title": "Online Learning of Combinatorial Objects via Extended Formulation",
        "text": "The standard techniques for online learning of combinatorial objects perform multiplicative updates followed by projections into the convex hull of all the objects. However, this methodology can be expensive if the convex hull contains many facets. For exa"
    },
    {
        "title": "Online Learning of Combinatorial Objects via Extended Formulation",
        "text": "mple, the convex hull of $n$-symbol Huffman trees is known to have exponentially many facets (Maurras et al., 2010). We get around this difficulty by exploiting extended formulations (Kaibel, 2011), which encode the polytope of combinatorial objects in a h"
    },
    {
        "title": "Online Learning of Combinatorial Objects via Extended Formulation",
        "text": "igher dimensional \"extended\" space with only polynomially many facets. We develop a general framework for converting extended formulations into efficient online algorithms with good relative loss bounds. We present applications of our framework to online l"
    },
    {
        "title": "Online Learning of Combinatorial Objects via Extended Formulation",
        "text": "earning of Huffman trees and permutations. The regret bounds of the resulting algorithms are within a factor of $O(\\sqrt{\\log(n)})$ of the state-of-the-art specialized algorithms for permutations, and depending on the loss regimes, improve on or match the "
    },
    {
        "title": "Online Learning of Combinatorial Objects via Extended Formulation",
        "text": "state-of-the-art for Huffman trees. Our method is general and can be applied to other combinatorial objects."
    },
    {
        "title": "Architecture Matters: Investigating the Influence of Differential   Privacy on Neural Network Design",
        "text": "One barrier to more widespread adoption of differentially private neural networks is the entailed accuracy loss. To address this issue, the relationship between neural network architectures and model accuracy under differential privacy constraints needs to"
    },
    {
        "title": "Architecture Matters: Investigating the Influence of Differential   Privacy on Neural Network Design",
        "text": " be better understood. As a first step, we test whether extant knowledge on architecture design also holds in the differentially private setting. Our findings show that it does not; architectures that perform well without differential privacy, do not neces"
    },
    {
        "title": "Architecture Matters: Investigating the Influence of Differential   Privacy on Neural Network Design",
        "text": "sarily do so with differential privacy. Consequently, extant knowledge on neural network architecture design cannot be seamlessly translated into the differential privacy context. Future research is required to better understand the relationship between ne"
    },
    {
        "title": "Architecture Matters: Investigating the Influence of Differential   Privacy on Neural Network Design",
        "text": "ural network architectures and model accuracy to enable better architecture design choices under differential privacy constraints."
    },
    {
        "title": "Provable Robust Classification via Learned Smoothed Densities",
        "text": "Smoothing classifiers and probability density functions with Gaussian kernels appear unrelated, but in this work, they are unified for the problem of robust classification. The key building block is approximating the $\\textit{energy function}$ of the rando"
    },
    {
        "title": "Provable Robust Classification via Learned Smoothed Densities",
        "text": "m variable $Y=X+N(0,\\sigma^2 I_d)$ with a neural network which we use to formulate the problem of robust classification in terms of $\\widehat{x}(Y)$, the $\\textit{Bayes estimator}$ of $X$ given the noisy measurements $Y$. We introduce $\\textit{empirical Ba"
    },
    {
        "title": "Provable Robust Classification via Learned Smoothed Densities",
        "text": "yes smoothed classifiers}$ within the framework of $\\textit{randomized smoothing}$ and study it theoretically for the two-class linear classifier, where we show one can improve their robustness above $\\textit{the margin}$. We test the theory on MNIST and w"
    },
    {
        "title": "Provable Robust Classification via Learned Smoothed Densities",
        "text": "e show that with a learned smoothed energy function and a linear classifier we can achieve provable $\\ell_2$ robust accuracies that are competitive with empirical defenses. This setup can be significantly improved by $\\textit{learning}$ empirical Bayes smo"
    },
    {
        "title": "Provable Robust Classification via Learned Smoothed Densities",
        "text": "othed classifiers with adversarial training and on MNIST we show that we can achieve provable robust accuracies higher than the state-of-the-art empirical defenses in a range of radii. We discuss some fundamental challenges of randomized smoothing based on"
    },
    {
        "title": "Provable Robust Classification via Learned Smoothed Densities",
        "text": " a geometric interpretation due to concentration of Gaussians in high dimensions, and we finish the paper with a proposal for using walk-jump sampling, itself based on learned smoothed densities, for robust classification."
    },
    {
        "title": "Manifold Optimization for Gaussian Mixture Models",
        "text": "We take a new look at parameter estimation for Gaussian Mixture Models (GMMs). In particular, we propose using \\emph{Riemannian manifold optimization} as a powerful counterpart to Expectation Maximization (EM). An out-of-the-box invocation of manifold opti"
    },
    {
        "title": "Manifold Optimization for Gaussian Mixture Models",
        "text": "mization, however, fails spectacularly: it converges to the same solution but vastly slower. Driven by intuition from manifold convexity, we then propose a reparamerization that has remarkable empirical consequences. It makes manifold optimization not only"
    },
    {
        "title": "Manifold Optimization for Gaussian Mixture Models",
        "text": " match EM---a highly encouraging result in itself given the poor record nonlinear programming methods have had against EM so far---but also outperform EM in many practical settings, while displaying much less variability in running times. We further highli"
    },
    {
        "title": "Manifold Optimization for Gaussian Mixture Models",
        "text": "ght the strengths of manifold optimization by developing a somewhat tuned manifold LBFGS method that proves even more competitive and reliable than existing manifold optimization tools. We hope that our results encourage a wider consideration of manifold o"
    },
    {
        "title": "Manifold Optimization for Gaussian Mixture Models",
        "text": "ptimization for parameter estimation problems."
    },
    {
        "title": "Adversarial Examples for Good: Adversarial Examples Guided Imbalanced   Learning",
        "text": "Adversarial examples are inputs for machine learning models that have been designed by attackers to cause the model to make mistakes. In this paper, we demonstrate that adversarial examples can also be utilized for good to improve the performance of imbala"
    },
    {
        "title": "Adversarial Examples for Good: Adversarial Examples Guided Imbalanced   Learning",
        "text": "nced learning. We provide a new perspective on how to deal with imbalanced data: adjust the biased decision boundary by training with Guiding Adversarial Examples (GAEs). Our method can effectively increase the accuracy of minority classes while sacrificin"
    },
    {
        "title": "Adversarial Examples for Good: Adversarial Examples Guided Imbalanced   Learning",
        "text": "g little accuracy on majority classes. We empirically show, on several benchmark datasets, our proposed method is comparable to the state-of-the-art method. To our best knowledge, we are the first to deal with imbalanced learning with adversarial examples."
    },
    {
        "title": "Seq-U-Net: A One-Dimensional Causal U-Net for Efficient Sequence   Modelling",
        "text": "Convolutional neural networks (CNNs) with dilated filters such as the Wavenet or the Temporal Convolutional Network (TCN) have shown good results in a variety of sequence modelling tasks. However, efficiently modelling long-term dependencies in these seque"
    },
    {
        "title": "Seq-U-Net: A One-Dimensional Causal U-Net for Efficient Sequence   Modelling",
        "text": "nces is still challenging. Although the receptive field of these models grows exponentially with the number of layers, computing the convolutions over very long sequences of features in each layer is time and memory-intensive, prohibiting the use of longer"
    },
    {
        "title": "Seq-U-Net: A One-Dimensional Causal U-Net for Efficient Sequence   Modelling",
        "text": " receptive fields in practice. To increase efficiency, we make use of the \"slow feature\" hypothesis stating that many features of interest are slowly varying over time. For this, we use a U-Net architecture that computes features at multiple time-scales an"
    },
    {
        "title": "Seq-U-Net: A One-Dimensional Causal U-Net for Efficient Sequence   Modelling",
        "text": "d adapt it to our auto-regressive scenario by making convolutions causal. We apply our model (\"Seq-U-Net\") to a variety of tasks including language and audio generation. In comparison to TCN and Wavenet, our network consistently saves memory and computatio"
    },
    {
        "title": "Seq-U-Net: A One-Dimensional Causal U-Net for Efficient Sequence   Modelling",
        "text": "n time, with speed-ups for training and inference of over 4x in the audio generation experiment in particular, while achieving a comparable performance in all tasks."
    },
    {
        "title": "Inverse Reinforcement Learning from a Gradient-based Learner",
        "text": "Inverse Reinforcement Learning addresses the problem of inferring an expert's reward function from demonstrations. However, in many applications, we not only have access to the expert's near-optimal behavior, but we also observe part of her learning proces"
    },
    {
        "title": "Inverse Reinforcement Learning from a Gradient-based Learner",
        "text": "s. In this paper, we propose a new algorithm for this setting, in which the goal is to recover the reward function being optimized by an agent, given a sequence of policies produced during learning. Our approach is based on the assumption that the observed"
    },
    {
        "title": "Inverse Reinforcement Learning from a Gradient-based Learner",
        "text": " agent is updating her policy parameters along the gradient direction. Then we extend our method to deal with the more realistic scenario where we only have access to a dataset of learning trajectories. For both settings, we provide theoretical insights in"
    },
    {
        "title": "Inverse Reinforcement Learning from a Gradient-based Learner",
        "text": "to our algorithms' performance. Finally, we evaluate the approach in a simulated GridWorld environment and on the MuJoCo environments, comparing it with the state-of-the-art baseline."
    },
    {
        "title": "Least Squares Approximation for a Distributed System",
        "text": "In this work, we develop a distributed least squares approximation (DLSA) method that is able to solve a large family of regression problems (e.g., linear regression, logistic regression, and Cox's model) on a distributed system. By approximating the local"
    },
    {
        "title": "Least Squares Approximation for a Distributed System",
        "text": " objective function using a local quadratic form, we are able to obtain a combined estimator by taking a weighted average of local estimators. The resulting estimator is proved to be statistically as efficient as the global estimator. Moreover, it requires"
    },
    {
        "title": "Least Squares Approximation for a Distributed System",
        "text": " only one round of communication. We further conduct a shrinkage estimation based on the DLSA estimation using an adaptive Lasso approach. The solution can be easily obtained by using the LARS algorithm on the master node. It is theoretically shown that th"
    },
    {
        "title": "Least Squares Approximation for a Distributed System",
        "text": "e resulting estimator possesses the oracle property and is selection consistent by using a newly designed distributed Bayesian information criterion (DBIC). The finite sample performance and computational efficiency are further illustrated by an extensive "
    },
    {
        "title": "Least Squares Approximation for a Distributed System",
        "text": "numerical study and an airline dataset. The airline dataset is 52 GB in size. The entire methodology has been implemented in Python for a {\\it de-facto} standard Spark system. The proposed DLSA algorithm on the Spark system takes 26 minutes to obtain a log"
    },
    {
        "title": "Least Squares Approximation for a Distributed System",
        "text": "istic regression estimator, which is more efficient and memory friendly than conventional methods."
    },
    {
        "title": "Analyzing the impact of feature selection on the accuracy of heart disease prediction",
        "text": "Heart Disease has become one of the most serious diseases that has a significant impact on human life. It has emerged as one of the leading causes of mortality among the people across the globe during the last decade. In order to prevent patients from furt"
    },
    {
        "title": "Analyzing the impact of feature selection on the accuracy of heart disease prediction",
        "text": "her damage, an accurate diagnosis of heart disease on time is an essential factor. Recently we have seen the usage of non-invasive medical procedures, such as artificial intelligence-based techniques in the field of medical. Specially machine learning empl"
    },
    {
        "title": "Analyzing the impact of feature selection on the accuracy of heart disease prediction",
        "text": "oys several algorithms and techniques that are widely used and are highly useful in accurately diagnosing the heart disease with less amount of time. However, the prediction of heart disease is not an easy task. The increasing size of medical datasets has "
    },
    {
        "title": "Analyzing the impact of feature selection on the accuracy of heart disease prediction",
        "text": "made it a complicated task for practitioners to understand the complex feature relations and make disease predictions. Accordingly, the aim of this research is to identify the most important risk-factors from a highly dimensional dataset which helps in the"
    },
    {
        "title": "Analyzing the impact of feature selection on the accuracy of heart disease prediction",
        "text": " accurate classification of heart disease with less complications. For a broader analysis, we have used two heart disease datasets with various medical features. The classification results of the benchmarked models proved that there is a high impact of rel"
    },
    {
        "title": "Analyzing the impact of feature selection on the accuracy of heart disease prediction",
        "text": "evant features on the classification accuracy. Even with a reduced number of features, the performance of the classification models improved significantly with a reduced training time as compared with models trained on full feature set."
    },
    {
        "title": "GECKO: Reconciling Privacy, Accuracy and Efficiency in Embedded Deep   Learning",
        "text": "Embedded systems demand on-device processing of data using Neural Networks (NNs) while conforming to the memory, power and computation constraints, leading to an efficiency and accuracy tradeoff. To bring NNs to edge devices, several optimizations such as "
    },
    {
        "title": "GECKO: Reconciling Privacy, Accuracy and Efficiency in Embedded Deep   Learning",
        "text": "model compression through pruning, quantization, and off-the-shelf architectures with efficient design have been extensively adopted. These algorithms when deployed to real world sensitive applications, requires to resist inference attacks to protect priva"
    },
    {
        "title": "GECKO: Reconciling Privacy, Accuracy and Efficiency in Embedded Deep   Learning",
        "text": "cy of users training data. However, resistance against inference attacks is not accounted for designing NN models for IoT. In this work, we analyse the three-dimensional privacy-accuracy-efficiency tradeoff in NNs for IoT devices and propose Gecko training"
    },
    {
        "title": "GECKO: Reconciling Privacy, Accuracy and Efficiency in Embedded Deep   Learning",
        "text": " methodology where we explicitly add resistance to private inferences as a design objective. We optimize the inference-time memory, computation, and power constraints of embedded devices as a criterion for designing NN architecture while also preserving pr"
    },
    {
        "title": "GECKO: Reconciling Privacy, Accuracy and Efficiency in Embedded Deep   Learning",
        "text": "ivacy. We choose quantization as design choice for highly efficient and private models. This choice is driven by the observation that compressed models leak more information compared to baseline models while off-the-shelf efficient architectures indicate p"
    },
    {
        "title": "GECKO: Reconciling Privacy, Accuracy and Efficiency in Embedded Deep   Learning",
        "text": "oor efficiency and privacy tradeoff. We show that models trained using Gecko methodology are comparable to prior defences against black-box membership attacks in terms of accuracy and privacy while providing efficiency."
    },
    {
        "title": "ARC -- Actor Residual Critic for Adversarial Imitation Learning",
        "text": "Adversarial Imitation Learning (AIL) is a class of popular state-of-the-art Imitation Learning algorithms where an artificial adversary's misclassification is used as a reward signal and is optimized by any standard Reinforcement Learning (RL) algorithm. U"
    },
    {
        "title": "ARC -- Actor Residual Critic for Adversarial Imitation Learning",
        "text": "nlike most RL settings, the reward in AIL is differentiable but model-free RL algorithms do not make use of this property to train a policy. In contrast, we leverage the differentiability property of the AIL reward function and formulate a class of Actor R"
    },
    {
        "title": "ARC -- Actor Residual Critic for Adversarial Imitation Learning",
        "text": "esidual Critic (ARC) RL algorithms that draw a parallel to the standard Actor-Critic (AC) algorithms in RL literature and uses a residual critic, C function (instead of the standard Q function) to approximate only the discounted future return (excluding th"
    },
    {
        "title": "ARC -- Actor Residual Critic for Adversarial Imitation Learning",
        "text": "e immediate reward). ARC algorithms have similar convergence properties as the standard AC algorithms with the additional advantage that the gradient through the immediate reward is exact. For the discrete (tabular) case with finite states, actions, and kn"
    },
    {
        "title": "ARC -- Actor Residual Critic for Adversarial Imitation Learning",
        "text": "own dynamics, we prove that policy iteration with $C$ function converges to an optimal policy. In the continuous case with function approximation and unknown dynamics, we experimentally show that ARC aided AIL outperforms standard AIL in simulated continuo"
    },
    {
        "title": "ARC -- Actor Residual Critic for Adversarial Imitation Learning",
        "text": "us-control and real robotic manipulation tasks. ARC algorithms are simple to implement and can be incorporated into any existing AIL implementation with an AC algorithm."
    },
    {
        "title": "Machine learning on DNA-encoded library count data using an   uncertainty-aware probabilistic loss function",
        "text": "DNA-encoded library (DEL) screening and quantitative structure-activity relationship (QSAR) modeling are two techniques used in drug discovery to find small molecules that bind a protein target. Applying QSAR modeling to DEL data can facilitate the selecti"
    },
    {
        "title": "Machine learning on DNA-encoded library count data using an   uncertainty-aware probabilistic loss function",
        "text": "on of compounds for off-DNA synthesis and evaluation. Such a combined approach has been shown recently by training binary classifiers to learn DEL enrichments of aggregated \"disynthons\" to accommodate the sparse and noisy nature of DEL data. However, a bin"
    },
    {
        "title": "Machine learning on DNA-encoded library count data using an   uncertainty-aware probabilistic loss function",
        "text": "ary classifier cannot distinguish between different levels of enrichment, and information is potentially lost during disynthon aggregation. Here, we demonstrate a regression approach to learning DEL enrichments of individual molecules using a custom negati"
    },
    {
        "title": "Machine learning on DNA-encoded library count data using an   uncertainty-aware probabilistic loss function",
        "text": "ve log-likelihood loss function that effectively denoises DEL data and introduces opportunities for visualization of learned structure-activity relationships (SAR). Our approach explicitly models the Poisson statistics of the sequencing process used in the"
    },
    {
        "title": "Machine learning on DNA-encoded library count data using an   uncertainty-aware probabilistic loss function",
        "text": " DEL experimental workflow under a frequentist view. We illustrate this approach on a dataset of 108k compounds screened against CAIX, and a dataset of 5.7M compounds screened against sEH and SIRT2. Due to the treatment of uncertainty in the data through t"
    },
    {
        "title": "Machine learning on DNA-encoded library count data using an   uncertainty-aware probabilistic loss function",
        "text": "he negative log-likelihood loss function, the models can ignore low-confidence outliers. While our approach does not demonstrate a benefit for extrapolation to novel structures, we expect our denoising and visualization pipeline to be useful in identifying"
    },
    {
        "title": "Machine learning on DNA-encoded library count data using an   uncertainty-aware probabilistic loss function",
        "text": " SAR trends and enriched pharmacophores in DEL data. Further, this approach to uncertainty-aware regression is applicable to other sparse or noisy datasets where the nature of stochasticity is known or can be modeled; in particular, the Poisson enrichment "
    },
    {
        "title": "Machine learning on DNA-encoded library count data using an   uncertainty-aware probabilistic loss function",
        "text": "ratio metric we use can apply to other settings that compare sequencing count data between two experimental conditions."
    },
    {
        "title": "VisualEnv: visual Gym environments with Blender",
        "text": "In this paper VisualEnv, a new tool for creating visual environment for reinforcement learning is introduced. It is the product of an integration of an open-source modelling and rendering software, Blender, and a python module used to generate environment "
    },
    {
        "title": "VisualEnv: visual Gym environments with Blender",
        "text": "model for simulation, OpenAI Gym. VisualEnv allows the user to create custom environments with photorealistic rendering capabilities and full integration with python. The framework is described and tested on a series of example problems that showcase its f"
    },
    {
        "title": "VisualEnv: visual Gym environments with Blender",
        "text": "eatures for training reinforcement learning agents."
    },
    {
        "title": "Analyzing Bias in Sensitive Personal Information Used to Train Financial   Models",
        "text": "Bias in data can have unintended consequences that propagate to the design, development, and deployment of machine learning models. In the financial services sector, this can result in discrimination from certain financial instruments and services. At the "
    },
    {
        "title": "Analyzing Bias in Sensitive Personal Information Used to Train Financial   Models",
        "text": "same time, data privacy is of paramount importance, and recent data breaches have seen reputational damage for large institutions. Presented in this paper is a trusted model-lifecycle management platform that attempts to ensure consumer data protection, an"
    },
    {
        "title": "Analyzing Bias in Sensitive Personal Information Used to Train Financial   Models",
        "text": "onymization, and fairness. Specifically, we examine how datasets can be reproduced using deep learning techniques to effectively retain important statistical features in datasets whilst simultaneously protecting data privacy and enabling safe and secure sh"
    },
    {
        "title": "Analyzing Bias in Sensitive Personal Information Used to Train Financial   Models",
        "text": "aring of sensitive personal information beyond the current state-of-practice."
    },
    {
        "title": "Insights into Data through Model Behaviour: An Explainability-driven   Strategy for Data Auditing for Responsible Computer Vision Applications",
        "text": "In this study, we take a departure and explore an explainability-driven strategy to data auditing, where actionable insights into the data at hand are discovered through the eyes of quantitative explainability on the behaviour of a dummy model prototype wh"
    },
    {
        "title": "Insights into Data through Model Behaviour: An Explainability-driven   Strategy for Data Auditing for Responsible Computer Vision Applications",
        "text": "en exposed to data. We demonstrate this strategy by auditing two popular medical benchmark datasets, and discover hidden data quality issues that lead deep learning models to make predictions for the wrong reasons. The actionable insights gained from this "
    },
    {
        "title": "Insights into Data through Model Behaviour: An Explainability-driven   Strategy for Data Auditing for Responsible Computer Vision Applications",
        "text": "explainability driven data auditing strategy is then leveraged to address the discovered issues to enable the creation of high-performing deep learning models with appropriate prediction behaviour. The hope is that such an explainability-driven strategy ca"
    },
    {
        "title": "Insights into Data through Model Behaviour: An Explainability-driven   Strategy for Data Auditing for Responsible Computer Vision Applications",
        "text": "n be complimentary to data-driven strategies to facilitate for more responsible development of machine learning algorithms for computer vision applications."
    },
    {
        "title": "Handling Incomplete Heterogeneous Data using VAEs",
        "text": "Variational autoencoders (VAEs), as well as other generative models, have been shown to be efficient and accurate for capturing the latent structure of vast amounts of complex high-dimensional data. However, existing VAEs can still not directly handle data"
    },
    {
        "title": "Handling Incomplete Heterogeneous Data using VAEs",
        "text": " that are heterogenous (mixed continuous and discrete) or incomplete (with missing data at random), which is indeed common in real-world applications. In this paper, we propose a general framework to design VAEs suitable for fitting incomplete heterogenous"
    },
    {
        "title": "Handling Incomplete Heterogeneous Data using VAEs",
        "text": " data. The proposed HI-VAE includes likelihood models for real-valued, positive real valued, interval, categorical, ordinal and count data, and allows accurate estimation (and potentially imputation) of missing data. Furthermore, HI-VAE presents competitiv"
    },
    {
        "title": "Handling Incomplete Heterogeneous Data using VAEs",
        "text": "e predictive performance in supervised tasks, outperforming supervised models when trained on incomplete data."
    },
    {
        "title": "A Self-Tuning Actor-Critic Algorithm",
        "text": "Reinforcement learning algorithms are highly sensitive to the choice of hyperparameters, typically requiring significant manual effort to identify hyperparameters that perform well on a new domain. In this paper, we take a step towards addressing this issu"
    },
    {
        "title": "A Self-Tuning Actor-Critic Algorithm",
        "text": "e by using metagradients to automatically adapt hyperparameters online by meta-gradient descent (Xu et al., 2018). We apply our algorithm, Self-Tuning Actor-Critic (STAC), to self-tune all the differentiable hyperparameters of an actor-critic loss function"
    },
    {
        "title": "A Self-Tuning Actor-Critic Algorithm",
        "text": ", to discover auxiliary tasks, and to improve off-policy learning using a novel leaky V-trace operator. STAC is simple to use, sample efficient and does not require a significant increase in compute. Ablative studies show that the overall performance of ST"
    },
    {
        "title": "A Self-Tuning Actor-Critic Algorithm",
        "text": "AC improved as we adapt more hyperparameters. When applied to the Arcade Learning Environment (Bellemare et al. 2012), STAC improved the median human normalized score in 200M steps from 243% to 364%. When applied to the DM Control suite (Tassa et al., 2018"
    },
    {
        "title": "A Self-Tuning Actor-Critic Algorithm",
        "text": "), STAC improved the mean score in 30M steps from 217 to 389 when learning with features, from 108 to 202 when learning from pixels, and from 195 to 295 in the Real-World Reinforcement Learning Challenge (Dulac-Arnold et al., 2020)."
    },
    {
        "title": "The continuous categorical: a novel simplex-valued exponential family",
        "text": "Simplex-valued data appear throughout statistics and machine learning, for example in the context of transfer learning and compression of deep networks. Existing models for this class of data rely on the Dirichlet distribution or other related loss functio"
    },
    {
        "title": "The continuous categorical: a novel simplex-valued exponential family",
        "text": "ns; here we show these standard choices suffer systematically from a number of limitations, including bias and numerical issues that frustrate the use of flexible network models upstream of these distributions. We resolve these limitations by introducing a"
    },
    {
        "title": "The continuous categorical: a novel simplex-valued exponential family",
        "text": " novel exponential family of distributions for modeling simplex-valued data - the continuous categorical, which arises as a nontrivial multivariate generalization of the recently discovered continuous Bernoulli. Unlike the Dirichlet and other typical choic"
    },
    {
        "title": "The continuous categorical: a novel simplex-valued exponential family",
        "text": "es, the continuous categorical results in a well-behaved probabilistic loss function that produces unbiased estimators, while preserving the mathematical simplicity of the Dirichlet. As well as exploring its theoretical properties, we introduce sampling me"
    },
    {
        "title": "The continuous categorical: a novel simplex-valued exponential family",
        "text": "thods for this distribution that are amenable to the reparameterization trick, and evaluate their performance. Lastly, we demonstrate that the continuous categorical outperforms standard choices empirically, across a simulation study, an applied example on"
    },
    {
        "title": "The continuous categorical: a novel simplex-valued exponential family",
        "text": " multi-party elections, and a neural network compression task."
    },
    {
        "title": "\"The Squawk Bot\": Joint Learning of Time Series and Text Data Modalities   for Automated Financial Information Filtering",
        "text": "Multimodal analysis that uses numerical time series and textual corpora as input data sources is becoming a promising approach, especially in the financial industry. However, the main focus of such analysis has been on achieving high prediction accuracy wh"
    },
    {
        "title": "\"The Squawk Bot\": Joint Learning of Time Series and Text Data Modalities   for Automated Financial Information Filtering",
        "text": "ile little effort has been spent on the important task of understanding the association between the two data modalities. Performance on the time series hence receives little explanation though human-understandable textual information is available. In this "
    },
    {
        "title": "\"The Squawk Bot\": Joint Learning of Time Series and Text Data Modalities   for Automated Financial Information Filtering",
        "text": "work, we address the problem of given a numerical time series, and a general corpus of textual stories collected in the same period of the time series, the task is to timely discover a succinct set of textual stories associated with that time series. Towar"
    },
    {
        "title": "\"The Squawk Bot\": Joint Learning of Time Series and Text Data Modalities   for Automated Financial Information Filtering",
        "text": "ds this goal, we propose a novel multi-modal neural model called MSIN that jointly learns both numerical time series and categorical text articles in order to unearth the association between them. Through multiple steps of data interrelation between the tw"
    },
    {
        "title": "\"The Squawk Bot\": Joint Learning of Time Series and Text Data Modalities   for Automated Financial Information Filtering",
        "text": "o data modalities, MSIN learns to focus on a small subset of text articles that best align with the performance in the time series. This succinct set is timely discovered and presented as recommended documents, acting as automated information filtering, fo"
    },
    {
        "title": "\"The Squawk Bot\": Joint Learning of Time Series and Text Data Modalities   for Automated Financial Information Filtering",
        "text": "r the given time series. We empirically evaluate the performance of our model on discovering relevant news articles for two stock time series from Apple and Google companies, along with the daily news articles collected from the Thomson Reuters over a peri"
    },
    {
        "title": "\"The Squawk Bot\": Joint Learning of Time Series and Text Data Modalities   for Automated Financial Information Filtering",
        "text": "od of seven consecutive years. The experimental results demonstrate that MSIN achieves up to 84.9% and 87.2% in recalling the ground truth articles respectively to the two examined time series, far more superior to state-of-the-art algorithms that rely on "
    },
    {
        "title": "\"The Squawk Bot\": Joint Learning of Time Series and Text Data Modalities   for Automated Financial Information Filtering",
        "text": "conventional attention mechanism in deep learning."
    },
    {
        "title": "Prediction with a Short Memory",
        "text": "We consider the problem of predicting the next observation given a sequence of past observations, and consider the extent to which accurate prediction requires complex algorithms that explicitly leverage long-range dependencies. Perhaps surprisingly, our p"
    },
    {
        "title": "Prediction with a Short Memory",
        "text": "ositive results show that for a broad class of sequences, there is an algorithm that predicts well on average, and bases its predictions only on the most recent few observation together with a set of simple summary statistics of the past observations. Spec"
    },
    {
        "title": "Prediction with a Short Memory",
        "text": "ifically, we show that for any distribution over observations, if the mutual information between past observations and future observations is upper bounded by $I$, then a simple Markov model over the most recent $I/\\epsilon$ observations obtains expected K"
    },
    {
        "title": "Prediction with a Short Memory",
        "text": "L error $\\epsilon$---and hence $\\ell_1$ error $\\sqrt{\\epsilon}$---with respect to the optimal predictor that has access to the entire past and knows the data generating distribution. For a Hidden Markov Model with $n$ hidden states, $I$ is bounded by $\\log"
    },
    {
        "title": "Prediction with a Short Memory",
        "text": " n$, a quantity that does not depend on the mixing time, and we show that the trivial prediction algorithm based on the empirical frequencies of length $O(\\log n/\\epsilon)$ windows of observations achieves this error, provided the length of the sequence is"
    },
    {
        "title": "Prediction with a Short Memory",
        "text": " $d^{\\Omega(\\log n/\\epsilon)}$, where $d$ is the size of the observation alphabet.   We also establish that this result cannot be improved upon, even for the class of HMMs, in the following two senses: First, for HMMs with $n$ hidden states, a window lengt"
    },
    {
        "title": "Prediction with a Short Memory",
        "text": "h of $\\log n/\\epsilon$ is information-theoretically necessary to achieve expected $\\ell_1$ error $\\sqrt{\\epsilon}$. Second, the $d^{\\Theta(\\log n/\\epsilon)}$ samples required to estimate the Markov model for an observation alphabet of size $d$ is necessary"
    },
    {
        "title": "Prediction with a Short Memory",
        "text": " for any computationally tractable learning algorithm, assuming the hardness of strongly refuting a certain class of CSPs."
    },
    {
        "title": "HyperSAGE: Generalizing Inductive Representation Learning on Hypergraphs",
        "text": "Graphs are the most ubiquitous form of structured data representation used in machine learning. They model, however, only pairwise relations between nodes and are not designed for encoding the higher-order relations found in many real-world datasets. To mo"
    },
    {
        "title": "HyperSAGE: Generalizing Inductive Representation Learning on Hypergraphs",
        "text": "del such complex relations, hypergraphs have proven to be a natural representation. Learning the node representations in a hypergraph is more complex than in a graph as it involves information propagation at two levels: within every hyperedge and across th"
    },
    {
        "title": "HyperSAGE: Generalizing Inductive Representation Learning on Hypergraphs",
        "text": "e hyperedges. Most current approaches first transform a hypergraph structure to a graph for use in existing geometric deep learning algorithms. This transformation leads to information loss, and sub-optimal exploitation of the hypergraph's expressive power"
    },
    {
        "title": "HyperSAGE: Generalizing Inductive Representation Learning on Hypergraphs",
        "text": ". We present HyperSAGE, a novel hypergraph learning framework that uses a two-level neural message passing strategy to accurately and efficiently propagate information through hypergraphs. The flexible design of HyperSAGE facilitates different ways of aggr"
    },
    {
        "title": "HyperSAGE: Generalizing Inductive Representation Learning on Hypergraphs",
        "text": "egating neighborhood information. Unlike the majority of related work which is transductive, our approach, inspired by the popular GraphSAGE method, is inductive. Thus, it can also be used on previously unseen nodes, facilitating deployment in problems suc"
    },
    {
        "title": "HyperSAGE: Generalizing Inductive Representation Learning on Hypergraphs",
        "text": "h as evolving or partially observed hypergraphs. Through extensive experimentation, we show that HyperSAGE outperforms state-of-the-art hypergraph learning methods on representative benchmark datasets. We also demonstrate that the higher expressive power o"
    },
    {
        "title": "HyperSAGE: Generalizing Inductive Representation Learning on Hypergraphs",
        "text": "f HyperSAGE makes it more stable in learning node representations as compared to the alternatives."
    },
    {
        "title": "Deep Learning using Rectified Linear Units (ReLU)",
        "text": "We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have"
    },
    {
        "title": "Deep Learning using Rectified Linear Units (ReLU)",
        "text": " been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer $h_{n - 1}$ in a neural network, then multiply it by weight parameters"
    },
    {
        "title": "Deep Learning using Rectified Linear Units (ReLU)",
        "text": " $\\theta$ to get the raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$, i.e. $f(o) = \\max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide class predictions $\\hat{y}$ through argmax function, i.e. argmax $f(x)$."
    },
    {
        "title": "An Auxiliary Classifier Generative Adversarial Framework for Relation   Extraction",
        "text": "Relation extraction models suffer from limited qualified training data. Using human annotators to label sentences is too expensive and does not scale well especially when dealing with large datasets. In this paper, we use Auxiliary Classifier Generative Ad"
    },
    {
        "title": "An Auxiliary Classifier Generative Adversarial Framework for Relation   Extraction",
        "text": "versarial Networks (AC-GANs) to generate high-quality relational sentences and to improve the performance of relation classifier in end-to-end models. In AC-GAN, the discriminator gives not only a probability distribution over the real source, but also a p"
    },
    {
        "title": "An Auxiliary Classifier Generative Adversarial Framework for Relation   Extraction",
        "text": "robability distribution over the relation labels. This helps to generate meaningful relational sentences. Experimental results show that our proposed data augmentation method significantly improves the performance of relation extraction compared to state-o"
    },
    {
        "title": "An Auxiliary Classifier Generative Adversarial Framework for Relation   Extraction",
        "text": "f-the-art methods"
    },
    {
        "title": "Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning   the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP",
        "text": "Graph neural networks have triggered a resurgence of graph-based text classification methods, defining today's state of the art. We show that a wide multi-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent graph-based models TextGCN a"
    },
    {
        "title": "Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning   the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP",
        "text": "nd HeteGCN in an inductive text classification setting and is comparable with HyperGAT. Moreover, we fine-tune a sequence-based BERT and a lightweight DistilBERT model, which both outperform all state-of-the-art models. These results question the importanc"
    },
    {
        "title": "Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning   the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP",
        "text": "e of synthetic graphs used in modern text classifiers. In terms of efficiency, DistilBERT is still twice as large as our BoW-based wide MLP, while graph-based models like TextGCN require setting up an $\\mathcal{O}(N^2)$ graph, where $N$ is the vocabulary p"
    },
    {
        "title": "Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning   the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP",
        "text": "lus corpus size. Finally, since Transformers need to compute $\\mathcal{O}(L^2)$ attention weights with sequence length $L$, the MLP models show higher training and inference speeds on datasets with long sequences."
    },
    {
        "title": "Deep Neural Networks for Physics Analysis on low-level whole-detector   data at the LHC",
        "text": "There has been considerable recent activity applying deep convolutional neural nets (CNNs) to data from particle physics experiments. Current approaches on ATLAS/CMS have largely focussed on a subset of the calorimeter, and for identifying objects or parti"
    },
    {
        "title": "Deep Neural Networks for Physics Analysis on low-level whole-detector   data at the LHC",
        "text": "cular particle types. We explore approaches that use the entire calorimeter, combined with track information, for directly conducting physics analyses: i.e. classifying events as known-physics background or new-physics signals.   We use an existing RPV-Sup"
    },
    {
        "title": "Deep Neural Networks for Physics Analysis on low-level whole-detector   data at the LHC",
        "text": "ersymmetry analysis as a case study and explore CNNs on multi-channel, high-resolution sparse images: applied on GPU and multi-node CPU architectures (including Knights Landing (KNL) Xeon Phi nodes) on the Cori supercomputer at NERSC."
    },
    {
        "title": "In-training Matrix Factorization for Parameter-frugal Neural Machine   Translation",
        "text": "In this paper, we propose the use of in-training matrix factorization to reduce the model size for neural machine translation. Using in-training matrix factorization, parameter matrices may be decomposed into the products of smaller matrices, which can com"
    },
    {
        "title": "In-training Matrix Factorization for Parameter-frugal Neural Machine   Translation",
        "text": "press large machine translation architectures by vastly reducing the number of learnable parameters. We apply in-training matrix factorization to different layers of standard neural architectures and show that in-training factorization is capable of reduci"
    },
    {
        "title": "In-training Matrix Factorization for Parameter-frugal Neural Machine   Translation",
        "text": "ng nearly 50% of learnable parameters without any associated loss in BLEU score. Further, we find that in-training matrix factorization is especially powerful on embedding layers, providing a simple and effective method to curtail the number of parameters "
    },
    {
        "title": "In-training Matrix Factorization for Parameter-frugal Neural Machine   Translation",
        "text": "with minimal impact on model performance, and, at times, an increase in performance."
    },
    {
        "title": "A Comprehensive Review of Deep Learning-based Single Image   Super-resolution",
        "text": "Image super-resolution (SR) is one of the vital image processing methods that improve the resolution of an image in the field of computer vision. In the last two decades, significant progress has been made in the field of super-resolution, especially by ut"
    },
    {
        "title": "A Comprehensive Review of Deep Learning-based Single Image   Super-resolution",
        "text": "ilizing deep learning methods. This survey is an effort to provide a detailed survey of recent progress in single-image super-resolution in the perspective of deep learning while also informing about the initial classical methods used for image super-resol"
    },
    {
        "title": "A Comprehensive Review of Deep Learning-based Single Image   Super-resolution",
        "text": "ution. The survey classifies the image SR methods into four categories, i.e., classical methods, supervised learning-based methods, unsupervised learning-based methods, and domain-specific SR methods. We also introduce the problem of SR to provide intuitio"
    },
    {
        "title": "A Comprehensive Review of Deep Learning-based Single Image   Super-resolution",
        "text": "n about image quality metrics, available reference datasets, and SR challenges. Deep learning-based approaches of SR are evaluated using a reference dataset. Some of the reviewed state-of-the-art image SR methods include the enhanced deep SR network (EDSR)"
    },
    {
        "title": "A Comprehensive Review of Deep Learning-based Single Image   Super-resolution",
        "text": ", cycle-in-cycle GAN (CinCGAN), multiscale residual network (MSRN), meta residual dense network (Meta-RDN), recurrent back-projection network (RBPN), second-order attention network (SAN), SR feedback network (SRFBN) and the wavelet-based residual attention"
    },
    {
        "title": "A Comprehensive Review of Deep Learning-based Single Image   Super-resolution",
        "text": " network (WRAN). Finally, this survey is concluded with future directions and trends in SR and open problems in SR to be addressed by the researchers."
    },
    {
        "title": "Learning Languages in the Limit from Positive Information with Finitely   Many Memory Changes",
        "text": "We investigate learning collections of languages from texts by an inductive inference machine with access to the current datum and a bounded memory in form of states. Such a bounded memory states (BMS) learner is considered successful in case it eventually"
    },
    {
        "title": "Learning Languages in the Limit from Positive Information with Finitely   Many Memory Changes",
        "text": " settles on a correct hypothesis while exploiting only finitely many different states.   We give the complete map of all pairwise relations for an established collection of criteria of successfull learning. Most prominently, we show that non-U-shapedness i"
    },
    {
        "title": "Learning Languages in the Limit from Positive Information with Finitely   Many Memory Changes",
        "text": "s not restrictive, while conservativeness and (strong) monotonicity are. Some results carry over from iterative learning by a general lemma showing that, for a wealth of restrictions (the semantic restrictions), iterative and bounded memory states learning"
    },
    {
        "title": "Learning Languages in the Limit from Positive Information with Finitely   Many Memory Changes",
        "text": " are equivalent. We also give an example of a non-semantic restriction (strongly non-U-shapedness) where the two settings differ."
    },
    {
        "title": "Towards Making the Most of BERT in Neural Machine Translation",
        "text": "GPT-2 and BERT demonstrate the effectiveness of using pre-trained language models (LMs) on various natural language processing tasks. However, LM fine-tuning often suffers from catastrophic forgetting when applied to resource-rich tasks. In this work, we i"
    },
    {
        "title": "Towards Making the Most of BERT in Neural Machine Translation",
        "text": "ntroduce a concerted training framework (\\method) that is the key to integrate the pre-trained LMs to neural machine translation (NMT). Our proposed Cnmt consists of three techniques: a) asymptotic distillation to ensure that the NMT model can retain the p"
    },
    {
        "title": "Towards Making the Most of BERT in Neural Machine Translation",
        "text": "revious pre-trained knowledge; b) a dynamic switching gate to avoid catastrophic forgetting of pre-trained knowledge; and c) a strategy to adjust the learning paces according to a scheduled policy. Our experiments in machine translation show \\method gains "
    },
    {
        "title": "Towards Making the Most of BERT in Neural Machine Translation",
        "text": "of up to 3 BLEU score on the WMT14 English-German language pair which even surpasses the previous state-of-the-art pre-training aided NMT by 1.4 BLEU score. While for the large WMT14 English-French task with 40 millions of sentence-pairs, our base model st"
    },
    {
        "title": "Towards Making the Most of BERT in Neural Machine Translation",
        "text": "ill significantly improves upon the state-of-the-art Transformer big model by more than 1 BLEU score."
    },
    {
        "title": "Regularized Training of Intermediate Layers for Generative Models for   Inverse Problems",
        "text": "Generative Adversarial Networks (GANs) have been shown to be powerful and flexible priors when solving inverse problems. One challenge of using them is overcoming representation error, the fundamental limitation of the network in representing any particula"
    },
    {
        "title": "Regularized Training of Intermediate Layers for Generative Models for   Inverse Problems",
        "text": "r signal. Recently, multiple proposed inversion algorithms reduce representation error by optimizing over intermediate layer representations. These methods are typically applied to generative models that were trained agnostic of the downstream inversion al"
    },
    {
        "title": "Regularized Training of Intermediate Layers for Generative Models for   Inverse Problems",
        "text": "gorithm. In our work, we introduce a principle that if a generative model is intended for inversion using an algorithm based on optimization of intermediate layers, it should be trained in a way that regularizes those intermediate layers. We instantiate th"
    },
    {
        "title": "Regularized Training of Intermediate Layers for Generative Models for   Inverse Problems",
        "text": "is principle for two notable recent inversion algorithms: Intermediate Layer Optimization and the Multi-Code GAN prior. For both of these inversion algorithms, we introduce a new regularized GAN training algorithm and demonstrate that the learned generativ"
    },
    {
        "title": "Regularized Training of Intermediate Layers for Generative Models for   Inverse Problems",
        "text": "e model results in lower reconstruction errors across a wide range of under sampling ratios when solving compressed sensing, inpainting, and super-resolution problems."
    },
    {
        "title": "A Deep Learning Based Automated Hand Hygiene Training System",
        "text": "Hand hygiene is crucial for preventing viruses and infections. Due to the pervasive outbreak of COVID-19, wearing a mask and hand hygiene appear to be the most effective ways for the public to curb the spread of these viruses. The World Health Organization"
    },
    {
        "title": "A Deep Learning Based Automated Hand Hygiene Training System",
        "text": " (WHO) recommends a guideline for alcohol-based hand rub in eight steps to ensure that all surfaces of hands are entirely clean. As these steps involve complex gestures, human assessment of them lacks enough accuracy. However, Deep Neural Network (DNN) and"
    },
    {
        "title": "A Deep Learning Based Automated Hand Hygiene Training System",
        "text": " machine vision have made it possible to accurately evaluate hand rubbing quality for the purposes of training and feedback. In this paper, an automated deep learning based hand rub assessment system with real-time feedback is presented. The system evaluat"
    },
    {
        "title": "A Deep Learning Based Automated Hand Hygiene Training System",
        "text": "es the compliance with the 8-step guideline using a DNN architecture trained on a dataset of videos collected from volunteers with various skin tones and hand characteristics following the hand rubbing guideline. Various DNN architectures were tested, and "
    },
    {
        "title": "A Deep Learning Based Automated Hand Hygiene Training System",
        "text": "an Inception-ResNet model led to the best results with 97% test accuracy. In the proposed system, an NVIDIA Jetson AGX Xavier embedded board runs the software. The efficacy of the system is evaluated in a concrete situation of being used by various users, "
    },
    {
        "title": "A Deep Learning Based Automated Hand Hygiene Training System",
        "text": "and challenging steps are identified. In this experiment, the average time taken by the hand rubbing steps among volunteers is 27.2 seconds, which conforms to the WHO guidelines."
    },
    {
        "title": "Evaluating Off-the-Shelf Machine Listening and Natural Language Models   for Automated Audio Captioning",
        "text": "Automated audio captioning (AAC) is the task of automatically generating textual descriptions for general audio signals. A captioning system has to identify various information from the input signal and express it with natural language. Existing works main"
    },
    {
        "title": "Evaluating Off-the-Shelf Machine Listening and Natural Language Models   for Automated Audio Captioning",
        "text": "ly focus on investigating new methods and try to improve their performance measured on existing datasets. Having attracted attention only recently, very few works on AAC study the performance of existing pre-trained audio and natural language processing re"
    },
    {
        "title": "Evaluating Off-the-Shelf Machine Listening and Natural Language Models   for Automated Audio Captioning",
        "text": "sources. In this paper, we evaluate the performance of off-the-shelf models with a Transformer-based captioning approach. We utilize the freely available Clotho dataset to compare four different pre-trained machine listening models, four word embedding mod"
    },
    {
        "title": "Evaluating Off-the-Shelf Machine Listening and Natural Language Models   for Automated Audio Captioning",
        "text": "els, and their combinations in many different settings. Our evaluation suggests that YAMNet combined with BERT embeddings produces the best captions. Moreover, in general, fine-tuning pre-trained word embeddings can lead to better performance. Finally, we "
    },
    {
        "title": "Evaluating Off-the-Shelf Machine Listening and Natural Language Models   for Automated Audio Captioning",
        "text": "show that sequences of audio embeddings can be processed using a Transformer encoder to produce higher-quality captions."
    },
    {
        "title": "Truth Discovery in Sequence Labels from Crowds",
        "text": "Annotations quality and quantity positively affect the performance of sequence labeling, a vital task in Natural Language Processing. Hiring domain experts to annotate a corpus set is very costly in terms of money and time. Crowdsourcing platforms, such as"
    },
    {
        "title": "Truth Discovery in Sequence Labels from Crowds",
        "text": " Amazon Mechanical Turk (AMT), have been deployed to assist in this purpose. However, these platforms are prone to human errors due to the lack of expertise; hence, one worker's annotations cannot be directly used to train the model. Existing literature in"
    },
    {
        "title": "Truth Discovery in Sequence Labels from Crowds",
        "text": " annotation aggregation more focuses on binary or multi-choice problems. In recent years, handling the sequential label aggregation tasks on imbalanced datasets with complex dependencies between tokens has been challenging. To conquer the challenge, we pro"
    },
    {
        "title": "Truth Discovery in Sequence Labels from Crowds",
        "text": "pose an optimization-based method that infers the best set of aggregated annotations using labels provided by workers. The proposed Aggregation method for Sequential Labels from Crowds ($AggSLC$) jointly considers the characteristics of sequential labeling"
    },
    {
        "title": "Truth Discovery in Sequence Labels from Crowds",
        "text": " tasks, workers' reliabilities, and advanced machine learning techniques. We evaluate $AggSLC$ on different crowdsourced data for Named Entity Recognition (NER), Information Extraction tasks in biomedical (PICO), and the simulated dataset. Our results show"
    },
    {
        "title": "Truth Discovery in Sequence Labels from Crowds",
        "text": " that the proposed method outperforms the state-of-the-art aggregation methods. To achieve insights into the framework, we study $AggSLC$ components' effectiveness through ablation studies by evaluating our model in the absence of the prediction module and"
    },
    {
        "title": "Truth Discovery in Sequence Labels from Crowds",
        "text": " inconsistency loss function. Theoretical analysis of our algorithm's convergence points that the proposed $AggSLC$ halts after a finite number of iterations."
    },
    {
        "title": "Online Optimization with Costly and Noisy Measurements using Random   Fourier Expansions",
        "text": "This paper analyzes DONE, an online optimization algorithm that iteratively minimizes an unknown function based on costly and noisy measurements. The algorithm maintains a surrogate of the unknown function in the form of a random Fourier expansion (RFE). T"
    },
    {
        "title": "Online Optimization with Costly and Noisy Measurements using Random   Fourier Expansions",
        "text": "he surrogate is updated whenever a new measurement is available, and then used to determine the next measurement point. The algorithm is comparable to Bayesian optimization algorithms, but its computational complexity per iteration does not depend on the n"
    },
    {
        "title": "Online Optimization with Costly and Noisy Measurements using Random   Fourier Expansions",
        "text": "umber of measurements. We derive several theoretical results that provide insight on how the hyper-parameters of the algorithm should be chosen. The algorithm is compared to a Bayesian optimization algorithm for a benchmark problem and three applications, "
    },
    {
        "title": "Online Optimization with Costly and Noisy Measurements using Random   Fourier Expansions",
        "text": "namely, optical coherence tomography, optical beam-forming network tuning, and robot arm control. It is found that the DONE algorithm is significantly faster than Bayesian optimization in the discussed problems, while achieving a similar or better performa"
    },
    {
        "title": "Online Optimization with Costly and Noisy Measurements using Random   Fourier Expansions",
        "text": "nce."
    },
    {
        "title": "Causal Decision Making and Causal Effect Estimation Are Not the Same...   and Why It Matters",
        "text": "Causal decision making (CDM) based on machine learning has become a routine part of business. Businesses algorithmically target offers, incentives, and recommendations to affect consumer behavior. Recently, we have seen an acceleration of research related "
    },
    {
        "title": "Causal Decision Making and Causal Effect Estimation Are Not the Same...   and Why It Matters",
        "text": "to CDM and causal effect estimation (CEE) using machine-learned models. This article highlights an important perspective: CDM is not the same as CEE, and counterintuitively, accurate CEE is not necessary for accurate CDM. Our experience is that this is not"
    },
    {
        "title": "Causal Decision Making and Causal Effect Estimation Are Not the Same...   and Why It Matters",
        "text": " well understood by practitioners or most researchers. Technically, the estimand of interest is different, and this has important implications both for modeling and for the use of statistical models for CDM. We draw on prior research to highlight three imp"
    },
    {
        "title": "Causal Decision Making and Causal Effect Estimation Are Not the Same...   and Why It Matters",
        "text": "lications. (1) We should consider carefully the objective function of the causal machine learning, and if possible, optimize for accurate treatment assignment rather than for accurate effect-size estimation. (2) Confounding does not have the same effect on"
    },
    {
        "title": "Causal Decision Making and Causal Effect Estimation Are Not the Same...   and Why It Matters",
        "text": " CDM as it does on CEE. The upshot is that for supporting CDM it may be just as good or even better to learn with confounded data as with unconfounded data. Finally, (3) causal statistical modeling may not be necessary to support CDM because a proxy target"
    },
    {
        "title": "Causal Decision Making and Causal Effect Estimation Are Not the Same...   and Why It Matters",
        "text": " for statistical modeling might do as well or better. This third observation helps to explain at least one broad common CDM practice that seems wrong at first blush: the widespread use of non-causal models for targeting interventions. The last two implicat"
    },
    {
        "title": "Causal Decision Making and Causal Effect Estimation Are Not the Same...   and Why It Matters",
        "text": "ions are particularly important in practice, as acquiring (unconfounded) data on all counterfactuals can be costly and often impracticable. These observations open substantial research ground. We hope to facilitate research in this area by pointing to rela"
    },
    {
        "title": "Causal Decision Making and Causal Effect Estimation Are Not the Same...   and Why It Matters",
        "text": "ted articles from multiple contributing fields, including two dozen articles published the last three to four years."
    },
    {
        "title": "Zero-Shot Multi-View Indoor Localization via Graph Location Networks",
        "text": "Indoor localization is a fundamental problem in location-based applications. Current approaches to this problem typically rely on Radio Frequency technology, which requires not only supporting infrastructures but human efforts to measure and calibrate the "
    },
    {
        "title": "Zero-Shot Multi-View Indoor Localization via Graph Location Networks",
        "text": "signal. Moreover, data collection for all locations is indispensable in existing methods, which in turn hinders their large-scale deployment. In this paper, we propose a novel neural network based architecture Graph Location Networks (GLN) to perform infra"
    },
    {
        "title": "Zero-Shot Multi-View Indoor Localization via Graph Location Networks",
        "text": "structure-free, multi-view image based indoor localization. GLN makes location predictions based on robust location representations extracted from images through message-passing networks. Furthermore, we introduce a novel zero-shot indoor localization sett"
    },
    {
        "title": "Zero-Shot Multi-View Indoor Localization via Graph Location Networks",
        "text": "ing and tackle it by extending the proposed GLN to a dedicated zero-shot version, which exploits a novel mechanism Map2Vec to train location-aware embeddings and make predictions on novel unseen locations. Our extensive experiments show that the proposed a"
    },
    {
        "title": "Zero-Shot Multi-View Indoor Localization via Graph Location Networks",
        "text": "pproach outperforms state-of-the-art methods in the standard setting, and achieves promising accuracy even in the zero-shot setting where data for half of the locations are not available. The source code and datasets are publicly available at https://githu"
    },
    {
        "title": "Zero-Shot Multi-View Indoor Localization via Graph Location Networks",
        "text": "b.com/coldmanck/zero-shot-indoor-localization-release."
    },
    {
        "title": "An Approach to Evaluating Learning Algorithms for Decision Trees",
        "text": "Learning algorithms produce software models for realising critical classification tasks. Decision trees models are simpler than other models such as neural network and they are used in various critical domains such as the medical and the aeronautics. Low o"
    },
    {
        "title": "An Approach to Evaluating Learning Algorithms for Decision Trees",
        "text": "r unknown learning ability algorithms does not permit us to trust the produced software models, which lead to costly test activities for validating the models and to the waste of learning time in case the models are likely to be faulty due to the learning "
    },
    {
        "title": "An Approach to Evaluating Learning Algorithms for Decision Trees",
        "text": "inability. Methods for evaluating the decision trees learning ability, as well as that for the other models, are needed especially since the testing of the learned models is still a hot topic. We propose a novel oracle-centered approach to evaluate (the le"
    },
    {
        "title": "An Approach to Evaluating Learning Algorithms for Decision Trees",
        "text": "arning ability of) learning algorithms for decision trees. It consists of generating data from reference trees playing the role of oracles, producing learned trees with existing learning algorithms, and determining the degree of correctness (DOE) of the le"
    },
    {
        "title": "An Approach to Evaluating Learning Algorithms for Decision Trees",
        "text": "arned trees by comparing them with the oracles. The average DOE is used to estimate the quality of the learning algorithm. the We assess five decision tree learning algorithms based on the proposed approach."
    },
    {
        "title": "Development of swarm behavior in artificial learning agents that adapt   to different foraging environments",
        "text": "Collective behavior, and swarm formation in particular, has been studied from several perspectives within a large variety of fields, ranging from biology to physics. In this work, we apply Projective Simulation to model each individual as an artificial lea"
    },
    {
        "title": "Development of swarm behavior in artificial learning agents that adapt   to different foraging environments",
        "text": "rning agent that interacts with its neighbors and surroundings in order to make decisions and learn from them. Within a reinforcement learning framework, we discuss one-dimensional learning scenarios where agents need to get to food resources to be rewarde"
    },
    {
        "title": "Development of swarm behavior in artificial learning agents that adapt   to different foraging environments",
        "text": "d. We observe how different types of collective motion emerge depending on the distance the agents need to travel to reach the resources. For instance, strongly aligned swarms emerge when the food source is placed far away from the region where agents are "
    },
    {
        "title": "Development of swarm behavior in artificial learning agents that adapt   to different foraging environments",
        "text": "situated initially. In addition, we study the properties of the individual trajectories that occur within the different types of emergent collective dynamics. Agents trained to find distant resources exhibit individual trajectories with L\\'evy-like charact"
    },
    {
        "title": "Development of swarm behavior in artificial learning agents that adapt   to different foraging environments",
        "text": "eristics as a consequence of the collective motion, whereas agents trained to reach nearby resources present Brownian-like trajectories."
    },
    {
        "title": "Towards a Universal Theory of Artificial Intelligence based on   Algorithmic Probability and Sequential Decision Theory",
        "text": "Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown distri"
    },
    {
        "title": "Towards a Universal Theory of Artificial Intelligence based on   Algorithmic Probability and Sequential Decision Theory",
        "text": "bution. We unify both theories and give strong arguments that the resulting universal AIXI model behaves optimal in any computable environment. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modifi"
    },
    {
        "title": "Towards a Universal Theory of Artificial Intelligence based on   Algorithmic Probability and Sequential Decision Theory",
        "text": "ed algorithm AIXI^tl, which is still superior to any other time t and space l bounded agent. The computation time of AIXI^tl is of the order t x 2^l."
    },
    {
        "title": "Summarizing Videos with Attention",
        "text": "In this work we propose a novel method for supervised, keyshots based video summarization by applying a conceptually simple and computationally efficient soft, self-attention mechanism. Current state of the art methods leverage bi-directional recurrent net"
    },
    {
        "title": "Summarizing Videos with Attention",
        "text": "works such as BiLSTM combined with attention. These networks are complex to implement and computationally demanding compared to fully connected networks. To that end we propose a simple, self-attention based network for video summarization which performs t"
    },
    {
        "title": "Summarizing Videos with Attention",
        "text": "he entire sequence to sequence transformation in a single feed forward pass and single backward pass during training. Our method sets a new state of the art results on two benchmarks TvSum and SumMe, commonly used in this domain."
    },
    {
        "title": "Understanding Agent Incentives using Causal Influence Diagrams. Part I:   Single Action Settings",
        "text": "Agents are systems that optimize an objective function in an environment. Together, the goal and the environment induce secondary objectives, incentives. Modeling the agent-environment interaction using causal influence diagrams, we can answer two fundamen"
    },
    {
        "title": "Understanding Agent Incentives using Causal Influence Diagrams. Part I:   Single Action Settings",
        "text": "tal questions about an agent's incentives directly from the graph: (1) which nodes can the agent have an incentivize to observe, and (2) which nodes can the agent have an incentivize to control? The answers tell us which information and influence points ne"
    },
    {
        "title": "Understanding Agent Incentives using Causal Influence Diagrams. Part I:   Single Action Settings",
        "text": "ed extra protection. For example, we may want a classifier for job applications to not use the ethnicity of the candidate, and a reinforcement learning agent not to take direct control of its reward mechanism. Different algorithms and training paradigms ca"
    },
    {
        "title": "Understanding Agent Incentives using Causal Influence Diagrams. Part I:   Single Action Settings",
        "text": "n lead to different causal influence diagrams, so our method can be used to identify algorithms with problematic incentives and help in designing algorithms with better incentives."
    },
    {
        "title": "Privacy Threats Against Federated Matrix Factorization",
        "text": "Matrix Factorization has been very successful in practical recommendation applications and e-commerce. Due to data shortage and stringent regulations, it can be hard to collect sufficient data to build performant recommender systems for a single company. F"
    },
    {
        "title": "Privacy Threats Against Federated Matrix Factorization",
        "text": "ederated learning provides the possibility to bridge the data silos and build machine learning models without compromising privacy and security. Participants sharing common users or items collaboratively build a model over data from all the participants. T"
    },
    {
        "title": "Privacy Threats Against Federated Matrix Factorization",
        "text": "here have been some works exploring the application of federated learning to recommender systems and the privacy issues in collaborative filtering systems. However, the privacy threats in federated matrix factorization are not studied. In this paper, we ca"
    },
    {
        "title": "Privacy Threats Against Federated Matrix Factorization",
        "text": "tegorize federated matrix factorization into three types based on the partition of feature space and analyze privacy threats against each type of federated matrix factorization model. We also discuss privacy-preserving approaches. As far as we are aware, t"
    },
    {
        "title": "Privacy Threats Against Federated Matrix Factorization",
        "text": "his is the first study of privacy threats of the matrix factorization method in the federated learning framework."
    },
    {
        "title": "Active Learning Solution on Distributed Edge Computing",
        "text": "Industry 4.0 becomes possible through the convergence between Operational and Information Technologies. All the requirements to realize the convergence is integrated on the Fog Platform. Fog Platform is introduced between the cloud server and edge devices "
    },
    {
        "title": "Active Learning Solution on Distributed Edge Computing",
        "text": "when the unprecedented generation of data causes the burden of the cloud server, leading the ineligible latency. In this new paradigm, we divide the computation tasks and push it down to edge devices. Furthermore, local computing (at edge side) may improve"
    },
    {
        "title": "Active Learning Solution on Distributed Edge Computing",
        "text": " privacy and trust. To address these problems, we present a new method, in which we decompose the data aggregation and processing, by dividing them between edge devices and fog nodes intelligently. We apply active learning on edge devices; and federated le"
    },
    {
        "title": "Active Learning Solution on Distributed Edge Computing",
        "text": "arning on the fog node which significantly reduces the data samples to train the model as well as the communication cost. To show the effectiveness of the proposed method, we implemented and evaluated its performance for an image classification task. In ad"
    },
    {
        "title": "Active Learning Solution on Distributed Edge Computing",
        "text": "dition, we consider two settings: massively distributed and non-massively distributed and offer the corresponding solutions."
    },
    {
        "title": "Non-Greedy L21-Norm Maximization for Principal Component Analysis",
        "text": "Principal Component Analysis (PCA) is one of the most important unsupervised methods to handle high-dimensional data. However, due to the high computational complexity of its eigen decomposition solution, it hard to apply PCA to the large-scale data with h"
    },
    {
        "title": "Non-Greedy L21-Norm Maximization for Principal Component Analysis",
        "text": "igh dimensionality. Meanwhile, the squared L2-norm based objective makes it sensitive to data outliers. In recent research, the L1-norm maximization based PCA method was proposed for efficient computation and being robust to outliers. However, this work us"
    },
    {
        "title": "Non-Greedy L21-Norm Maximization for Principal Component Analysis",
        "text": "ed a greedy strategy to solve the eigen vectors. Moreover, the L1-norm maximization based objective may not be the correct robust PCA formulation, because it loses the theoretical connection to the minimization of data reconstruction error, which is one of"
    },
    {
        "title": "Non-Greedy L21-Norm Maximization for Principal Component Analysis",
        "text": " the most important intuitions and goals of PCA. In this paper, we propose to maximize the L21-norm based robust PCA objective, which is theoretically connected to the minimization of reconstruction error. More importantly, we propose the efficient non-gre"
    },
    {
        "title": "Non-Greedy L21-Norm Maximization for Principal Component Analysis",
        "text": "edy optimization algorithms to solve our objective and the more general L21-norm maximization problem with theoretically guaranteed convergence. Experimental results on real world data sets show the effectiveness of the proposed method for principal compon"
    },
    {
        "title": "Non-Greedy L21-Norm Maximization for Principal Component Analysis",
        "text": "ent analysis."
    },
    {
        "title": "GitTables: A Large-Scale Corpus of Relational Tables",
        "text": "The success of deep learning has sparked interest in improving relational table tasks, like data preparation and search, with table representation models trained on large table corpora. Existing table corpora primarily contain tables extracted from HTML pa"
    },
    {
        "title": "GitTables: A Large-Scale Corpus of Relational Tables",
        "text": "ges, limiting the capability to represent offline database tables. To train and evaluate high-capacity models for applications beyond the Web, we need resources with tables that resemble relational database tables. Here we introduce GitTables, a corpus of "
    },
    {
        "title": "GitTables: A Large-Scale Corpus of Relational Tables",
        "text": "1M relational tables extracted from GitHub. Our continuing curation aims at growing the corpus to at least 10M tables. Analyses of GitTables show that its structure, content, and topical coverage differ significantly from existing table corpora. We annotat"
    },
    {
        "title": "GitTables: A Large-Scale Corpus of Relational Tables",
        "text": "e table columns in GitTables with semantic types, hierarchical relations and descriptions from Schema.org and DBpedia. The evaluation of our annotation pipeline on the T2Dv2 benchmark illustrates that our approach provides results on par with human annotat"
    },
    {
        "title": "GitTables: A Large-Scale Corpus of Relational Tables",
        "text": "ions. We present three applications of GitTables, demonstrating its value for learned semantic type detection models, schema completion methods, and benchmarks for table-to-KG matching, data search, and preparation. We make the corpus and code available at"
    },
    {
        "title": "GitTables: A Large-Scale Corpus of Relational Tables",
        "text": " https://gittables.github.io."
    },
    {
        "title": "Estimation of Rate Control Parameters for Video Coding Using CNN",
        "text": "Rate-control is essential to ensure efficient video delivery. Typical rate-control algorithms rely on bit allocation strategies, to appropriately distribute bits among frames. As reference frames are essential for exploiting temporal redundancies, intra fr"
    },
    {
        "title": "Estimation of Rate Control Parameters for Video Coding Using CNN",
        "text": "ames are usually assigned a larger portion of the available bits. In this paper, an accurate method to estimate number of bits and quality of intra frames is proposed, which can be used for bit allocation in a rate-control scheme. The algorithm is based on"
    },
    {
        "title": "Estimation of Rate Control Parameters for Video Coding Using CNN",
        "text": " deep learning, where networks are trained using the original frames as inputs, while distortions and sizes of compressed frames after encoding are used as ground truths. Two approaches are proposed where either local or global distortions are predicted."
    },
    {
        "title": "Online Learned Continual Compression with Adaptive Quantization Modules",
        "text": "We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-e"
    },
    {
        "title": "Online Learned Continual Compression with Adaptive Quantization Modules",
        "text": "ncoders in this setting encounters a major challenge: representations derived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantiz"
    },
    {
        "title": "Online Learned Continual Compression with Adaptive Quantization Modules",
        "text": "ation Modules (AQM) to control variation in the compression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and current progr"
    },
    {
        "title": "Online Learned Continual Compression with Adaptive Quantization Modules",
        "text": "ess of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to significant gains on co"
    },
    {
        "title": "Online Learned Continual Compression with Adaptive Quantization Modules",
        "text": "ntinual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learning environments."
    },
    {
        "title": "Malware Classification Using Transfer Learning",
        "text": "With the rapid growth of the number of devices on the Internet, malware poses a threat not only to the affected devices but also their ability to use said devices to launch attacks on the Internet ecosystem. Rapid malware classification is an important too"
    },
    {
        "title": "Malware Classification Using Transfer Learning",
        "text": "ls to combat that threat. One of the successful approaches to classification is based on malware images and deep learning. While many deep learning architectures are very accurate they usually take a long time to train. In this work we perform experiments "
    },
    {
        "title": "Malware Classification Using Transfer Learning",
        "text": "on multiple well known, pre-trained, deep network architectures in the context of transfer learning. We show that almost all them classify malware accurately with a very short training period."
    },
    {
        "title": "CTCModel: a Keras Model for Connectionist Temporal Classification",
        "text": "We report an extension of a Keras Model, called CTCModel, to perform the Connectionist Temporal Classification (CTC) in a transparent way. Combined with Recurrent Neural Networks, the Connectionist Temporal Classification is the reference method for dealin"
    },
    {
        "title": "CTCModel: a Keras Model for Connectionist Temporal Classification",
        "text": "g with unsegmented input sequences, i.e. with data that are a couple of observation and label sequences where each label is related to a subset of observation frames. CTCModel makes use of the CTC implementation in the Tensorflow backend for training and p"
    },
    {
        "title": "CTCModel: a Keras Model for Connectionist Temporal Classification",
        "text": "redicting sequences of labels using Keras. It consists of three branches made of Keras models: one for training, computing the CTC loss function; one for predicting, providing sequences of labels; and one for evaluating that returns standard metrics for an"
    },
    {
        "title": "CTCModel: a Keras Model for Connectionist Temporal Classification",
        "text": "alyzing sequences of predictions."
    },
    {
        "title": "Neuron with Steady Response Leads to Better Generalization",
        "text": "Regularization can mitigate the generalization gap between training and inference by introducing inductive bias. Existing works have already proposed various inductive biases from diverse perspectives. However, to the best of our knowledge, none of them ex"
    },
    {
        "title": "Neuron with Steady Response Leads to Better Generalization",
        "text": "plores inductive bias from the perspective of class-dependent response distribution of individual neurons. In this paper, we conduct a substantial analysis of the characteristics of such distribution. Based on the analysis results, we articulate the Neuron"
    },
    {
        "title": "Neuron with Steady Response Leads to Better Generalization",
        "text": " Steadiness Hypothesis: the neuron with similar responses to instances of the same class leads to better generalization. Accordingly, we propose a new regularization method called Neuron Steadiness Regularization to reduce neuron intra-class response varia"
    },
    {
        "title": "Neuron with Steady Response Leads to Better Generalization",
        "text": "nce. We conduct extensive experiments on Multilayer Perceptron, Convolutional Neural Network, and Graph Neural Network with popular benchmark datasets of diverse domains, which show that our Neuron Steadiness Regularization consistently outperforms the van"
    },
    {
        "title": "Neuron with Steady Response Leads to Better Generalization",
        "text": "illa version of models with significant gain and low additional overhead."
    },
    {
        "title": "Graphs, Entities, and Step Mixture",
        "text": "Existing approaches for graph neural networks commonly suffer from the oversmoothing issue, regardless of how neighborhoods are aggregated. Most methods also focus on transductive scenarios for fixed graphs, leading to poor generalization for unseen graphs"
    },
    {
        "title": "Graphs, Entities, and Step Mixture",
        "text": ". To address these issues, we propose a new graph neural network that considers both edge-based neighborhood relationships and node-based entity features, i.e. Graph Entities with Step Mixture via random walk (GESM). GESM employs a mixture of various steps"
    },
    {
        "title": "Graphs, Entities, and Step Mixture",
        "text": " through random walk to alleviate the oversmoothing problem, attention to dynamically reflect interrelations depending on node information, and structure-based regularization to enhance embedding representation. With intensive experiments, we show that the"
    },
    {
        "title": "Graphs, Entities, and Step Mixture",
        "text": " proposed GESM achieves state-of-the-art or comparable performances on eight benchmark graph datasets comprising transductive and inductive learning tasks. Furthermore, we empirically demonstrate the significance of considering global information."
    },
    {
        "title": "Scalable Combinatorial Bayesian Optimization with Tractable Statistical   models",
        "text": "We study the problem of optimizing expensive blackbox functions over combinatorial spaces (e.g., sets, sequences, trees, and graphs). BOCS (Baptista and Poloczek, 2018) is a state-of-the-art Bayesian optimization method for tractable statistical models, wh"
    },
    {
        "title": "Scalable Combinatorial Bayesian Optimization with Tractable Statistical   models",
        "text": "ich performs semi-definite programming based acquisition function optimization (AFO) to select the next structure for evaluation. Unfortunately, BOCS scales poorly for large number of binary and/or categorical variables. Based on recent advances in submodu"
    },
    {
        "title": "Scalable Combinatorial Bayesian Optimization with Tractable Statistical   models",
        "text": "lar relaxation (Ito and Fujimaki, 2016) for solving Binary Quadratic Programs, we study an approach referred as Parametrized Submodular Relaxation (PSR) towards the goal of improving the scalability and accuracy of solving AFO problems for BOCS model. PSR "
    },
    {
        "title": "Scalable Combinatorial Bayesian Optimization with Tractable Statistical   models",
        "text": "approach relies on two key ideas. First, reformulation of AFO problem as submodular relaxation with some unknown parameters, which can be solved efficiently using minimum graph cut algorithms. Second, construction of an optimization problem to estimate the"
    },
    {
        "title": "Scalable Combinatorial Bayesian Optimization with Tractable Statistical   models",
        "text": " unknown parameters with close approximation to the true objective. Experiments on diverse benchmark problems show significant improvements with PSR for BOCS model. The source code is available at https://github.com/aryandeshwal/Submodular_Relaxation_BOCS "
    },
    {
        "title": "Scalable Combinatorial Bayesian Optimization with Tractable Statistical   models",
        "text": "."
    },
    {
        "title": "Not Enough Data? Deep Learning to the Rescue!",
        "text": "Based on recent advances in natural language modeling and those in text generation capabilities, we propose a novel data augmentation method for text classification tasks. We use a powerful pre-trained neural network model to artificially synthesize new la"
    },
    {
        "title": "Not Enough Data? Deep Learning to the Rescue!",
        "text": "beled data for supervised learning. We mainly focus on cases with scarce labeled data. Our method, referred to as language-model-based data augmentation (LAMBADA), involves fine-tuning a state-of-the-art language generator to a specific task through an ini"
    },
    {
        "title": "Not Enough Data? Deep Learning to the Rescue!",
        "text": "tial training phase on the existing (usually small) labeled data. Using the fine-tuned model and given a class label, new sentences for the class are generated. Our process then filters these new sentences by using a classifier trained on the original data"
    },
    {
        "title": "Not Enough Data? Deep Learning to the Rescue!",
        "text": ". In a series of experiments, we show that LAMBADA improves classifiers' performance on a variety of datasets. Moreover, LAMBADA significantly improves upon the state-of-the-art techniques for data augmentation, specifically those applicable to text classi"
    },
    {
        "title": "Not Enough Data? Deep Learning to the Rescue!",
        "text": "fication tasks with little data."
    },
    {
        "title": "Granger Causality: A Review and Recent Advances",
        "text": "Introduced more than a half century ago, Granger causality has become a popular tool for analyzing time series data in many application domains, from economics and finance to genomics and neuroscience. Despite this popularity, the validity of this notion f"
    },
    {
        "title": "Granger Causality: A Review and Recent Advances",
        "text": "or inferring causal relationships among time series has remained the topic of continuous debate. Moreover, while the original definition was general, limitations in computational tools have primarily limited the applications of Granger causality to simple "
    },
    {
        "title": "Granger Causality: A Review and Recent Advances",
        "text": "bivariate vector auto-regressive processes or pairwise relationships among a set of variables. Starting with a review of early developments and debates, this paper discusses recent advances that address various shortcomings of the earlier approaches, from "
    },
    {
        "title": "Granger Causality: A Review and Recent Advances",
        "text": "models for high-dimensional time series to more recent developments that account for nonlinear and non-Gaussian observations and allow for sub-sampled and mixed frequency time series."
    },
    {
        "title": "k-Center Clustering with Outliers in Sliding Windows",
        "text": "Metric $k$-center clustering is a fundamental unsupervised learning primitive. Although widely used, this primitive is heavily affected by noise in the data, so that a more sensible variant seeks for the best solution that disregards a given number $z$ of "
    },
    {
        "title": "k-Center Clustering with Outliers in Sliding Windows",
        "text": "points of the dataset, called outliers. We provide efficient algorithms for this important variant in the streaming model under the sliding window setting, where, at each time step, the dataset to be clustered is the window $W$ of the most recent data item"
    },
    {
        "title": "k-Center Clustering with Outliers in Sliding Windows",
        "text": "s. Our algorithms achieve $O(1)$ approximation and, remarkably, require a working memory linear in $k+z$ and only logarithmic in $|W|$. As a by-product, we show how to estimate the effective diameter of the window $W$, which is a measure of the spread of t"
    },
    {
        "title": "k-Center Clustering with Outliers in Sliding Windows",
        "text": "he window points, disregarding a given fraction of noisy distances. We also provide experimental evidence of the practical viability of our theoretical results."
    },
    {
        "title": "Nonparametric Contextual Bandits in an Unknown Metric Space",
        "text": "Consider a nonparametric contextual multi-arm bandit problem where each arm $a \\in [K]$ is associated to a nonparametric reward function $f_a: [0,1] \\to \\mathbb{R}$ mapping from contexts to the expected reward. Suppose that there is a large set of arms, ye"
    },
    {
        "title": "Nonparametric Contextual Bandits in an Unknown Metric Space",
        "text": "t there is a simple but unknown structure amongst the arm reward functions, e.g. finite types or smooth with respect to an unknown metric space. We present a novel algorithm which learns data-driven similarities amongst the arms, in order to implement adap"
    },
    {
        "title": "Nonparametric Contextual Bandits in an Unknown Metric Space",
        "text": "tive partitioning of the context-arm space for more efficient learning. We provide regret bounds along with simulations that highlight the algorithm's dependence on the local geometry of the reward functions."
    },
    {
        "title": "Incremental Unsupervised Domain-Adversarial Training of Neural Networks",
        "text": "In the context of supervised statistical learning, it is typically assumed that the training set comes from the same distribution that draws the test samples. When this is not the case, the behavior of the learned model is unpredictable and becomes depende"
    },
    {
        "title": "Incremental Unsupervised Domain-Adversarial Training of Neural Networks",
        "text": "nt upon the degree of similarity between the distribution of the training set and the distribution of the test set. One of the research topics that investigates this scenario is referred to as domain adaptation. Deep neural networks brought dramatic advanc"
    },
    {
        "title": "Incremental Unsupervised Domain-Adversarial Training of Neural Networks",
        "text": "es in pattern recognition and that is why there have been many attempts to provide good domain adaptation algorithms for these models. Here we take a different avenue and approach the problem from an incremental point of view, where the model is adapted to"
    },
    {
        "title": "Incremental Unsupervised Domain-Adversarial Training of Neural Networks",
        "text": " the new domain iteratively. We make use of an existing unsupervised domain-adaptation algorithm to identify the target samples on which there is greater confidence about their true label. The output of the model is analyzed in different ways to determine "
    },
    {
        "title": "Incremental Unsupervised Domain-Adversarial Training of Neural Networks",
        "text": "the candidate samples. The selected set is then added to the source training set by considering the labels provided by the network as ground truth, and the process is repeated until all target samples are labelled. Our results report a clear improvement wi"
    },
    {
        "title": "Incremental Unsupervised Domain-Adversarial Training of Neural Networks",
        "text": "th respect to the non-incremental case in several datasets, also outperforming other state-of-the-art domain adaptation algorithms."
    },
    {
        "title": "Noise Contrastive Estimation and Negative Sampling for Conditional   Models: Consistency and Statistical Efficiency",
        "text": "Noise Contrastive Estimation (NCE) is a powerful parameter estimation method for log-linear models, which avoids calculation of the partition function or its derivatives at each training step, a computationally demanding step in many cases. It is closely r"
    },
    {
        "title": "Noise Contrastive Estimation and Negative Sampling for Conditional   Models: Consistency and Statistical Efficiency",
        "text": "elated to negative sampling methods, now widely used in NLP. This paper considers NCE-based estimation of conditional models. Conditional models are frequently encountered in practice; however there has not been a rigorous theoretical analysis of NCE in th"
    },
    {
        "title": "Noise Contrastive Estimation and Negative Sampling for Conditional   Models: Consistency and Statistical Efficiency",
        "text": "is setting, and we will argue there are subtle but important questions when generalizing NCE to the conditional case. In particular, we analyze two variants of NCE for conditional models: one based on a classification objective, the other based on a rankin"
    },
    {
        "title": "Noise Contrastive Estimation and Negative Sampling for Conditional   Models: Consistency and Statistical Efficiency",
        "text": "g objective. We show that the ranking-based variant of NCE gives consistent parameter estimates under weaker assumptions than the classification-based method; we analyze the statistical efficiency of the ranking-based and classification-based variants of N"
    },
    {
        "title": "Noise Contrastive Estimation and Negative Sampling for Conditional   Models: Consistency and Statistical Efficiency",
        "text": "CE; finally we describe experiments on synthetic data and language modeling showing the effectiveness and trade-offs of both methods."
    },
    {
        "title": "An Intelligent Safety System for Human-Centered Semi-Autonomous Vehicles",
        "text": "Nowadays, automobile manufacturers make efforts to develop ways to make cars fully safe. Monitoring driver's actions by computer vision techniques to detect driving mistakes in real-time and then planning for autonomous driving to avoid vehicle collisions "
    },
    {
        "title": "An Intelligent Safety System for Human-Centered Semi-Autonomous Vehicles",
        "text": "is one of the most important issues that has been investigated in the machine vision and Intelligent Transportation Systems (ITS). The main goal of this study is to prevent accidents caused by fatigue, drowsiness, and driver distraction. To avoid these inc"
    },
    {
        "title": "An Intelligent Safety System for Human-Centered Semi-Autonomous Vehicles",
        "text": "idents, this paper proposes an integrated safety system that continuously monitors the driver's attention and vehicle surroundings, and finally decides whether the actual steering control status is safe or not. For this purpose, we equipped an ordinary car"
    },
    {
        "title": "An Intelligent Safety System for Human-Centered Semi-Autonomous Vehicles",
        "text": " called FARAZ with a vision system consisting of four mounted cameras along with a universal car tool for communicating with surrounding factory-installed sensors and other car systems, and sending commands to actuators. The proposed system leverages a sce"
    },
    {
        "title": "An Intelligent Safety System for Human-Centered Semi-Autonomous Vehicles",
        "text": "ne understanding pipeline using deep convolutional encoder-decoder networks and a driver state detection pipeline. We have been identifying and assessing domestic capabilities for the development of technologies specifically of the ordinary vehicles in ord"
    },
    {
        "title": "An Intelligent Safety System for Human-Centered Semi-Autonomous Vehicles",
        "text": "er to manufacture smart cars and eke providing an intelligent system to increase safety and to assist the driver in various conditions/situations."
    },
    {
        "title": "Harmonic Networks: Deep Translation and Rotation Equivariance",
        "text": "Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is n"
    },
    {
        "title": "Harmonic Networks: Deep Translation and Rotation Equivariance",
        "text": "ot the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-ro"
    },
    {
        "title": "Harmonic Networks: Deep Translation and Rotation Equivariance",
        "text": "tation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch.   H-Nets use a rich, parameter-efficient and low computational complexity representation, and we"
    },
    {
        "title": "Harmonic Networks: Deep Translation and Rotation Equivariance",
        "text": " show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normaliza"
    },
    {
        "title": "Harmonic Networks: Deep Translation and Rotation Equivariance",
        "text": "tion. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges."
    },
    {
        "title": "Semantic Image Synthesis with Spatially-Adaptive Normalization",
        "text": "We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed throug"
    },
    {
        "title": "Semantic Image Synthesis with Spatially-Adaptive Normalization",
        "text": "h stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations "
    },
    {
        "title": "Semantic Image Synthesis with Spatially-Adaptive Normalization",
        "text": "in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input lay"
    },
    {
        "title": "Semantic Image Synthesis with Spatially-Adaptive Normalization",
        "text": "outs. Finally, our model allows user control over both semantic and style. Code is available at https://github.com/NVlabs/SPADE ."
    },
    {
        "title": "Adaptive Anomaly Detection for IoT Data in Hierarchical Edge Computing",
        "text": "Advances in deep neural networks (DNN) greatly bolster real-time detection of anomalous IoT data. However, IoT devices can barely afford complex DNN models due to limited computational power and energy supply. While one can offload anomaly detection tasks "
    },
    {
        "title": "Adaptive Anomaly Detection for IoT Data in Hierarchical Edge Computing",
        "text": "to the cloud, it incurs long delay and requires large bandwidth when thousands of IoT devices stream data to the cloud concurrently. In this paper, we propose an adaptive anomaly detection approach for hierarchical edge computing (HEC) systems to solve thi"
    },
    {
        "title": "Adaptive Anomaly Detection for IoT Data in Hierarchical Edge Computing",
        "text": "s problem. Specifically, we first construct three anomaly detection DNN models of increasing complexity, and associate them with the three layers of HEC from bottom to top, i.e., IoT devices, edge servers, and cloud. Then, we design an adaptive scheme to s"
    },
    {
        "title": "Adaptive Anomaly Detection for IoT Data in Hierarchical Edge Computing",
        "text": "elect one of the models based on the contextual information extracted from input data, to perform anomaly detection. The selection is formulated as a contextual bandit problem and is characterized by a single-step Markov decision process, with an objective"
    },
    {
        "title": "Adaptive Anomaly Detection for IoT Data in Hierarchical Edge Computing",
        "text": " of achieving high detection accuracy and low detection delay simultaneously. We evaluate our proposed approach using a real IoT dataset, and demonstrate that it reduces detection delay by 84% while maintaining almost the same accuracy as compared to offlo"
    },
    {
        "title": "Adaptive Anomaly Detection for IoT Data in Hierarchical Edge Computing",
        "text": "ading detection tasks to the cloud. In addition, our evaluation also shows that it outperforms other baseline schemes."
    },
    {
        "title": "A neural network based on SPD manifold learning for skeleton-based hand   gesture recognition",
        "text": "This paper proposes a new neural network based on SPD manifold learning for skeleton-based hand gesture recognition. Given the stream of hand's joint positions, our approach combines two aggregation processes on respectively spatial and temporal domains. T"
    },
    {
        "title": "A neural network based on SPD manifold learning for skeleton-based hand   gesture recognition",
        "text": "he pipeline of our network architecture consists in three main stages. The first stage is based on a convolutional layer to increase the discriminative power of learned features. The second stage relies on different architectures for spatial and temporal G"
    },
    {
        "title": "A neural network based on SPD manifold learning for skeleton-based hand   gesture recognition",
        "text": "aussian aggregation of joint features. The third stage learns a final SPD matrix from skeletal data. A new type of layer is proposed for the third stage, based on a variant of stochastic gradient descent on Stiefel manifolds. The proposed network is valida"
    },
    {
        "title": "A neural network based on SPD manifold learning for skeleton-based hand   gesture recognition",
        "text": "ted on two challenging datasets and shows state-of-the-art accuracies on both datasets."
    },
    {
        "title": "Learning Mechanically Driven Emergent Behavior with Message Passing   Neural Networks",
        "text": "From designing architected materials to connecting mechanical behavior across scales, computational modeling is a critical tool in solid mechanics. Recently, there has been a growing interest in using machine learning to reduce the computational cost of ph"
    },
    {
        "title": "Learning Mechanically Driven Emergent Behavior with Message Passing   Neural Networks",
        "text": "ysics-based simulations. Notably, while machine learning approaches that rely on Graph Neural Networks (GNNs) have shown success in learning mechanics, the performance of GNNs has yet to be investigated on a myriad of solid mechanics problems. In this work"
    },
    {
        "title": "Learning Mechanically Driven Emergent Behavior with Message Passing   Neural Networks",
        "text": ", we examine the ability of GNNs to predict a fundamental aspect of mechanically driven emergent behavior: the connection between a column's geometric structure and the direction that it buckles. To accomplish this, we introduce the Asymmetric Buckling Col"
    },
    {
        "title": "Learning Mechanically Driven Emergent Behavior with Message Passing   Neural Networks",
        "text": "umns (ABC) dataset, a dataset comprised of three sub-datasets of asymmetric and heterogeneous column geometries where the goal is to classify the direction of symmetry breaking (left or right) under compression after the onset of instability. Because of co"
    },
    {
        "title": "Learning Mechanically Driven Emergent Behavior with Message Passing   Neural Networks",
        "text": "mplex local geometry, the \"image-like\" data representations required for implementing standard convolutional neural network based metamodels are not ideal, thus motivating the use of GNNs. In addition to investigating GNN model architecture, we study the e"
    },
    {
        "title": "Learning Mechanically Driven Emergent Behavior with Message Passing   Neural Networks",
        "text": "ffect of different input data representation approaches, data augmentation, and combining multiple models as an ensemble. While we were able to obtain good results, we also showed that predicting solid mechanics based emergent behavior is non-trivial. Beca"
    },
    {
        "title": "Learning Mechanically Driven Emergent Behavior with Message Passing   Neural Networks",
        "text": "use both our model implementation and dataset are distributed under open-source licenses, we hope that future researchers can build on our work to create enhanced mechanics-specific machine learning pipelines for capturing the behavior of complex geometric"
    },
    {
        "title": "Learning Mechanically Driven Emergent Behavior with Message Passing   Neural Networks",
        "text": " structures."
    },
    {
        "title": "Communication-Efficient Distributed Optimization in Networks with   Gradient Tracking and Variance Reduction",
        "text": "There is growing interest in large-scale machine learning and optimization over decentralized networks, e.g. in the context of multi-agent learning and federated learning. Due to the imminent need to alleviate the communication burden, the investigation of"
    },
    {
        "title": "Communication-Efficient Distributed Optimization in Networks with   Gradient Tracking and Variance Reduction",
        "text": " communication-efficient distributed optimization algorithms - particularly for empirical risk minimization - has flourished in recent years. A large fraction of these algorithms have been developed for the master/slave setting, relying on a central parame"
    },
    {
        "title": "Communication-Efficient Distributed Optimization in Networks with   Gradient Tracking and Variance Reduction",
        "text": "ter server that can communicate with all agents. This paper focuses on distributed optimization over networks, or decentralized optimization, where each agent is only allowed to aggregate information from its neighbors. By properly adjusting the global gra"
    },
    {
        "title": "Communication-Efficient Distributed Optimization in Networks with   Gradient Tracking and Variance Reduction",
        "text": "dient estimate via local averaging in conjunction with proper correction, we develop a communication-efficient approximate Newton-type method Network-DANE, which generalizes DANE to the decentralized scenarios. Our key ideas can be applied in a systematic "
    },
    {
        "title": "Communication-Efficient Distributed Optimization in Networks with   Gradient Tracking and Variance Reduction",
        "text": "manner to obtain decentralized versions of other master/slave distributed algorithms. A notable development is Network-SVRG/SARAH, which employs variance reduction to further accelerate local computation. We establish linear convergence of Network-DANE and"
    },
    {
        "title": "Communication-Efficient Distributed Optimization in Networks with   Gradient Tracking and Variance Reduction",
        "text": " Network-SVRG for strongly convex losses, and Network-SARAH for quadratic losses, which shed light on the impacts of data homogeneity, network connectivity, and local averaging upon the rate of convergence. We further extend Network-DANE to composite optim"
    },
    {
        "title": "Communication-Efficient Distributed Optimization in Networks with   Gradient Tracking and Variance Reduction",
        "text": "ization by allowing a nonsmooth penalty term. Numerical evidence is provided to demonstrate the appealing performance of our algorithms over competitive baselines, in terms of both communication and computation efficiency. Our work suggests that performing"
    },
    {
        "title": "Communication-Efficient Distributed Optimization in Networks with   Gradient Tracking and Variance Reduction",
        "text": " a certain amount of local communications and computations per iteration can substantially improve the overall efficiency."
    },
    {
        "title": "Bioinspired Cortex-based Fast Codebook Generation",
        "text": "A major archetype of artificial intelligence is developing algorithms facilitating temporal efficiency and accuracy while boosting the generalization performance. Even with the latest developments in machine learning, a key limitation has been the ineffici"
    },
    {
        "title": "Bioinspired Cortex-based Fast Codebook Generation",
        "text": "ent feature extraction from the initial data, which is essential in performance optimization. Here, we introduce a feature extraction method inspired by sensory cortical networks in the brain. Dubbed as bioinspired cortex, the algorithm provides convergenc"
    },
    {
        "title": "Bioinspired Cortex-based Fast Codebook Generation",
        "text": "e to orthogonal features from streaming signals with superior computational efficiency while processing data in compressed form. We demonstrate the performance of the new algorithm using artificially created complex data by comparing it with the commonly u"
    },
    {
        "title": "Bioinspired Cortex-based Fast Codebook Generation",
        "text": "sed traditional clustering algorithms, such as Birch, GMM, and K-means. While the data processing time is significantly reduced, seconds versus hours, encoding distortions remain essentially the same in the new algorithm providing a basis for better genera"
    },
    {
        "title": "Bioinspired Cortex-based Fast Codebook Generation",
        "text": "lization. Although we show herein the superior performance of the cortex model in clustering and vector quantization, it also provides potent implementation opportunities for machine learning fundamental components, such as reasoning, anomaly detection and"
    },
    {
        "title": "Bioinspired Cortex-based Fast Codebook Generation",
        "text": " classification in large scope applications, e.g., finance, cybersecurity, and healthcare."
    },
    {
        "title": "Domain Adaptation for Autoencoder-Based End-to-End Communication Over   Wireless Channels",
        "text": "The problem of domain adaptation conventionally considers the setting where a source domain has plenty of labeled data, and a target domain (with a different data distribution) has plenty of unlabeled data but none or very limited labeled data. In this pap"
    },
    {
        "title": "Domain Adaptation for Autoencoder-Based End-to-End Communication Over   Wireless Channels",
        "text": "er, we address the setting where the target domain has only limited labeled data from a distribution that is expected to change frequently. We first propose a fast and light-weight method for adapting a Gaussian mixture density network (MDN) using only a s"
    },
    {
        "title": "Domain Adaptation for Autoencoder-Based End-to-End Communication Over   Wireless Channels",
        "text": "mall set of target domain samples. This method is well-suited for the setting where the distribution of target data changes rapidly (e.g., a wireless channel), making it challenging to collect a large number of samples and retrain. We then apply the propos"
    },
    {
        "title": "Domain Adaptation for Autoencoder-Based End-to-End Communication Over   Wireless Channels",
        "text": "ed MDN adaptation method to the problem of end-of-end learning of a wireless communication autoencoder. A communication autoencoder models the encoder, decoder, and the channel using neural networks, and learns them jointly to minimize the overall decoding"
    },
    {
        "title": "Domain Adaptation for Autoencoder-Based End-to-End Communication Over   Wireless Channels",
        "text": " error rate. However, the error rate of an autoencoder trained on a particular (source) channel distribution can degrade as the channel distribution changes frequently, not allowing enough time for data collection and retraining of the autoencoder to the t"
    },
    {
        "title": "Domain Adaptation for Autoencoder-Based End-to-End Communication Over   Wireless Channels",
        "text": "arget channel distribution. We propose a method for adapting the autoencoder without modifying the encoder and decoder neural networks, and adapting only the MDN model of the channel. The method utilizes feature transformations at the decoder to compensate"
    },
    {
        "title": "Domain Adaptation for Autoencoder-Based End-to-End Communication Over   Wireless Channels",
        "text": " for changes in the channel distribution, and effectively present to the decoder samples close to the source distribution. Experimental evaluation on simulated datasets and real mmWave wireless channels demonstrate that the proposed methods can quickly ada"
    },
    {
        "title": "Domain Adaptation for Autoencoder-Based End-to-End Communication Over   Wireless Channels",
        "text": "pt the MDN model, and improve or maintain the error rate of the autoencoder under changing channel conditions."
    },
    {
        "title": "Optimal Sequential Detection of Signals with Unknown Appearance and   Disappearance Points in Time",
        "text": "The paper addresses a sequential changepoint detection problem, assuming that the duration of change may be finite and unknown. This problem is of importance for many applications, e.g., for signal and image processing where signals appear and disappear at"
    },
    {
        "title": "Optimal Sequential Detection of Signals with Unknown Appearance and   Disappearance Points in Time",
        "text": " unknown points in time or space. In contrast to the conventional optimality criterion in quickest change detection that requires minimization of the expected delay to detection for a given average run length to a false alarm, we focus on a reliable maximi"
    },
    {
        "title": "Optimal Sequential Detection of Signals with Unknown Appearance and   Disappearance Points in Time",
        "text": "n change detection criterion of maximizing the minimal probability of detection in a given time (or space) window for a given local maximal probability of false alarm in the prescribed window. We show that the optimal detection procedure is a modified CUSU"
    },
    {
        "title": "Optimal Sequential Detection of Signals with Unknown Appearance and   Disappearance Points in Time",
        "text": "M procedure. We then compare operating characteristics of this optimal procedure with popular in engineering the Finite Moving Average (FMA) detection algorithm and the ordinary CUSUM procedure using Monte Carlo simulations, which show that typically the l"
    },
    {
        "title": "Optimal Sequential Detection of Signals with Unknown Appearance and   Disappearance Points in Time",
        "text": "ater algorithms have almost the same performance as the optimal one. At the same time, the FMA procedure has a substantial advantage -- independence to the intensity of the signal, which is usually unknown. Finally, the FMA algorithm is applied to detectin"
    },
    {
        "title": "Optimal Sequential Detection of Signals with Unknown Appearance and   Disappearance Points in Time",
        "text": "g faint streaks of satellites in optical images."
    },
    {
        "title": "Adaptive Multi-Resolution Attention with Linear Complexity",
        "text": "Transformers have improved the state-of-the-art across numerous tasks in sequence modeling. Besides the quadratic computational and memory complexity w.r.t the sequence length, the self-attention mechanism only processes information at the same scale, i.e."
    },
    {
        "title": "Adaptive Multi-Resolution Attention with Linear Complexity",
        "text": ", all attention heads are in the same resolution, resulting in the limited power of the Transformer. To remedy this, we propose a novel and efficient structure named Adaptive Multi-Resolution Attention (AdaMRA for short), which scales linearly to sequence "
    },
    {
        "title": "Adaptive Multi-Resolution Attention with Linear Complexity",
        "text": "length in terms of time and space. Specifically, we leverage a multi-resolution multi-head attention mechanism, enabling attention heads to capture long-range contextual information in a coarse-to-fine fashion. Moreover, to capture the potential relations "
    },
    {
        "title": "Adaptive Multi-Resolution Attention with Linear Complexity",
        "text": "between query representation and clues of different attention granularities, we leave the decision of which resolution of attention to use to query, which further improves the model's capacity compared to vanilla Transformer. In an effort to reduce complex"
    },
    {
        "title": "Adaptive Multi-Resolution Attention with Linear Complexity",
        "text": "ity, we adopt kernel attention without degrading the performance. Extensive experiments on several benchmarks demonstrate the effectiveness and efficiency of our model by achieving a state-of-the-art performance-efficiency-memory trade-off. To facilitate A"
    },
    {
        "title": "Adaptive Multi-Resolution Attention with Linear Complexity",
        "text": "daMRA utilization by the scientific community, the code implementation will be made publicly available."
    },
    {
        "title": "Trends in Integration of Vision and Language Research: A Survey of   Tasks, Datasets, and Methods",
        "text": "Interest in Artificial Intelligence (AI) and its applications has seen unprecedented growth in the last few years. This success can be partly attributed to the advancements made in the sub-fields of AI such as machine learning, computer vision, and natural"
    },
    {
        "title": "Trends in Integration of Vision and Language Research: A Survey of   Tasks, Datasets, and Methods",
        "text": " language processing. Much of the growth in these fields has been made possible with deep learning, a sub-area of machine learning that uses artificial neural networks. This has created significant interest in the integration of vision and language. In thi"
    },
    {
        "title": "Trends in Integration of Vision and Language Research: A Survey of   Tasks, Datasets, and Methods",
        "text": "s survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. Our efforts"
    },
    {
        "title": "Trends in Integration of Vision and Language Research: A Survey of   Tasks, Datasets, and Methods",
        "text": " go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. Furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey"
    },
    {
        "title": "Trends in Integration of Vision and Language Research: A Survey of   Tasks, Datasets, and Methods",
        "text": " stimulates innovative thoughts and ideas to address the existing challenges and build new applications."
    },
    {
        "title": "Federated Nearest Neighbor Classification with a Colony of Fruit-Flies:   With Supplement",
        "text": "The mathematical formalization of a neurological mechanism in the olfactory circuit of a fruit-fly as a locality sensitive hash (Flyhash) and bloom filter (FBF) has been recently proposed and \"reprogrammed\" for various machine learning tasks such as simila"
    },
    {
        "title": "Federated Nearest Neighbor Classification with a Colony of Fruit-Flies:   With Supplement",
        "text": "rity search, outlier detection and text embeddings. We propose a novel reprogramming of this hash and bloom filter to emulate the canonical nearest neighbor classifier (NNC) in the challenging Federated Learning (FL) setup where training and test data are "
    },
    {
        "title": "Federated Nearest Neighbor Classification with a Colony of Fruit-Flies:   With Supplement",
        "text": "spread across parties and no data can leave their respective parties. Specifically, we utilize Flyhash and FBF to create the FlyNN classifier, and theoretically establish conditions where FlyNN matches NNC. We show how FlyNN is trained exactly in a FL setu"
    },
    {
        "title": "Federated Nearest Neighbor Classification with a Colony of Fruit-Flies:   With Supplement",
        "text": "p with low communication overhead to produce FlyNNFL, and how it can be differentially private. Empirically, we demonstrate that (i) FlyNN matches NNC accuracy across 70 OpenML datasets, (ii) FlyNNFL training is highly scalable with low communication overh"
    },
    {
        "title": "Federated Nearest Neighbor Classification with a Colony of Fruit-Flies:   With Supplement",
        "text": "ead, providing up to $8\\times$ speedup with $16$ parties."
    },
    {
        "title": "A Generic Multi-modal Dynamic Gesture Recognition System using Machine   Learning",
        "text": "Human computer interaction facilitates intelligent communication between humans and computers, in which gesture recognition plays a prominent role. This paper proposes a machine learning system to identify dynamic gestures using tri-axial acceleration data"
    },
    {
        "title": "A Generic Multi-modal Dynamic Gesture Recognition System using Machine   Learning",
        "text": " acquired from two public datasets. These datasets, uWave and Sony, were acquired using accelerometers embedded in Wii remotes and smartwatches, respectively. A dynamic gesture signed by the user is characterized by a generic set of features extracted acro"
    },
    {
        "title": "A Generic Multi-modal Dynamic Gesture Recognition System using Machine   Learning",
        "text": "ss time and frequency domains. The system was analyzed from an end-user perspective and was modelled to operate in three modes. The modes of operation determine the subsets of data to be used for training and testing the system. From an initial set of seve"
    },
    {
        "title": "A Generic Multi-modal Dynamic Gesture Recognition System using Machine   Learning",
        "text": "n classifiers, three were chosen to evaluate each dataset across all modes rendering the system towards mode-neutrality and dataset-independence. The proposed system is able to classify gestures performed at varying speeds with minimum preprocessing, makin"
    },
    {
        "title": "A Generic Multi-modal Dynamic Gesture Recognition System using Machine   Learning",
        "text": "g it computationally efficient. Moreover, this system was found to run on a low-cost embedded platform - Raspberry Pi Zero (USD 5), making it economically viable."
    },
    {
        "title": "Unadversarial Examples: Designing Objects for Robust Vision",
        "text": "We study a class of realistic computer vision settings wherein one can influence the design of the objects being recognized. We develop a framework that leverages this capability to significantly improve vision models' performance and robustness. This fram"
    },
    {
        "title": "Unadversarial Examples: Designing Objects for Robust Vision",
        "text": "ework exploits the sensitivity of modern machine learning algorithms to input perturbations in order to design \"robust objects,\" i.e., objects that are explicitly optimized to be confidently detected or classified. We demonstrate the efficacy of the framew"
    },
    {
        "title": "Unadversarial Examples: Designing Objects for Robust Vision",
        "text": "ork on a wide variety of vision-based tasks ranging from standard benchmarks, to (in-simulation) robotics, to real-world experiments. Our code can be found at https://git.io/unadversarial ."
    },
    {
        "title": "Interactive Robot Learning of Gestures, Language and Affordances",
        "text": "A growing field in robotics and Artificial Intelligence (AI) research is human-robot collaboration, whose target is to enable effective teamwork between humans and robots. However, in many situations human teams are still superior to human-robot teams, pri"
    },
    {
        "title": "Interactive Robot Learning of Gestures, Language and Affordances",
        "text": "marily because human teams can easily agree on a common goal with language, and the individual members observe each other effectively, leveraging their shared motor repertoire and sensorimotor resources. This paper shows that for cognitive robots it is pos"
    },
    {
        "title": "Interactive Robot Learning of Gestures, Language and Affordances",
        "text": "sible, and indeed fruitful, to combine knowledge acquired from interacting with elements of the environment (affordance exploration) with the probabilistic observation of another agent's actions.   We propose a model that unites (i) learning robot affordan"
    },
    {
        "title": "Interactive Robot Learning of Gestures, Language and Affordances",
        "text": "ces and word descriptions with (ii) statistical recognition of human gestures with vision sensors. We discuss theoretical motivations, possible implementations, and we show initial results which highlight that, after having acquired knowledge of its surrou"
    },
    {
        "title": "Interactive Robot Learning of Gestures, Language and Affordances",
        "text": "nding environment, a humanoid robot can generalize this knowledge to the case when it observes another agent (human partner) performing the same motor actions previously executed during training."
    },
    {
        "title": "Privacy for Free: Communication-Efficient Learning with Differential   Privacy Using Sketches",
        "text": "Communication and privacy are two critical concerns in distributed learning. Many existing works treat these concerns separately. In this work, we argue that a natural connection exists between methods for communication reduction and privacy preservation i"
    },
    {
        "title": "Privacy for Free: Communication-Efficient Learning with Differential   Privacy Using Sketches",
        "text": "n the context of distributed machine learning. In particular, we prove that Count Sketch, a simple method for data stream summarization, has inherent differential privacy properties. Using these derived privacy guarantees, we propose a novel sketch-based f"
    },
    {
        "title": "Privacy for Free: Communication-Efficient Learning with Differential   Privacy Using Sketches",
        "text": "ramework (DiffSketch) for distributed learning, where we compress the transmitted messages via sketches to simultaneously achieve communication efficiency and provable privacy benefits. Our evaluation demonstrates that DiffSketch can provide strong differe"
    },
    {
        "title": "Privacy for Free: Communication-Efficient Learning with Differential   Privacy Using Sketches",
        "text": "ntial privacy guarantees (e.g., $\\varepsilon$= 1) and reduce communication by 20-50x with only marginal decreases in accuracy. Compared to baselines that treat privacy and communication separately, DiffSketch improves absolute test accuracy by 5%-50% while"
    },
    {
        "title": "Privacy for Free: Communication-Efficient Learning with Differential   Privacy Using Sketches",
        "text": " offering the same privacy guarantees and communication compression."
    },
    {
        "title": "Legendre Decomposition for Tensors",
        "text": "We present a novel nonnegative tensor decomposition method, called Legendre decomposition, which factorizes an input tensor into a multiplicative combination of parameters. Thanks to the well-developed theory of information geometry, the reconstructed tens"
    },
    {
        "title": "Legendre Decomposition for Tensors",
        "text": "or is unique and always minimizes the KL divergence from an input tensor. We empirically show that Legendre decomposition can more accurately reconstruct tensors than other nonnegative tensor decomposition methods."
    },
    {
        "title": "Neural Variational Inference for Text Processing",
        "text": "Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational met"
    },
    {
        "title": "Neural Variational Inference for Text Processing",
        "text": "hods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very dif"
    },
    {
        "title": "Neural Variational Inference for Text Processing",
        "text": "ferent text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest"
    },
    {
        "title": "Neural Variational Inference for Text Processing",
        "text": " reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks"
    },
    {
        "title": "Neural Variational Inference for Text Processing",
        "text": " this model exceeds all previous published benchmarks."
    },
    {
        "title": "Audio Classification of Bit-Representation Waveform",
        "text": "This study investigated the waveform representation for audio signal classification. Recently, many studies on audio waveform classification such as acoustic event detection and music genre classification have been published. Most studies on audio waveform"
    },
    {
        "title": "Audio Classification of Bit-Representation Waveform",
        "text": " classification have proposed the use of a deep learning (neural network) framework. Generally, a frequency analysis method such as Fourier transform is applied to extract the frequency or spectral information from the input audio waveform before inputting"
    },
    {
        "title": "Audio Classification of Bit-Representation Waveform",
        "text": " the raw audio waveform into the neural network. In contrast to these previous studies, in this paper, we propose a novel waveform representation method, in which audio waveforms are represented as a bit sequence, for audio classification. In our experimen"
    },
    {
        "title": "Audio Classification of Bit-Representation Waveform",
        "text": "t, we compare the proposed bit representation waveform, which is directly given to a neural network, to other representations of audio waveforms such as a raw audio waveform and a power spectrum with two classification tasks: one is an acoustic event class"
    },
    {
        "title": "Audio Classification of Bit-Representation Waveform",
        "text": "ification task and the other is a sound/music classification task. The experimental results showed that the bit representation waveform achieved the best classification performance for both the tasks."
    },
    {
        "title": "A Brief Introduction to Machine Learning for Engineers",
        "text": "This monograph aims at providing an introduction to key concepts, algorithms, and theoretical results in machine learning. The treatment concentrates on probabilistic models for supervised and unsupervised learning problems. It introduces fundamental conce"
    },
    {
        "title": "A Brief Introduction to Machine Learning for Engineers",
        "text": "pts and algorithms by building on first principles, while also exposing the reader to more advanced topics with extensive pointers to the literature, within a unified notation and mathematical framework. The material is organized according to clearly defin"
    },
    {
        "title": "A Brief Introduction to Machine Learning for Engineers",
        "text": "ed categories, such as discriminative and generative models, frequentist and Bayesian approaches, exact and approximate inference, as well as directed and undirected models. This monograph is meant as an entry point for researchers with a background in pro"
    },
    {
        "title": "A Brief Introduction to Machine Learning for Engineers",
        "text": "bability and linear algebra."
    },
    {
        "title": "Automatic Microprocessor Performance Bug Detection",
        "text": "Processor design validation and debug is a difficult and complex task, which consumes the lion's share of the design process. Design bugs that affect processor performance rather than its functionality are especially difficult to catch, particularly in new"
    },
    {
        "title": "Automatic Microprocessor Performance Bug Detection",
        "text": " microarchitectures. This is because, unlike functional bugs, the correct processor performance of new microarchitectures on complex, long-running benchmarks is typically not deterministically known. Thus, when performance benchmarking new microarchitectur"
    },
    {
        "title": "Automatic Microprocessor Performance Bug Detection",
        "text": "es, performance teams may assume that the design is correct when the performance of the new microarchitecture exceeds that of the previous generation, despite significant performance regressions existing in the design. In this work, we present a two-stage,"
    },
    {
        "title": "Automatic Microprocessor Performance Bug Detection",
        "text": " machine learning-based methodology that is able to detect the existence of performance bugs in microprocessors. Our results show that our best technique detects 91.5% of microprocessor core performance bugs whose average IPC impact across the studied appl"
    },
    {
        "title": "Automatic Microprocessor Performance Bug Detection",
        "text": "ications is greater than 1% versus a bug-free design with zero false positives. When evaluated on memory system bugs, our technique achieves 100% detection with zero false positives. Moreover, the detection is automatic, requiring very little performance e"
    },
    {
        "title": "Automatic Microprocessor Performance Bug Detection",
        "text": "ngineer time."
    },
    {
        "title": "GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm   and Accelerator Co-Design",
        "text": "Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art graph learning model. However, it can be notoriously challenging to inference GCNs over large graph datasets, limiting their application to large real-world graphs and hindering the e"
    },
    {
        "title": "GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm   and Accelerator Co-Design",
        "text": "xploration of deeper and more sophisticated GCN graphs. This is because real-world graphs can be extremely large and sparse. Furthermore, the node degree of GCNs tends to follow the power-law distribution and therefore have highly irregular adjacency matri"
    },
    {
        "title": "GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm   and Accelerator Co-Design",
        "text": "ces, resulting in prohibitive inefficiencies in both data processing and movement and thus substantially limiting the achievable GCN acceleration efficiency. To this end, this paper proposes a GCN algorithm and accelerator Co-Design framework dubbed GCoD w"
    },
    {
        "title": "GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm   and Accelerator Co-Design",
        "text": "hich can largely alleviate the aforementioned GCN irregularity and boost GCNs' inference efficiency. Specifically, on the algorithm level, GCoD integrates a split and conquer GCN training strategy that polarizes the graphs to be either denser or sparser in"
    },
    {
        "title": "GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm   and Accelerator Co-Design",
        "text": " local neighborhoods without compromising the model accuracy, resulting in graph adjacency matrices that (mostly) have merely two levels of workload and enjoys largely enhanced regularity and thus ease of acceleration. On the hardware level, we further dev"
    },
    {
        "title": "GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm   and Accelerator Co-Design",
        "text": "elop a dedicated two-pronged accelerator with a separated engine to process each of the aforementioned denser and sparser workloads, further boosting the overall utilization and acceleration efficiency. Extensive experiments and ablation studies validate t"
    },
    {
        "title": "GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm   and Accelerator Co-Design",
        "text": "hat our GCoD consistently reduces the number of off-chip accesses, leading to speedups of 15286x, 294x, 7.8x, and 2.5x as compared to CPUs, GPUs, and prior-art GCN accelerators including HyGCN and AWB-GCN, respectively, while maintaining or even improving "
    },
    {
        "title": "GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm   and Accelerator Co-Design",
        "text": "the task accuracy. Codes are available at https://github.com/RICE-EIC/GCoD."
    },
    {
        "title": "A Variational-Sequential Graph Autoencoder for Neural Architecture   Performance Prediction",
        "text": "In computer vision research, the process of automating architecture engineering, Neural Architecture Search (NAS), has gained substantial interest. In the past, NAS was hardly accessible to researchers without access to large-scale compute systems, due to "
    },
    {
        "title": "A Variational-Sequential Graph Autoencoder for Neural Architecture   Performance Prediction",
        "text": "very long compute times for the recurrent search and evaluation of new candidate architectures. The NAS-Bench-101 dataset facilitates a paradigm change towards classical methods such as supervised learning to evaluate neural architectures. In this paper, w"
    },
    {
        "title": "A Variational-Sequential Graph Autoencoder for Neural Architecture   Performance Prediction",
        "text": "e propose a graph encoder built upon Graph Neural Networks (GNN). We demonstrate the effectiveness of the proposed encoder on NAS performance prediction for seen architecture types as well an unseen ones (i.e., zero shot prediction). We also provide a new "
    },
    {
        "title": "A Variational-Sequential Graph Autoencoder for Neural Architecture   Performance Prediction",
        "text": "variational-sequential graph autoencoder (VS-GAE) based on the proposed graph encoder. The VS-GAE is specialized on encoding and decoding graphs of varying length utilizing GNNs. Experiments on different sampling methods show that the embedding space learn"
    },
    {
        "title": "A Variational-Sequential Graph Autoencoder for Neural Architecture   Performance Prediction",
        "text": "ed by our VS-GAE increases the stability on the accuracy prediction task."
    },
    {
        "title": "Deep Reinforcement Learning for Wireless Resource Allocation Using   Buffer State Information",
        "text": "As the number of user equipments (UEs) with various data rate and latency requirements increases in wireless networks, the resource allocation problem for orthogonal frequency-division multiple access (OFDMA) becomes challenging. In particular, varying req"
    },
    {
        "title": "Deep Reinforcement Learning for Wireless Resource Allocation Using   Buffer State Information",
        "text": "uirements lead to a non-convex optimization problem when maximizing the systems data rate while preserving fairness between UEs. In this paper, we solve the non-convex optimization problem using deep reinforcement learning (DRL). We outline, train and eval"
    },
    {
        "title": "Deep Reinforcement Learning for Wireless Resource Allocation Using   Buffer State Information",
        "text": "uate a DRL agent, which performs the task of media access control scheduling for a downlink OFDMA scenario. To kickstart training of our agent, we introduce mimicking learning. For improvement of scheduling performance, full buffer state information at the"
    },
    {
        "title": "Deep Reinforcement Learning for Wireless Resource Allocation Using   Buffer State Information",
        "text": " base station (e.g. packet age, packet size) is taken into account. Techniques like input feature compression, packet shuffling and age capping further improve the performance of the agent. We train and evaluate our agents using Nokia's wireless suite and "
    },
    {
        "title": "Deep Reinforcement Learning for Wireless Resource Allocation Using   Buffer State Information",
        "text": "evaluate against different benchmark agents. We show that our agents clearly outperform the benchmark agents."
    },
    {
        "title": "Faster Kernel Matrix Algebra via Density Estimation",
        "text": "We study fast algorithms for computing fundamental properties of a positive semidefinite kernel matrix $K \\in \\mathbb{R}^{n \\times n}$ corresponding to $n$ points $x_1,\\ldots,x_n \\in \\mathbb{R}^d$. In particular, we consider estimating the sum of kernel ma"
    },
    {
        "title": "Faster Kernel Matrix Algebra via Density Estimation",
        "text": "trix entries, along with its top eigenvalue and eigenvector.   We show that the sum of matrix entries can be estimated to $1+\\epsilon$ relative error in time $sublinear$ in $n$ and linear in $d$ for many popular kernels, including the Gaussian, exponential"
    },
    {
        "title": "Faster Kernel Matrix Algebra via Density Estimation",
        "text": ", and rational quadratic kernels. For these kernels, we also show that the top eigenvalue (and an approximate eigenvector) can be approximated to $1+\\epsilon$ relative error in time $subquadratic$ in $n$ and linear in $d$.   Our algorithms represent signif"
    },
    {
        "title": "Faster Kernel Matrix Algebra via Density Estimation",
        "text": "icant advances in the best known runtimes for these problems. They leverage the positive definiteness of the kernel matrix, along with a recent line of work on efficient kernel density estimation."
    },
    {
        "title": "Self-supervised Knowledge Distillation Using Singular Value   Decomposition",
        "text": "To solve deep neural network (DNN)'s huge training dataset and its high computation issue, so-called teacher-student (T-S) DNN which transfers the knowledge of T-DNN to S-DNN has been proposed. However, the existing T-S-DNN has limited range of use, and th"
    },
    {
        "title": "Self-supervised Knowledge Distillation Using Singular Value   Decomposition",
        "text": "e knowledge of T-DNN is insufficiently transferred to S-DNN. To improve the quality of the transferred knowledge from T-DNN, we propose a new knowledge distillation using singular value decomposition (SVD). In addition, we define a knowledge transfer as a "
    },
    {
        "title": "Self-supervised Knowledge Distillation Using Singular Value   Decomposition",
        "text": "self-supervised task and suggest a way to continuously receive information from T-DNN. Simulation results show that a S-DNN with a computational cost of 1/5 of the T-DNN can be up to 1.1\\% better than the T-DNN in terms of classification accuracy. Also ass"
    },
    {
        "title": "Self-supervised Knowledge Distillation Using Singular Value   Decomposition",
        "text": "uming the same computational cost, our S-DNN outperforms the S-DNN driven by the state-of-the-art distillation with a performance advantage of 1.79\\%. code is available on https://github.com/sseung0703/SSKD\\_SVD."
    },
    {
        "title": "OCTID: Optical Coherence Tomography Image Database",
        "text": "Optical coherence tomography (OCT) is a non-invasive imaging modality which is widely used in clinical ophthalmology. OCT images are capable of visualizing deep retinal layers which is crucial for early diagnosis of retinal diseases. In this paper, we desc"
    },
    {
        "title": "OCTID: Optical Coherence Tomography Image Database",
        "text": "ribe a comprehensive open-access database containing more than 500 highresolution images categorized into different pathological conditions. The image classes include Normal (NO), Macular Hole (MH), Age-related Macular Degeneration (AMD), Central Serous Re"
    },
    {
        "title": "OCTID: Optical Coherence Tomography Image Database",
        "text": "tinopathy (CSR), and Diabetic Retinopathy (DR). The images were obtained from a raster scan protocol with a 2mm scan length and 512x1024 pixel resolution. We have also included 25 normal OCT images with their corresponding ground truth delineations which c"
    },
    {
        "title": "OCTID: Optical Coherence Tomography Image Database",
        "text": "an be used for an accurate evaluation of OCT image segmentation. In addition, we have provided a user-friendly GUI which can be used by clinicians for manual (and semi-automated) segmentation."
    },
    {
        "title": "Privately Publishable Per-instance Privacy",
        "text": "We consider how to privately share the personalized privacy losses incurred by objective perturbation, using per-instance differential privacy (pDP). Standard differential privacy (DP) gives us a worst-case bound that might be orders of magnitude larger th"
    },
    {
        "title": "Privately Publishable Per-instance Privacy",
        "text": "an the privacy loss to a particular individual relative to a fixed dataset. The pDP framework provides a more fine-grained analysis of the privacy guarantee to a target individual, but the per-instance privacy loss itself might be a function of sensitive d"
    },
    {
        "title": "Privately Publishable Per-instance Privacy",
        "text": "ata. In this paper, we analyze the per-instance privacy loss of releasing a private empirical risk minimizer learned via objective perturbation, and propose a group of methods to privately and accurately publish the pDP losses at little to no additional pr"
    },
    {
        "title": "Privately Publishable Per-instance Privacy",
        "text": "ivacy cost."
    },
    {
        "title": "Online Learning Sensing Matrix and Sparsifying Dictionary Simultaneously   for Compressive Sensing",
        "text": "This paper considers the problem of simultaneously learning the Sensing Matrix and Sparsifying Dictionary (SMSD) on a large training dataset. To address the formulated joint learning problem, we propose an online algorithm that consists of a closed-form so"
    },
    {
        "title": "Online Learning Sensing Matrix and Sparsifying Dictionary Simultaneously   for Compressive Sensing",
        "text": "lution for optimizing the sensing matrix with a fixed sparsifying dictionary and a stochastic method for learning the sparsifying dictionary on a large dataset when the sensing matrix is given. Benefiting from training on a large dataset, the obtained comp"
    },
    {
        "title": "Online Learning Sensing Matrix and Sparsifying Dictionary Simultaneously   for Compressive Sensing",
        "text": "ressive sensing (CS) system by the proposed algorithm yields a much better performance in terms of signal recovery accuracy than the existing ones. The simulation results on natural images demonstrate the effectiveness of the suggested online algorithm com"
    },
    {
        "title": "Online Learning Sensing Matrix and Sparsifying Dictionary Simultaneously   for Compressive Sensing",
        "text": "pared with the existing methods."
    },
    {
        "title": "The spiked matrix model with generative priors",
        "text": "Using a low-dimensional parametrization of signals is a generic and powerful way to enhance performance in signal processing and statistical inference. A very popular and widely explored type of dimensionality reduction is sparsity; another type is generat"
    },
    {
        "title": "The spiked matrix model with generative priors",
        "text": "ive modelling of signal distributions. Generative models based on neural networks, such as GANs or variational auto-encoders, are particularly performant and are gaining on applicability. In this paper we study spiked matrix models, where a low-rank matrix"
    },
    {
        "title": "The spiked matrix model with generative priors",
        "text": " is observed through a noisy channel. This problem with sparse structure of the spikes has attracted broad attention in the past literature. Here, we replace the sparsity assumption by generative modelling, and investigate the consequences on statistical a"
    },
    {
        "title": "The spiked matrix model with generative priors",
        "text": "nd algorithmic properties. We analyze the Bayes-optimal performance under specific generative models for the spike. In contrast with the sparsity assumption, we do not observe regions of parameters where statistical performance is superior to the best know"
    },
    {
        "title": "The spiked matrix model with generative priors",
        "text": "n algorithmic performance. We show that in the analyzed cases the approximate message passing algorithm is able to reach optimal performance. We also design enhanced spectral algorithms and analyze their performance and thresholds using random matrix theor"
    },
    {
        "title": "The spiked matrix model with generative priors",
        "text": "y, showing their superiority to the classical principal component analysis. We complement our theoretical results by illustrating the performance of the spectral algorithms when the spikes come from real datasets."
    },
    {
        "title": "Relevant-features based Auxiliary Cells for Energy Efficient Detection   of Natural Errors",
        "text": "Deep neural networks have demonstrated state-of-the-art performance on many classification tasks. However, they have no inherent capability to recognize when their predictions are wrong. There have been several efforts in the recent past to detect natural "
    },
    {
        "title": "Relevant-features based Auxiliary Cells for Energy Efficient Detection   of Natural Errors",
        "text": "errors but the suggested mechanisms pose additional energy requirements. To address this issue, we propose an ensemble of classifiers at hidden layers to enable energy efficient detection of natural errors. In particular, we append Relevant-features based "
    },
    {
        "title": "Relevant-features based Auxiliary Cells for Energy Efficient Detection   of Natural Errors",
        "text": "Auxiliary Cells (RACs) which are class specific binary linear classifiers trained on relevant features. The consensus of RACs is used to detect natural errors. Based on combined confidence of RACs, classification can be terminated early, thereby resulting "
    },
    {
        "title": "Relevant-features based Auxiliary Cells for Energy Efficient Detection   of Natural Errors",
        "text": "in energy efficient detection. We demonstrate the effectiveness of our technique on various image classification datasets such as CIFAR-10, CIFAR-100 and Tiny-ImageNet."
    },
    {
        "title": "Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting Epidemics",
        "text": "Infectious diseases remain among the top contributors to human illness and death worldwide, among which many diseases produce epidemic waves of infection. The unavailability of specific drugs and ready-to-use vaccines to prevent most of these epidemics mak"
    },
    {
        "title": "Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting Epidemics",
        "text": "es the situation worse. These force public health officials, health care providers, and policymakers to rely on early warning systems generated by reliable and accurate forecasts of epidemics. Accurate forecasts of epidemics can assist stakeholders in tail"
    },
    {
        "title": "Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting Epidemics",
        "text": "oring countermeasures, such as vaccination campaigns, staff scheduling, and resource allocation, to the situation at hand, which could translate to reductions in the impact of a disease. Unfortunately, most of these past epidemics (e.g., dengue, malaria, h"
    },
    {
        "title": "Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting Epidemics",
        "text": "epatitis, influenza, and most recent, Covid-19) exhibit nonlinear and non-stationary characteristics due to their spreading fluctuations based on seasonal-dependent variability and the nature of these epidemics. We analyze a wide variety of epidemic time s"
    },
    {
        "title": "Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting Epidemics",
        "text": "eries datasets using a maximal overlap discrete wavelet transform (MODWT) based autoregressive neural network and call it EWNet. MODWT techniques effectively characterize non-stationary behavior and seasonal dependencies in the epidemic time series and imp"
    },
    {
        "title": "Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting Epidemics",
        "text": "rove the forecasting scheme of the autoregressive neural network in the proposed ensemble wavelet network framework. From a nonlinear time series viewpoint, we explore the asymptotic stationarity of the proposed EWNet model to show the asymptotic behavior "
    },
    {
        "title": "Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting Epidemics",
        "text": "of the associated Markov Chain. We also theoretically investigate the effect of learning stability and the choice of hidden neurons in the proposed EWNet model. From a practical perspective, we compare our proposed EWNet framework with several statistical,"
    },
    {
        "title": "Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting Epidemics",
        "text": " machine learning, and deep learning models that have been previously used for epidemic forecasting."
    },
    {
        "title": "ReachNN: Reachability Analysis of Neural-Network Controlled Systems",
        "text": "Applying neural networks as controllers in dynamical systems has shown great promises. However, it is critical yet challenging to verify the safety of such control systems with neural-network controllers in the loop. Previous methods for verifying neural n"
    },
    {
        "title": "ReachNN: Reachability Analysis of Neural-Network Controlled Systems",
        "text": "etwork controlled systems are limited to a few specific activation functions. In this work, we propose a new reachability analysis approach based on Bernstein polynomials that can verify neural-network controlled systems with a more general form of activat"
    },
    {
        "title": "ReachNN: Reachability Analysis of Neural-Network Controlled Systems",
        "text": "ion functions, i.e., as long as they ensure that the neural networks are Lipschitz continuous. Specifically, we consider abstracting feedforward neural networks with Bernstein polynomials for a small subset of inputs. To quantify the error introduced by ab"
    },
    {
        "title": "ReachNN: Reachability Analysis of Neural-Network Controlled Systems",
        "text": "straction, we provide both theoretical error bound estimation based on the theory of Bernstein polynomials and more practical sampling based error bound estimation, following a tight Lipschitz constant estimation approach based on forward reachability anal"
    },
    {
        "title": "ReachNN: Reachability Analysis of Neural-Network Controlled Systems",
        "text": "ysis. Compared with previous methods, our approach addresses a much broader set of neural networks, including heterogeneous neural networks that contain multiple types of activation functions. Experiment results on a variety of benchmarks show the effectiv"
    },
    {
        "title": "ReachNN: Reachability Analysis of Neural-Network Controlled Systems",
        "text": "eness of our approach."
    },
    {
        "title": "Block CUR: Decomposing Matrices using Groups of Columns",
        "text": "A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual row"
    },
    {
        "title": "Block CUR: Decomposing Matrices using Groups of Columns",
        "text": "s or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined \\emph{blocks} of columns (or rows) from the matrix. We present an algorithm for sampling useful column blocks"
    },
    {
        "title": "Block CUR: Decomposing Matrices using Groups of Columns",
        "text": " and provide novel guarantees for the quality of the approximation. This algorithm has application in problems as diverse as biometric data analysis to distributed computing. We demonstrate the effectiveness of the proposed algorithms for computing the Blo"
    },
    {
        "title": "Block CUR: Decomposing Matrices using Groups of Columns",
        "text": "ck CUR decomposition of large matrices in a distributed setting with multiple nodes in a compute cluster, where such blocks correspond to columns (or rows) of the matrix stored on the same node, which can be retrieved with much less overhead than retrievin"
    },
    {
        "title": "Block CUR: Decomposing Matrices using Groups of Columns",
        "text": "g individual columns stored across different nodes. In the biometric setting, the rows correspond to different users and columns correspond to users' biometric reaction to external stimuli, {\\em e.g.,}~watching video content, at a particular time instant. "
    },
    {
        "title": "Block CUR: Decomposing Matrices using Groups of Columns",
        "text": "There is significant cost in acquiring each user's reaction to lengthy content so we sample a few important scenes to approximate the biometric response. An individual time sample in this use case cannot be queried in isolation due to the lack of context t"
    },
    {
        "title": "Block CUR: Decomposing Matrices using Groups of Columns",
        "text": "hat caused that biometric reaction. Instead, collections of time segments ({\\em i.e.,} blocks) must be presented to the user. The practical application of these algorithms is shown via experimental results using real-world user biometric data from a conten"
    },
    {
        "title": "Block CUR: Decomposing Matrices using Groups of Columns",
        "text": "t testing environment."
    },
    {
        "title": "Reprogramming Language Models for Molecular Representation Learning",
        "text": "Recent advancements in transfer learning have made it a promising approach for domain adaptation via transfer of learned representations. This is especially when relevant when alternate tasks have limited samples of well-defined and labeled data, which is "
    },
    {
        "title": "Reprogramming Language Models for Molecular Representation Learning",
        "text": "common in the molecule data domain. This makes transfer learning an ideal approach to solve molecular learning tasks. While Adversarial reprogramming has proven to be a successful method to repurpose neural networks for alternate tasks, most works consider"
    },
    {
        "title": "Reprogramming Language Models for Molecular Representation Learning",
        "text": " source and alternate tasks within the same domain. In this work, we propose a new algorithm, Representation Reprogramming via Dictionary Learning (R2DL), for adversarially reprogramming pretrained language models for molecular learning tasks, motivated by"
    },
    {
        "title": "Reprogramming Language Models for Molecular Representation Learning",
        "text": " leveraging learned representations in massive state of the art language models. The adversarial program learns a linear transformation between a dense source model input space (language data) and a sparse target model input space (e.g., chemical and biolo"
    },
    {
        "title": "Reprogramming Language Models for Molecular Representation Learning",
        "text": "gical molecule data) using a k-SVD solver to approximate a sparse representation of the encoded data, via dictionary learning. R2DL achieves the baseline established by state of the art toxicity prediction models trained on domain-specific data and outperf"
    },
    {
        "title": "Reprogramming Language Models for Molecular Representation Learning",
        "text": "orms the baseline in a limited training-data setting, thereby establishing avenues for domain-agnostic transfer learning for tasks with molecule data."
    },
    {
        "title": "Optimization of Smooth Functions with Noisy Observations: Local Minimax   Rates",
        "text": "We consider the problem of global optimization of an unknown non-convex smooth function with zeroth-order feedback. In this setup, an algorithm is allowed to adaptively query the underlying function at different locations and receives noisy evaluations of "
    },
    {
        "title": "Optimization of Smooth Functions with Noisy Observations: Local Minimax   Rates",
        "text": "function values at the queried points (i.e. the algorithm has access to zeroth-order information). Optimization performance is evaluated by the expected difference of function values at the estimated optimum and the true optimum. In contrast to the classic"
    },
    {
        "title": "Optimization of Smooth Functions with Noisy Observations: Local Minimax   Rates",
        "text": "al optimization setup, first-order information like gradients are not directly accessible to the optimization algorithm. We show that the classical minimax framework of analysis, which roughly characterizes the worst-case query complexity of an optimizatio"
    },
    {
        "title": "Optimization of Smooth Functions with Noisy Observations: Local Minimax   Rates",
        "text": "n algorithm in this setting, leads to excessively pessimistic results. We propose a local minimax framework to study the fundamental difficulty of optimizing smooth functions with adaptive function evaluations, which provides a refined picture of the intri"
    },
    {
        "title": "Optimization of Smooth Functions with Noisy Observations: Local Minimax   Rates",
        "text": "nsic difficulty of zeroth-order optimization. We show that for functions with fast level set growth around the global minimum, carefully designed optimization algorithms can identify a near global minimizer with many fewer queries. For the special case of "
    },
    {
        "title": "Optimization of Smooth Functions with Noisy Observations: Local Minimax   Rates",
        "text": "strongly convex and smooth functions, our implied convergence rates match the ones developed for zeroth-order convex optimization problems. At the other end of the spectrum, for worst-case smooth functions no algorithm can converge faster than the minimax "
    },
    {
        "title": "Optimization of Smooth Functions with Noisy Observations: Local Minimax   Rates",
        "text": "rate of estimating the entire unknown function in the $\\ell_\\infty$-norm. We provide an intuitive and efficient algorithm that attains the derived upper error bounds."
    },
    {
        "title": "Zero-Shot Cross-Lingual Opinion Target Extraction",
        "text": "Aspect-based sentiment analysis involves the recognition of so called opinion target expressions (OTEs). To automatically extract OTEs, supervised learning algorithms are usually employed which are trained on manually annotated corpora. The creation of the"
    },
    {
        "title": "Zero-Shot Cross-Lingual Opinion Target Extraction",
        "text": "se corpora is labor-intensive and sufficiently large datasets are therefore usually only available for a very narrow selection of languages and domains. In this work, we address the lack of available annotated data for specific languages by proposing a zer"
    },
    {
        "title": "Zero-Shot Cross-Lingual Opinion Target Extraction",
        "text": "o-shot cross-lingual approach for the extraction of opinion target expressions. We leverage multilingual word embeddings that share a common vector space across various languages and incorporate these into a convolutional neural network architecture for OT"
    },
    {
        "title": "Zero-Shot Cross-Lingual Opinion Target Extraction",
        "text": "E extraction. Our experiments with 5 languages give promising results: We can successfully train a model on annotated data of a source language and perform accurate prediction on a target language without ever using any annotated samples in that target lan"
    },
    {
        "title": "Zero-Shot Cross-Lingual Opinion Target Extraction",
        "text": "guage. Depending on the source and target language pairs, we reach performances in a zero-shot regime of up to 77% of a model trained on target language data. Furthermore, we can increase this performance up to 87% of a baseline model trained on target lan"
    },
    {
        "title": "Zero-Shot Cross-Lingual Opinion Target Extraction",
        "text": "guage data by performing cross-lingual learning from multiple source languages."
    },
    {
        "title": "Limited-Memory Matrix Adaptation for Large Scale Black-box Optimization",
        "text": "The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is a popular method to deal with nonconvex and/or stochastic optimization problems when the gradient information is not available. Being based on the CMA-ES, the recently proposed Matrix Adaptati"
    },
    {
        "title": "Limited-Memory Matrix Adaptation for Large Scale Black-box Optimization",
        "text": "on Evolution Strategy (MA-ES) provides a rather surprising result that the covariance matrix and all associated operations (e.g., potentially unstable eigendecomposition) can be replaced in the CMA-ES by a updated transformation matrix without any loss of "
    },
    {
        "title": "Limited-Memory Matrix Adaptation for Large Scale Black-box Optimization",
        "text": "performance. In order to further simplify MA-ES and reduce its $\\mathcal{O}\\big(n^2\\big)$ time and storage complexity to $\\mathcal{O}\\big(n\\log(n)\\big)$, we present the Limited-Memory Matrix Adaptation Evolution Strategy (LM-MA-ES) for efficient zeroth ord"
    },
    {
        "title": "Limited-Memory Matrix Adaptation for Large Scale Black-box Optimization",
        "text": "er large-scale optimization. The algorithm demonstrates state-of-the-art performance on a set of established large-scale benchmarks. We explore the algorithm on the problem of generating adversarial inputs for a (non-smooth) random forest classifier, demon"
    },
    {
        "title": "Limited-Memory Matrix Adaptation for Large Scale Black-box Optimization",
        "text": "strating a surprising vulnerability of the classifier."
    },
    {
        "title": "Automatic Expert Selection for Multi-Scenario and Multi-Task Search",
        "text": "Multi-scenario learning (MSL) enables a service provider to cater for users' fine-grained demands by separating services for different user sectors, e.g., by user's geographical region. Under each scenario there is a need to optimize multiple task-specific"
    },
    {
        "title": "Automatic Expert Selection for Multi-Scenario and Multi-Task Search",
        "text": " targets e.g., click through rate and conversion rate, known as multi-task learning (MTL). Recent solutions for MSL and MTL are mostly based on the multi-gate mixture-of-experts (MMoE) architecture. MMoE structure is typically static and its design require"
    },
    {
        "title": "Automatic Expert Selection for Multi-Scenario and Multi-Task Search",
        "text": "s domain-specific knowledge, making it less effective in handling both MSL and MTL. In this paper, we propose a novel Automatic Expert Selection framework for Multi-scenario and Multi-task search, named AESM^{2}. AESM^{2} integrates both MSL and MTL into a"
    },
    {
        "title": "Automatic Expert Selection for Multi-Scenario and Multi-Task Search",
        "text": " unified framework with an automatic structure learning. Specifically, AESM^{2} stacks multi-task layers over multi-scenario layers. This hierarchical design enables us to flexibly establish intrinsic connections between different scenarios, and at the sam"
    },
    {
        "title": "Automatic Expert Selection for Multi-Scenario and Multi-Task Search",
        "text": "e time also supports high-level feature extraction for different tasks. At each multi-scenario/multi-task layer, a novel expert selection algorithm is proposed to automatically identify scenario-/task-specific and shared experts for each input. Experiments"
    },
    {
        "title": "Automatic Expert Selection for Multi-Scenario and Multi-Task Search",
        "text": " over two real-world large-scale datasets demonstrate the effectiveness of AESM^{2} over a battery of strong baselines. Online A/B test also shows substantial performance gain on multiple metrics. Currently, AESM^{2} has been deployed online for serving ma"
    },
    {
        "title": "Automatic Expert Selection for Multi-Scenario and Multi-Task Search",
        "text": "jor traffic."
    },
    {
        "title": "Opportunities and Challenges in Deep Learning Adversarial Robustness: A   Survey",
        "text": "As we seek to deploy machine learning models beyond virtual and controlled domains, it is critical to analyze not only the accuracy or the fact that it works most of the time, but if such a model is truly robust and reliable. This paper studies strategies "
    },
    {
        "title": "Opportunities and Challenges in Deep Learning Adversarial Robustness: A   Survey",
        "text": "to implement adversary robustly trained algorithms towards guaranteeing safety in machine learning algorithms. We provide a taxonomy to classify adversarial attacks and defenses, formulate the Robust Optimization problem in a min-max setting and divide it "
    },
    {
        "title": "Opportunities and Challenges in Deep Learning Adversarial Robustness: A   Survey",
        "text": "into 3 subcategories, namely: Adversarial (re)Training, Regularization Approach, and Certified Defenses. We survey the most recent and important results in adversarial example generation, defense mechanisms with adversarial (re)Training as their main defen"
    },
    {
        "title": "Opportunities and Challenges in Deep Learning Adversarial Robustness: A   Survey",
        "text": "se against perturbations. We also survey mothods that add regularization terms that change the behavior of the gradient, making it harder for attackers to achieve their objective. Alternatively, we've surveyed methods which formally derive certificates of "
    },
    {
        "title": "Opportunities and Challenges in Deep Learning Adversarial Robustness: A   Survey",
        "text": "robustness by exactly solving the optimization problem or by approximations using upper or lower bounds. In addition, we discuss the challenges faced by most of the recent algorithms presenting future research perspectives."
    },
    {
        "title": "Copula-like Variational Inference",
        "text": "This paper considers a new family of variational distributions motivated by Sklar's theorem. This family is based on new copula-like densities on the hypercube with non-uniform marginals which can be sampled efficiently, i.e. with a complexity linear in th"
    },
    {
        "title": "Copula-like Variational Inference",
        "text": "e dimension of state space. Then, the proposed variational densities that we suggest can be seen as arising from these copula-like densities used as base distributions on the hypercube with Gaussian quantile functions and sparse rotation matrices as normal"
    },
    {
        "title": "Copula-like Variational Inference",
        "text": "izing flows. The latter correspond to a rotation of the marginals with complexity $\\mathcal{O}(d \\log d)$. We provide some empirical evidence that such a variational family can also approximate non-Gaussian posteriors and can be beneficial compared to Gaus"
    },
    {
        "title": "Copula-like Variational Inference",
        "text": "sian approximations. Our method performs largely comparably to state-of-the-art variational approximations on standard regression and classification benchmarks for Bayesian Neural Networks."
    },
    {
        "title": "Affine Transport for Sim-to-Real Domain Adaptation",
        "text": "Sample-efficient domain adaptation is an open problem in robotics. In this paper, we present affine transport -- a variant of optimal transport, which models the mapping between state transition distributions between the source and target domains with an a"
    },
    {
        "title": "Affine Transport for Sim-to-Real Domain Adaptation",
        "text": "ffine transformation. First, we derive the affine transport framework; then, we extend the basic framework with Procrustes alignment to model arbitrary affine transformations. We evaluate the method in a number of OpenAI Gym sim-to-sim experiments with sim"
    },
    {
        "title": "Affine Transport for Sim-to-Real Domain Adaptation",
        "text": "ulation environments, as well as on a sim-to-real domain adaptation task of a robot hitting a hockeypuck such that it slides and stops at a target position. In each experiment, we evaluate the results when transferring between each pair of dynamics domains"
    },
    {
        "title": "Affine Transport for Sim-to-Real Domain Adaptation",
        "text": ". The results show that affine transport can significantly reduce the model adaptation error in comparison to using the original, non-adapted dynamics model."
    },
    {
        "title": "Fine-tuning deep CNN models on specific MS COCO categories",
        "text": "Fine-tuning of a deep convolutional neural network (CNN) is often desired. This paper provides an overview of our publicly available py-faster-rcnn-ft software library that can be used to fine-tune the VGG_CNN_M_1024 model on custom subsets of the Microsof"
    },
    {
        "title": "Fine-tuning deep CNN models on specific MS COCO categories",
        "text": "t Common Objects in Context (MS COCO) dataset. For example, we improved the procedure so that the user does not have to look for suitable image files in the dataset by hand which can then be used in the demo program. Our implementation randomly selects ima"
    },
    {
        "title": "Fine-tuning deep CNN models on specific MS COCO categories",
        "text": "ges that contain at least one object of the categories on which the model is fine-tuned."
    },
    {
        "title": "Extending Label Smoothing Regularization with Self-Knowledge   Distillation",
        "text": "Inspired by the strong correlation between the Label Smoothing Regularization(LSR) and Knowledge distillation(KD), we propose an algorithm LsrKD for training boost by extending the LSR method to the KD regime and applying a softer temperature. Then we impr"
    },
    {
        "title": "Extending Label Smoothing Regularization with Self-Knowledge   Distillation",
        "text": "ove the LsrKD by a Teacher Correction(TC) method, which manually sets a constant larger proportion for the right class in the uniform distribution teacher. To further improve the performance of LsrKD, we develop a self-distillation method named Memory-repl"
    },
    {
        "title": "Extending Label Smoothing Regularization with Self-Knowledge   Distillation",
        "text": "ay Knowledge Distillation (MrKD) that provides a knowledgeable teacher to replace the uniform distribution one in LsrKD. The MrKD method penalizes the KD loss between the current model's output distributions and its copies' on the training trajectory. By p"
    },
    {
        "title": "Extending Label Smoothing Regularization with Self-Knowledge   Distillation",
        "text": "reventing the model learning so far from its historical output distribution space, MrKD can stabilize the learning and find a more robust minimum. Our experiments show that LsrKD can improve LSR performance consistently at no cost, especially on several de"
    },
    {
        "title": "Extending Label Smoothing Regularization with Self-Knowledge   Distillation",
        "text": "ep neural networks where LSR is ineffectual. Also, MrKD can significantly improve single model training. The experiment results confirm that the TC can help LsrKD and MrKD to boost training, especially on the networks they are failed. Overall, LsrKD, MrKD,"
    },
    {
        "title": "Extending Label Smoothing Regularization with Self-Knowledge   Distillation",
        "text": " and their TC variants are comparable to or outperform the LSR method, suggesting the broad applicability of these KD methods."
    },
    {
        "title": "Generalized Learning Vector Quantization for Classification in   Randomized Neural Networks and Hyperdimensional Computing",
        "text": "Machine learning algorithms deployed on edge devices must meet certain resource constraints and efficiency requirements. Random Vector Functional Link (RVFL) networks are favored for such applications due to their simple design and training efficiency. We "
    },
    {
        "title": "Generalized Learning Vector Quantization for Classification in   Randomized Neural Networks and Hyperdimensional Computing",
        "text": "propose a modified RVFL network that avoids computationally expensive matrix operations during training, thus expanding the network's range of potential applications. Our modification replaces the least-squares classifier with the Generalized Learning Vect"
    },
    {
        "title": "Generalized Learning Vector Quantization for Classification in   Randomized Neural Networks and Hyperdimensional Computing",
        "text": "or Quantization (GLVQ) classifier, which only employs simple vector and distance calculations. The GLVQ classifier can also be considered an improvement upon certain classification algorithms popularly used in the area of Hyperdimensional Computing. The pr"
    },
    {
        "title": "Generalized Learning Vector Quantization for Classification in   Randomized Neural Networks and Hyperdimensional Computing",
        "text": "oposed approach achieved state-of-the-art accuracy on a collection of datasets from the UCI Machine Learning Repository - higher than previously proposed RVFL networks. We further demonstrate that our approach still achieves high accuracy while severely li"
    },
    {
        "title": "Generalized Learning Vector Quantization for Classification in   Randomized Neural Networks and Hyperdimensional Computing",
        "text": "mited in training iterations (using on average only 21% of the least-squares classifier computational costs)."
    },
    {
        "title": "On the Convergence of AdaBound and its Connection to SGD",
        "text": "Adaptive gradient methods such as Adam have gained extreme popularity due to their success in training complex neural networks and less sensitivity to hyperparameter tuning compared to SGD. However, it has been recently shown that Adam can fail to converge"
    },
    {
        "title": "On the Convergence of AdaBound and its Connection to SGD",
        "text": " and might cause poor generalization -- this lead to the design of new, sophisticated adaptive methods which attempt to generalize well while being theoretically reliable. In this technical report we focus on AdaBound, a promising, recently proposed optimi"
    },
    {
        "title": "On the Convergence of AdaBound and its Connection to SGD",
        "text": "zer. We present a stochastic convex problem for which AdaBound can provably take arbitrarily long to converge in terms of a factor which is not accounted for in the convergence rate guarantee of Luo et al. (2019). We present a new $O(\\sqrt T)$ regret guara"
    },
    {
        "title": "On the Convergence of AdaBound and its Connection to SGD",
        "text": "ntee under different assumptions on the bound functions, and provide empirical results on CIFAR suggesting that a specific form of momentum SGD can match AdaBound's performance while having less hyperparameters and lower computational costs."
    },
    {
        "title": "Variational Quantum Eigensolver for Frustrated Quantum Systems",
        "text": "Hybrid quantum-classical algorithms have been proposed as a potentially viable application of quantum computers. A particular example - the variational quantum eigensolver, or VQE - is designed to determine a global minimum in an energy landscape specified"
    },
    {
        "title": "Variational Quantum Eigensolver for Frustrated Quantum Systems",
        "text": " by a quantum Hamiltonian, which makes it appealing for the needs of quantum chemistry. Experimental realizations have been reported in recent years and theoretical estimates of its efficiency are a subject of intense effort. Here we consider the performan"
    },
    {
        "title": "Variational Quantum Eigensolver for Frustrated Quantum Systems",
        "text": "ce of the VQE technique for a Hubbard-like model describing a one-dimensional chain of fermions with competing nearest- and next-nearest-neighbor interactions. We find that recovering the VQE solution allows one to obtain the correlation function of the gr"
    },
    {
        "title": "Variational Quantum Eigensolver for Frustrated Quantum Systems",
        "text": "ound state consistent with the exact result. We also study the barren plateau phenomenon for the Hamiltonian in question and find that the severity of this effect depends on the encoding of fermions to qubits. Our results are consistent with the current kn"
    },
    {
        "title": "Variational Quantum Eigensolver for Frustrated Quantum Systems",
        "text": "owledge about the barren plateaus in quantum optimization."
    },
    {
        "title": "Studying the Interplay between Information Loss and Operation Loss in   Representations for Classification",
        "text": "Information-theoretic measures have been widely adopted in the design of features for learning and decision problems. Inspired by this, we look at the relationship between i) a weak form of information loss in the Shannon sense and ii) the operation loss i"
    },
    {
        "title": "Studying the Interplay between Information Loss and Operation Loss in   Representations for Classification",
        "text": "n the minimum probability of error (MPE) sense when considering a family of lossy continuous representations (features) of a continuous observation. We present several results that shed light on this interplay. Our first result offers a lower bound on a we"
    },
    {
        "title": "Studying the Interplay between Information Loss and Operation Loss in   Representations for Classification",
        "text": "ak form of information loss as a function of its respective operation loss when adopting a discrete lossy representation (quantization) instead of the original raw observation. From this, our main result shows that a specific form of vanishing information "
    },
    {
        "title": "Studying the Interplay between Information Loss and Operation Loss in   Representations for Classification",
        "text": "loss (a weak notion of asymptotic informational sufficiency) implies a vanishing MPE loss (or asymptotic operational sufficiency) when considering a general family of lossy continuous representations. Our theoretical findings support the observation that t"
    },
    {
        "title": "Studying the Interplay between Information Loss and Operation Loss in   Representations for Classification",
        "text": "he selection of feature representations that attempt to capture informational sufficiency is appropriate for learning, but this selection is a rather conservative design principle if the intended goal is achieving MPE in classification. Supporting this las"
    },
    {
        "title": "Studying the Interplay between Information Loss and Operation Loss in   Representations for Classification",
        "text": "t point, and under some structural conditions, we show that it is possible to adopt an alternative notion of informational sufficiency (strictly weaker than pure sufficiency in the mutual information sense) to achieve operational sufficiency in learning."
    },
    {
        "title": "MMA Regularization: Decorrelating Weights of Neural Networks by   Maximizing the Minimal Angles",
        "text": "The strong correlation between neurons or filters can significantly weaken the generalization ability of neural networks. Inspired by the well-known Tammes problem, we propose a novel diversity regularization method to address this issue, which makes the n"
    },
    {
        "title": "MMA Regularization: Decorrelating Weights of Neural Networks by   Maximizing the Minimal Angles",
        "text": "ormalized weight vectors of neurons or filters distributed on a hypersphere as uniformly as possible, through maximizing the minimal pairwise angles (MMA). This method can easily exert its effect by plugging the MMA regularization term into the loss functi"
    },
    {
        "title": "MMA Regularization: Decorrelating Weights of Neural Networks by   Maximizing the Minimal Angles",
        "text": "on with negligible computational overhead. The MMA regularization is simple, efficient, and effective. Therefore, it can be used as a basic regularization method in neural network training. Extensive experiments demonstrate that MMA regularization is able "
    },
    {
        "title": "MMA Regularization: Decorrelating Weights of Neural Networks by   Maximizing the Minimal Angles",
        "text": "to enhance the generalization ability of various modern models and achieves considerable performance improvements on CIFAR100 and TinyImageNet datasets. In addition, experiments on face verification show that MMA regularization is also effective for featur"
    },
    {
        "title": "MMA Regularization: Decorrelating Weights of Neural Networks by   Maximizing the Minimal Angles",
        "text": "e learning. Code is available at: https://github.com/wznpub/MMA_Regularization."
    },
    {
        "title": "Hierarchical Deep Learning of Multiscale Differential Equation   Time-Steppers",
        "text": "Nonlinear differential equations rarely admit closed-form solutions, thus requiring numerical time-stepping algorithms to approximate solutions. Further, many systems characterized by multiscale physics exhibit dynamics over a vast range of timescales, mak"
    },
    {
        "title": "Hierarchical Deep Learning of Multiscale Differential Equation   Time-Steppers",
        "text": "ing numerical integration computationally expensive due to numerical stiffness. In this work, we develop a hierarchy of deep neural network time-steppers to approximate the flow map of the dynamical system over a disparate range of time-scales. The resulti"
    },
    {
        "title": "Hierarchical Deep Learning of Multiscale Differential Equation   Time-Steppers",
        "text": "ng model is purely data-driven and leverages features of the multiscale dynamics, enabling numerical integration and forecasting that is both accurate and highly efficient. Moreover, similar ideas can be used to couple neural network-based models with clas"
    },
    {
        "title": "Hierarchical Deep Learning of Multiscale Differential Equation   Time-Steppers",
        "text": "sical numerical time-steppers. Our multiscale hierarchical time-stepping scheme provides important advantages over current time-stepping algorithms, including (i) circumventing numerical stiffness due to disparate time-scales, (ii) improved accuracy in com"
    },
    {
        "title": "Hierarchical Deep Learning of Multiscale Differential Equation   Time-Steppers",
        "text": "parison with leading neural-network architectures, (iii) efficiency in long-time simulation/forecasting due to explicit training of slow time-scale dynamics, and (iv) a flexible framework that is parallelizable and may be integrated with standard numerical"
    },
    {
        "title": "Hierarchical Deep Learning of Multiscale Differential Equation   Time-Steppers",
        "text": " time-stepping algorithms. The method is demonstrated on a wide range of nonlinear dynamical systems, including the Van der Pol oscillator, the Lorenz system, the Kuramoto-Sivashinsky equation, and fluid flow pass a cylinder; audio and video signals are al"
    },
    {
        "title": "Hierarchical Deep Learning of Multiscale Differential Equation   Time-Steppers",
        "text": "so explored. On the sequence generation examples, we benchmark our algorithm against state-of-the-art methods, such as LSTM, reservoir computing, and clockwork RNN. Despite the structural simplicity of our method, it outperforms competing methods on numeri"
    },
    {
        "title": "Hierarchical Deep Learning of Multiscale Differential Equation   Time-Steppers",
        "text": "cal integration."
    },
    {
        "title": "Predicting Time-to-Failure of Plasma Etching Equipment using Machine   Learning",
        "text": "Predicting unscheduled breakdowns of plasma etching equipment can reduce maintenance costs and production losses in the semiconductor industry. However, plasma etching is a complex procedure and it is hard to capture all relevant equipment properties and b"
    },
    {
        "title": "Predicting Time-to-Failure of Plasma Etching Equipment using Machine   Learning",
        "text": "ehaviors in a single physical model. Machine learning offers an alternative for predicting upcoming machine failures based on relevant data points. In this paper, we describe three different machine learning tasks that can be used for that purpose: (i) pre"
    },
    {
        "title": "Predicting Time-to-Failure of Plasma Etching Equipment using Machine   Learning",
        "text": "dicting Time-To-Failure (TTF), (ii) predicting health state, and (iii) predicting TTF intervals of an equipment. Our results show that trained machine learning models can outperform benchmarks resembling human judgments in all three tasks. This suggests th"
    },
    {
        "title": "Predicting Time-to-Failure of Plasma Etching Equipment using Machine   Learning",
        "text": "at machine learning offers a viable alternative to currently deployed plasma etching equipment maintenance strategies and decision making processes."
    },
    {
        "title": "Neural Fixed-Point Acceleration for Convex Optimization",
        "text": "Fixed-point iterations are at the heart of numerical computing and are often a computational bottleneck in real-time applications that typically need a fast solution of moderate accuracy. We present neural fixed-point acceleration which combines ideas from"
    },
    {
        "title": "Neural Fixed-Point Acceleration for Convex Optimization",
        "text": " meta-learning and classical acceleration methods to automatically learn to accelerate fixed-point problems that are drawn from a distribution. We apply our framework to SCS, the state-of-the-art solver for convex cone programming, and design models and lo"
    },
    {
        "title": "Neural Fixed-Point Acceleration for Convex Optimization",
        "text": "ss functions to overcome the challenges of learning over unrolled optimization and acceleration instabilities. Our work brings neural acceleration into any optimization problem expressible with CVXPY. The source code behind this paper is available at https"
    },
    {
        "title": "Neural Fixed-Point Acceleration for Convex Optimization",
        "text": "://github.com/facebookresearch/neural-scs"
    },
    {
        "title": "How Developers Iterate on Machine Learning Workflows -- A Survey of the   Applied Machine Learning Literature",
        "text": "Machine learning workflow development is anecdotally regarded to be an iterative process of trial-and-error with humans-in-the-loop. However, we are not aware of quantitative evidence corroborating this popular belief. A quantitative characterization of it"
    },
    {
        "title": "How Developers Iterate on Machine Learning Workflows -- A Survey of the   Applied Machine Learning Literature",
        "text": "eration can serve as a benchmark for machine learning workflow development in practice, and can aid the development of human-in-the-loop machine learning systems. To this end, we conduct a small-scale survey of the applied machine learning literature from "
    },
    {
        "title": "How Developers Iterate on Machine Learning Workflows -- A Survey of the   Applied Machine Learning Literature",
        "text": "five distinct application domains. We collect and distill statistics on the role of iteration within machine learning workflow development, and report preliminary trends and insights from our investigation, as a starting point towards this benchmark. Based"
    },
    {
        "title": "How Developers Iterate on Machine Learning Workflows -- A Survey of the   Applied Machine Learning Literature",
        "text": " on our findings, we finally describe desiderata for effective and versatile human-in-the-loop machine learning systems that can cater to users in diverse domains."
    },
    {
        "title": "Xi-Learning: Successor Feature Transfer Learning for General Reward   Functions",
        "text": "Transfer in Reinforcement Learning aims to improve learning performance on target tasks using knowledge from experienced source tasks. Successor features (SF) are a prominent transfer mechanism in domains where the reward function changes between tasks. Th"
    },
    {
        "title": "Xi-Learning: Successor Feature Transfer Learning for General Reward   Functions",
        "text": "ey reevaluate the expected return of previously learned policies in a new target task and to transfer their knowledge. A limiting factor of the SF framework is its assumption that rewards linearly decompose into successor features and a reward weight vecto"
    },
    {
        "title": "Xi-Learning: Successor Feature Transfer Learning for General Reward   Functions",
        "text": "r. We propose a novel SF mechanism, $\\xi$-learning, based on learning the cumulative discounted probability of successor features. Crucially, $\\xi$-learning allows to reevaluate the expected return of policies for general reward functions. We introduce two"
    },
    {
        "title": "Xi-Learning: Successor Feature Transfer Learning for General Reward   Functions",
        "text": " $\\xi$-learning variations, prove its convergence, and provide a guarantee on its transfer performance. Experimental evaluations based on $\\xi$-learning with function approximation demonstrate the prominent advantage of $\\xi$-learning over available mechan"
    },
    {
        "title": "Xi-Learning: Successor Feature Transfer Learning for General Reward   Functions",
        "text": "isms not only for general reward functions, but also in the case of linearly decomposable reward functions."
    },
    {
        "title": "Collaborating with Humans without Human Data",
        "text": "Collaborating with humans requires rapidly adapting to their individual strengths, weaknesses, and preferences. Unfortunately, most standard multi-agent reinforcement learning techniques, such as self-play (SP) or population play (PP), produce agents that "
    },
    {
        "title": "Collaborating with Humans without Human Data",
        "text": "overfit to their training partners and do not generalize well to humans. Alternatively, researchers can collect human data, train a human model using behavioral cloning, and then use that model to train \"human-aware\" agents (\"behavioral cloning play\", or B"
    },
    {
        "title": "Collaborating with Humans without Human Data",
        "text": "CP). While such an approach can improve the generalization of agents to new human co-players, it involves the onerous and expensive step of collecting large amounts of human data first. Here, we study the problem of how to train agents that collaborate wel"
    },
    {
        "title": "Collaborating with Humans without Human Data",
        "text": "l with human partners without using human data. We argue that the crux of the problem is to produce a diverse set of training partners. Drawing inspiration from successful multi-agent approaches in competitive domains, we find that a surprisingly simple ap"
    },
    {
        "title": "Collaborating with Humans without Human Data",
        "text": "proach is highly effective. We train our agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method we call Fictitious Co-Play (FCP). Our experiments focus on a two-player collabora"
    },
    {
        "title": "Collaborating with Humans without Human Data",
        "text": "tive cooking simulator that has recently been proposed as a challenge problem for coordination with humans. We find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners. Furthermore, humans also re"
    },
    {
        "title": "Collaborating with Humans without Human Data",
        "text": "port a strong subjective preference to partnering with FCP agents over all baselines."
    },
    {
        "title": "To Charge or To Sell? EV Pack Useful Life Estimation via LSTMs and   Autoencoders",
        "text": "Electric Vehicles (EVs) are spreading fast as they promise to provide better performances and comfort, but above all, to help facing climate change. Despite their success, their cost is still a challenge. One of the most expensive components of EVs is lith"
    },
    {
        "title": "To Charge or To Sell? EV Pack Useful Life Estimation via LSTMs and   Autoencoders",
        "text": "ium-ion batteries, which became the standard for energy storage in a wide range of applications. Precisely estimating the Remaining Useful Life (RUL) of battery packs can open to their reuse and thus help to reduce the cost of EVs and improve sustainabilit"
    },
    {
        "title": "To Charge or To Sell? EV Pack Useful Life Estimation via LSTMs and   Autoencoders",
        "text": "y. A correct RUL estimation can be used to quantify the residual market value of the battery pack. The customer can then decide to sell the battery when it still has a value, i.e., before it exceeds its end of life of the target application and can still b"
    },
    {
        "title": "To Charge or To Sell? EV Pack Useful Life Estimation via LSTMs and   Autoencoders",
        "text": "e reused in a second domain without compromising safety and reliability. In this paper, we propose to use a Deep Learning approach based on LSTMs and Autoencoders to estimate the RUL of li-ion batteries. Compared to what has been proposed so far in the lit"
    },
    {
        "title": "To Charge or To Sell? EV Pack Useful Life Estimation via LSTMs and   Autoencoders",
        "text": "erature, we employ measures to ensure the applicability of the method also in the real deployed application. Such measures include (1) avoid using non-measurable variables as input, (2) employ appropriate datasets with wide variability and different condit"
    },
    {
        "title": "To Charge or To Sell? EV Pack Useful Life Estimation via LSTMs and   Autoencoders",
        "text": "ions, (3) do not use cycles to define the RUL."
    },
    {
        "title": "Automatic Grading of Knee Osteoarthritis on the Kellgren-Lawrence Scale   from Radiographs Using Convolutional Neural Networks",
        "text": "The severity of knee osteoarthritis is graded using the 5-point Kellgren-Lawrence (KL) scale where healthy knees are assigned grade 0, and the subsequent grades 1-4 represent increasing severity of the affliction. Although several methods have been propose"
    },
    {
        "title": "Automatic Grading of Knee Osteoarthritis on the Kellgren-Lawrence Scale   from Radiographs Using Convolutional Neural Networks",
        "text": "d in recent years to develop models that can automatically predict the KL grade from a given radiograph, most models have been developed and evaluated on datasets not sourced from India. These models fail to perform well on the radiographs of Indian patien"
    },
    {
        "title": "Automatic Grading of Knee Osteoarthritis on the Kellgren-Lawrence Scale   from Radiographs Using Convolutional Neural Networks",
        "text": "ts. In this paper, we propose a novel method using convolutional neural networks to automatically grade knee radiographs on the KL scale. Our method works in two connected stages: in the first stage, an object detection model segments individual knees from"
    },
    {
        "title": "Automatic Grading of Knee Osteoarthritis on the Kellgren-Lawrence Scale   from Radiographs Using Convolutional Neural Networks",
        "text": " the rest of the image; in the second stage, a regression model automatically grades each knee separately on the KL scale. We train our model using the publicly available Osteoarthritis Initiative (OAI) dataset and demonstrate that fine-tuning the model be"
    },
    {
        "title": "Automatic Grading of Knee Osteoarthritis on the Kellgren-Lawrence Scale   from Radiographs Using Convolutional Neural Networks",
        "text": "fore evaluating it on a dataset from a private hospital significantly improves the mean absolute error from 1.09 (95% CI: 1.03-1.15) to 0.28 (95% CI: 0.25-0.32). Additionally, we compare classification and regression models built for the same task and demo"
    },
    {
        "title": "Automatic Grading of Knee Osteoarthritis on the Kellgren-Lawrence Scale   from Radiographs Using Convolutional Neural Networks",
        "text": "nstrate that regression outperforms classification."
    },
    {
        "title": "Analysis and classification of main risk factors causing stroke in   Shanxi Province",
        "text": "In China, stroke is the first leading cause of death in recent years. It is a major cause of long-term physical and cognitive impairment, which bring great pressure on the National Public Health System. Evaluation of the risk of getting stroke is important"
    },
    {
        "title": "Analysis and classification of main risk factors causing stroke in   Shanxi Province",
        "text": " for the prevention and treatment of stroke in China. A data set with 2000 hospitalized stroke patients in 2018 and 27583 residents during the year 2017 to 2020 is analyzed in this study. Due to data incompleteness, inconsistency, and non-structured format"
    },
    {
        "title": "Analysis and classification of main risk factors causing stroke in   Shanxi Province",
        "text": "s, missing values in the raw data are filled with -1 as an abnormal class. With the cleaned features, three models on risk levels of getting stroke are built by using machine learning methods. The importance of \"8+2\" factors from China National Stroke Prev"
    },
    {
        "title": "Analysis and classification of main risk factors causing stroke in   Shanxi Province",
        "text": "ention Project (CSPP) is evaluated via decision tree and random forest models. Except for \"8+2\" factors the importance of features and SHAP1 values for lifestyle information, demographic information, and medical measurement are evaluated and ranked via a r"
    },
    {
        "title": "Analysis and classification of main risk factors causing stroke in   Shanxi Province",
        "text": "andom forest model. Furthermore, a logistic regression model is applied to evaluate the probability of getting stroke for different risk levels. Based on the census data in both communities and hospitals from Shanxi Province, we investigate different risk "
    },
    {
        "title": "Analysis and classification of main risk factors causing stroke in   Shanxi Province",
        "text": "factors of getting stroke and their ranking with interpretable machine learning models. The results show that Hypertension (Systolic blood pressure, Diastolic blood pressure), Physical Inactivity (Lack of sports), and Overweight (BMI) are ranked as the top"
    },
    {
        "title": "Analysis and classification of main risk factors causing stroke in   Shanxi Province",
        "text": " three high-risk factors of getting stroke in Shanxi province. The probability of getting stroke for a person can also be predicted via our machine learning model."
    },
    {
        "title": "Deep Unfolded Recovery of Sub-Nyquist Sampled Ultrasound Image",
        "text": "The most common technique for generating B-mode ultrasound (US) images is delay and sum (DAS) beamforming, where the signals received at the transducer array are sampled before an appropriate delay is applied. This necessitates sampling rates exceeding the"
    },
    {
        "title": "Deep Unfolded Recovery of Sub-Nyquist Sampled Ultrasound Image",
        "text": " Nyquist rate and the use of a large number of antenna elements to ensure sufficient image quality. Recently we proposed methods to reduce the sampling rate and the array size relying on image recovery using iterative algorithms, based on compressed sensin"
    },
    {
        "title": "Deep Unfolded Recovery of Sub-Nyquist Sampled Ultrasound Image",
        "text": "g (CS) and the finite rate of innovation (FRI) frameworks. Iterative algorithms typically require a large number of iterations, making them difficult to use in real-time. Here, we propose a reconstruction method from sub-Nyquist samples in the time and spa"
    },
    {
        "title": "Deep Unfolded Recovery of Sub-Nyquist Sampled Ultrasound Image",
        "text": "tial domain, that is based on unfolding the ISTA algorithm, resulting in an efficient and interpretable deep network. The inputs to our network are the subsampled beamformed signals after summation and delay in the frequency domain, requiring only a subset"
    },
    {
        "title": "Deep Unfolded Recovery of Sub-Nyquist Sampled Ultrasound Image",
        "text": " of the US signal to be stored for recovery. Our method allows reducing the number of array elements, sampling rate, and computational time while ensuring high quality imaging performance. Using \\emph{in vivo} data we demonstrate that the proposed method y"
    },
    {
        "title": "Deep Unfolded Recovery of Sub-Nyquist Sampled Ultrasound Image",
        "text": "ields high-quality images while reducing the data volume traditionally used up to 36 times. In terms of image resolution and contrast, our technique outperforms previously suggested methods as well as DAS and minimum-variance (MV) beamforming, paving the w"
    },
    {
        "title": "Deep Unfolded Recovery of Sub-Nyquist Sampled Ultrasound Image",
        "text": "ay to real-time applicable recovery methods."
    },
    {
        "title": "Learning Distributed Representations from Reviews for Collaborative   Filtering",
        "text": "Recent work has shown that collaborative filter-based recommender systems can be improved by incorporating side information, such as natural language reviews, as a way of regularizing the derived product representations. Motivated by the success of this ap"
    },
    {
        "title": "Learning Distributed Representations from Reviews for Collaborative   Filtering",
        "text": "proach, we introduce two different models of reviews and study their effect on collaborative filtering performance. While the previous state-of-the-art approach is based on a latent Dirichlet allocation (LDA) model of reviews, the models we explore are neu"
    },
    {
        "title": "Learning Distributed Representations from Reviews for Collaborative   Filtering",
        "text": "ral network based: a bag-of-words product-of-experts model and a recurrent neural network. We demonstrate that the increased flexibility offered by the product-of-experts model allowed it to achieve state-of-the-art performance on the Amazon review dataset"
    },
    {
        "title": "Learning Distributed Representations from Reviews for Collaborative   Filtering",
        "text": ", outperforming the LDA-based approach. However, interestingly, the greater modeling power offered by the recurrent neural network appears to undermine the model's ability to act as a regularizer of the product representations."
    },
    {
        "title": "A Comprehensive Study on Deep Learning Bug Characteristics",
        "text": "Deep learning has gained substantial popularity in recent years. Developers mainly rely on libraries and tools to add deep learning capabilities to their software. What kinds of bugs are frequently found in such software? What are the root causes of such b"
    },
    {
        "title": "A Comprehensive Study on Deep Learning Bug Characteristics",
        "text": "ugs? What impacts do such bugs have? Which stages of deep learning pipeline are more bug prone? Are there any antipatterns? Understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep lear"
    },
    {
        "title": "A Comprehensive Study on Deep Learning Bug Characteristics",
        "text": "ning platforms, debugging mechanisms, development practices, and encourage the development of analysis and verification frameworks. Therefore, we study 2716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep "
    },
    {
        "title": "A Comprehensive Study on Deep Learning Bug Characteristics",
        "text": "learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy softwa"
    },
    {
        "title": "A Comprehensive Study on Deep Learning Bug Characteristics",
        "text": "re. The key findings of our study include: data bug and logic bug are the most severe bug types in deep learning software appearing more than 48% of the times, major root causes of these bugs are Incorrect Model Parameter (IPS) and Structural Inefficiency "
    },
    {
        "title": "A Comprehensive Study on Deep Learning Bug Characteristics",
        "text": "(SI) showing up more than 43% of the times. We have also found that the bugs in the usage of deep learning libraries have some common antipatterns that lead to a strong correlation of bug types among the libraries."
    },
    {
        "title": "Investigating underdiagnosis of AI algorithms in the presence of   multiple sources of dataset bias",
        "text": "Deep learning models have shown great potential for image-based diagnosis assisting clinical decision making. At the same time, an increasing number of reports raise concerns about the potential risk that machine learning could amplify existing health disp"
    },
    {
        "title": "Investigating underdiagnosis of AI algorithms in the presence of   multiple sources of dataset bias",
        "text": "arities due to human biases that are embedded in the training data. It is of great importance to carefully investigate the extent to which biases may be reproduced or even amplified if we wish to build fair artificial intelligence systems. Seyyed-Kalantari"
    },
    {
        "title": "Investigating underdiagnosis of AI algorithms in the presence of   multiple sources of dataset bias",
        "text": " et al. advance this conversation by analysing the performance of a disease classifier across population subgroups. They raise performance disparities related to underdiagnosis as a point of concern; we identify areas from this analysis which we believe de"
    },
    {
        "title": "Investigating underdiagnosis of AI algorithms in the presence of   multiple sources of dataset bias",
        "text": "serve additional attention. Specifically, we wish to highlight some theoretical and practical difficulties associated with assessing model fairness through testing on data drawn from the same biased distribution as the training data, especially when the so"
    },
    {
        "title": "Investigating underdiagnosis of AI algorithms in the presence of   multiple sources of dataset bias",
        "text": "urces and amount of biases are unknown."
    },
    {
        "title": "Learning to Persuade on the Fly: Robustness Against Ignorance",
        "text": "We study a repeated persuasion setting between a sender and a receiver, where at each time $t$, the sender observes a payoff-relevant state drawn independently and identically from an unknown prior distribution, and shares state information with the receiv"
    },
    {
        "title": "Learning to Persuade on the Fly: Robustness Against Ignorance",
        "text": "er, who then myopically chooses an action. As in the standard setting, the sender seeks to persuade the receiver into choosing actions that are aligned with the sender's preference by selectively sharing information about the state. However, in contrast to"
    },
    {
        "title": "Learning to Persuade on the Fly: Robustness Against Ignorance",
        "text": " the standard models, the sender does not know the prior, and has to persuade while gradually learning the prior on the fly.   We study the sender's learning problem of making persuasive action recommendations to achieve low regret against the optimal pers"
    },
    {
        "title": "Learning to Persuade on the Fly: Robustness Against Ignorance",
        "text": "uasion mechanism with the knowledge of the prior distribution. Our main positive result is an algorithm that, with high probability, is persuasive across all rounds and achieves $O(\\sqrt{T\\log T})$ regret, where $T$ is the horizon length. The core philosop"
    },
    {
        "title": "Learning to Persuade on the Fly: Robustness Against Ignorance",
        "text": "hy behind the design of our algorithm is to leverage robustness against the sender's ignorance of the prior. Intuitively, at each time our algorithm maintains a set of candidate priors, and chooses a persuasion scheme that is simultaneously persuasive for "
    },
    {
        "title": "Learning to Persuade on the Fly: Robustness Against Ignorance",
        "text": "all of them. To demonstrate the effectiveness of our algorithm, we further prove that no algorithm can achieve regret better than $\\Omega(\\sqrt{T})$, even if the persuasiveness requirements were significantly relaxed. Therefore, our algorithm achieves opti"
    },
    {
        "title": "Learning to Persuade on the Fly: Robustness Against Ignorance",
        "text": "mal regret for the sender's learning problem up to terms logarithmic in $T$."
    },
    {
        "title": "Causal Markov Decision Processes: Learning Good Interventions   Efficiently",
        "text": "We introduce causal Markov Decision Processes (C-MDPs), a new formalism for sequential decision making which combines the standard MDP formulation with causal structures over state transition and reward functions. Many contemporary and emerging application"
    },
    {
        "title": "Causal Markov Decision Processes: Learning Good Interventions   Efficiently",
        "text": " areas such as digital healthcare and digital marketing can benefit from modeling with C-MDPs due to the causal mechanisms underlying the relationship between interventions and states/rewards. We propose the causal upper confidence bound value iteration (C"
    },
    {
        "title": "Causal Markov Decision Processes: Learning Good Interventions   Efficiently",
        "text": "-UCBVI) algorithm that exploits the causal structure in C-MDPs and improves the performance of standard reinforcement learning algorithms that do not take causal knowledge into account. We prove that C-UCBVI satisfies an $\\tilde{O}(HS\\sqrt{ZT})$ regret bou"
    },
    {
        "title": "Causal Markov Decision Processes: Learning Good Interventions   Efficiently",
        "text": "nd, where $T$ is the the total time steps, $H$ is the episodic horizon, and $S$ is the cardinality of the state space. Notably, our regret bound does not scale with the size of actions/interventions ($A$), but only scales with a causal graph dependent quan"
    },
    {
        "title": "Causal Markov Decision Processes: Learning Good Interventions   Efficiently",
        "text": "tity $Z$ which can be exponentially smaller than $A$. By extending C-UCBVI to the factored MDP setting, we propose the causal factored UCBVI (CF-UCBVI) algorithm, which further reduces the regret exponentially in terms of $S$. Furthermore, we show that RL "
    },
    {
        "title": "Causal Markov Decision Processes: Learning Good Interventions   Efficiently",
        "text": "algorithms for linear MDP problems can also be incorporated in C-MDPs. We empirically show the benefit of our causal approaches in various settings to validate our algorithms and theoretical results."
    },
    {
        "title": "Unmanned Aerial Vehicle Visual Detection and Tracking using Deep Neural   Networks: A Performance Benchmark",
        "text": "Unmanned Aerial Vehicles (UAV) can pose a major risk for aviation safety, due to both negligent and malicious use. For this reason, the automated detection and tracking of UAV is a fundamental task in aerial security systems. Common technologies for UAV de"
    },
    {
        "title": "Unmanned Aerial Vehicle Visual Detection and Tracking using Deep Neural   Networks: A Performance Benchmark",
        "text": "tection include visible-band and thermal infrared imaging, radio frequency and radar. Recent advances in deep neural networks (DNNs) for image-based object detection open the possibility to use visual information for this detection and tracking task. Furth"
    },
    {
        "title": "Unmanned Aerial Vehicle Visual Detection and Tracking using Deep Neural   Networks: A Performance Benchmark",
        "text": "ermore, these detection architectures can be implemented as backbones for visual tracking systems, thereby enabling persistent tracking of UAV incursions. To date, no comprehensive performance benchmark exists that applies DNNs to visible-band imagery for "
    },
    {
        "title": "Unmanned Aerial Vehicle Visual Detection and Tracking using Deep Neural   Networks: A Performance Benchmark",
        "text": "UAV detection and tracking. To this end, three datasets with varied environmental conditions for UAV detection and tracking, comprising a total of 241 videos (331,486 images), are assessed using four detection architectures and three tracking frameworks. T"
    },
    {
        "title": "Unmanned Aerial Vehicle Visual Detection and Tracking using Deep Neural   Networks: A Performance Benchmark",
        "text": "he best performing detector architecture obtains an mAP of 98.6% and the best performing tracking framework obtains a MOTA of 96.3%. Cross-modality evaluation is carried out between visible and infrared spectrums, achieving a maximal 82.8% mAP on visible i"
    },
    {
        "title": "Unmanned Aerial Vehicle Visual Detection and Tracking using Deep Neural   Networks: A Performance Benchmark",
        "text": "mages when training in the infrared modality. These results provide the first public multi-approach benchmark for state-of-the-art deep learning-based methods and give insight into which detection and tracking architectures are effective in the UAV domain."
    },
    {
        "title": "Inertial Newton Algorithms Avoiding Strict Saddle Points",
        "text": "We study the asymptotic behavior of second-order algorithms mixing Newton's method and inertial gradient descent in non-convex landscapes. We show that, despite the Newtonian behavior of these methods, they almost always escape strict saddle points. We als"
    },
    {
        "title": "Inertial Newton Algorithms Avoiding Strict Saddle Points",
        "text": "o evidence the role played by the hyper-parameters of these methods in their qualitative behavior near critical points. The theoretical results are supported by numerical illustrations."
    },
    {
        "title": "PIDForest: Anomaly Detection via Partial Identification",
        "text": "We consider the problem of detecting anomalies in a large dataset. We propose a framework called Partial Identification which captures the intuition that anomalies are easy to distinguish from the overwhelming majority of points by relatively few attribute"
    },
    {
        "title": "PIDForest: Anomaly Detection via Partial Identification",
        "text": " values. Formalizing this intuition, we propose a geometric anomaly measure for a point that we call PIDScore, which measures the minimum density of data points over all subcubes containing the point. We present PIDForest: a random forest based algorithm t"
    },
    {
        "title": "PIDForest: Anomaly Detection via Partial Identification",
        "text": "hat finds anomalies based on this definition. We show that it performs favorably in comparison to several popular anomaly detection methods, across a broad range of benchmarks. PIDForest also provides a succinct explanation for why a point is labelled anom"
    },
    {
        "title": "PIDForest: Anomaly Detection via Partial Identification",
        "text": "alous, by providing a set of features and ranges for them which are relatively uncommon in the dataset."
    },
    {
        "title": "Traffic4cast-Traffic Map Movie Forecasting -- Team MIE-Lab",
        "text": "The goal of the IARAI competition traffic4cast was to predict the city-wide traffic status within a 15-minute time window, based on information from the previous hour. The traffic status was given as multi-channel images (one pixel roughly corresponds to 1"
    },
    {
        "title": "Traffic4cast-Traffic Map Movie Forecasting -- Team MIE-Lab",
        "text": "00x100 meters), where one channel indicated the traffic volume, another one the average speed of vehicles, and a third one their rough heading. As part of our work on the competition, we evaluated many different network architectures, analyzed the statisti"
    },
    {
        "title": "Traffic4cast-Traffic Map Movie Forecasting -- Team MIE-Lab",
        "text": "cal properties of the given data in detail, and thought about how to transform the problem to be able to take additional spatio-temporal context-information into account, such as the street network, the positions of traffic lights, or the weather. This doc"
    },
    {
        "title": "Traffic4cast-Traffic Map Movie Forecasting -- Team MIE-Lab",
        "text": "ument summarizes our efforts that led to our best submission, and gives some insights about which other approaches we evaluated, and why they did not work as well as imagined."
    },
    {
        "title": "Reward Tweaking: Maximizing the Total Reward While Planning for Short   Horizons",
        "text": "In reinforcement learning, the discount factor $\\gamma$ controls the agent's effective planning horizon. Traditionally, this parameter was considered part of the MDP; however, as deep reinforcement learning algorithms tend to become unstable when the effec"
    },
    {
        "title": "Reward Tweaking: Maximizing the Total Reward While Planning for Short   Horizons",
        "text": "tive planning horizon is long, recent works refer to $\\gamma$ as a hyper-parameter -- thus changing the underlying MDP and potentially leading the agent towards sub-optimal behavior on the original task. In this work, we introduce \\emph{reward tweaking}. R"
    },
    {
        "title": "Reward Tweaking: Maximizing the Total Reward While Planning for Short   Horizons",
        "text": "eward tweaking learns a surrogate reward function $\\tilde r$ for the discounted setting that induces optimal behavior on the original finite-horizon total reward task. Theoretically, we show that there exists a surrogate reward that leads to optimality in "
    },
    {
        "title": "Reward Tweaking: Maximizing the Total Reward While Planning for Short   Horizons",
        "text": "the original task and discuss the robustness of our approach. Additionally, we perform experiments in high-dimensional continuous control tasks and show that reward tweaking guides the agent towards better long-horizon returns although it plans for short h"
    },
    {
        "title": "Reward Tweaking: Maximizing the Total Reward While Planning for Short   Horizons",
        "text": "orizons."
    },
    {
        "title": "A Benchmark to Select Data Mining Based Classification Algorithms For   Business Intelligence And Decision Support Systems",
        "text": "DSS serve the management, operations, and planning levels of an organization and help to make decisions, which may be rapidly changing and not easily specified in advance. Data mining has a vital role to extract important information to help in decision ma"
    },
    {
        "title": "A Benchmark to Select Data Mining Based Classification Algorithms For   Business Intelligence And Decision Support Systems",
        "text": "king of a decision support system. Integration of data mining and decision support systems (DSS) can lead to the improved performance and can enable the tackling of new types of problems. Artificial Intelligence methods are improving the quality of decisio"
    },
    {
        "title": "A Benchmark to Select Data Mining Based Classification Algorithms For   Business Intelligence And Decision Support Systems",
        "text": "n support, and have become embedded in many applications ranges from ant locking automobile brakes to these days interactive search engines. It provides various machine learning techniques to support data mining. The classification is one of the main and v"
    },
    {
        "title": "A Benchmark to Select Data Mining Based Classification Algorithms For   Business Intelligence And Decision Support Systems",
        "text": "aluable tasks of data mining. Several types of classification algorithms have been suggested, tested and compared to determine the future trends based on unseen data. There has been no single algorithm found to be superior over all others for all data sets"
    },
    {
        "title": "A Benchmark to Select Data Mining Based Classification Algorithms For   Business Intelligence And Decision Support Systems",
        "text": ". The objective of this paper is to compare various classification algorithms that have been frequently used in data mining for decision support systems. Three decision trees based algorithms, one artificial neural network, one statistical, one support vec"
    },
    {
        "title": "A Benchmark to Select Data Mining Based Classification Algorithms For   Business Intelligence And Decision Support Systems",
        "text": "tor machines with and without ada boost and one clustering algorithm are tested and compared on four data sets from different domains in terms of predictive accuracy, error rate, classification index, comprehensibility and training time. Experimental resul"
    },
    {
        "title": "A Benchmark to Select Data Mining Based Classification Algorithms For   Business Intelligence And Decision Support Systems",
        "text": "ts demonstrate that Genetic Algorithm (GA) and support vector machines based algorithms are better in terms of predictive accuracy. SVM without adaboost shall be the first choice in context of speed and predictive accuracy. Adaboost improves the accuracy o"
    },
    {
        "title": "A Benchmark to Select Data Mining Based Classification Algorithms For   Business Intelligence And Decision Support Systems",
        "text": "f SVM but on the cost of large training time."
    },
    {
        "title": "S-Rocket: Selective Random Convolution Kernels for Time Series   Classification",
        "text": "Random convolution kernel transform (Rocket) is a fast, efficient, and novel approach for time series feature extraction, using a large number of randomly initialized convolution kernels, and classification of the represented features with a linear classif"
    },
    {
        "title": "S-Rocket: Selective Random Convolution Kernels for Time Series   Classification",
        "text": "ier, without training the kernels. Since these kernels are generated randomly, a portion of these kernels may not positively contribute in performance of the model. Hence, selection of the most important kernels and pruning the redundant and less important"
    },
    {
        "title": "S-Rocket: Selective Random Convolution Kernels for Time Series   Classification",
        "text": " ones is necessary to reduce computational complexity and accelerate inference of Rocket. Selection of these kernels is a combinatorial optimization problem. In this paper, the kernels selection process is modeled as an optimization problem and a populatio"
    },
    {
        "title": "S-Rocket: Selective Random Convolution Kernels for Time Series   Classification",
        "text": "n-based approach is proposed for selecting the most important kernels. This approach is evaluated on the standard time series datasets and the results show that on average it can achieve a similar performance to the original models by pruning more than 60%"
    },
    {
        "title": "S-Rocket: Selective Random Convolution Kernels for Time Series   Classification",
        "text": " of kernels. In some cases, it can achieve a similar performance using only 1% of the kernels."
    },
    {
        "title": "Generative Adversarial Networks for Operational Scenario Planning of   Renewable Energy Farms: A Study on Wind and Photovoltaic",
        "text": "For the integration of renewable energy sources, power grid operators need realistic information about the effects of energy production and consumption to assess grid stability.   Recently, research in scenario planning benefits from utilizing generative a"
    },
    {
        "title": "Generative Adversarial Networks for Operational Scenario Planning of   Renewable Energy Farms: A Study on Wind and Photovoltaic",
        "text": "dversarial networks (GANs) as generative models for operational scenario planning.   In these scenarios, operators examine temporal as well as spatial influences of different energy sources on the grid.   The analysis of how renewable energy resources affe"
    },
    {
        "title": "Generative Adversarial Networks for Operational Scenario Planning of   Renewable Energy Farms: A Study on Wind and Photovoltaic",
        "text": "ct the grid enables the operators to evaluate the stability and to identify potential weak points such as a limiting transformer.   However, due to their novelty, there are limited studies on how well GANs model the underlying power distribution.   This an"
    },
    {
        "title": "Generative Adversarial Networks for Operational Scenario Planning of   Renewable Energy Farms: A Study on Wind and Photovoltaic",
        "text": "alysis is essential because, e.g., especially extreme situations with low or high power generation are required to evaluate grid stability.   We conduct a comparative study of the Wasserstein distance, binary-cross-entropy loss, and a Gaussian copula as th"
    },
    {
        "title": "Generative Adversarial Networks for Operational Scenario Planning of   Renewable Energy Farms: A Study on Wind and Photovoltaic",
        "text": "e baseline applied on two wind and two solar datasets with limited data compared to previous studies.   Both GANs achieve good results considering the limited amount of data, but the Wasserstein GAN is superior in modeling temporal and spatial relations, a"
    },
    {
        "title": "Generative Adversarial Networks for Operational Scenario Planning of   Renewable Energy Farms: A Study on Wind and Photovoltaic",
        "text": "nd the power distribution.   Besides evaluating the generated power distribution over all farms, it is essential to assess terrain specific distributions for wind scenarios.   These terrain specific power distributions affect the grid by their differences "
    },
    {
        "title": "Generative Adversarial Networks for Operational Scenario Planning of   Renewable Energy Farms: A Study on Wind and Photovoltaic",
        "text": "in their generating power magnitude.   Therefore, in a second study, we show that even when simultaneously learning distributions from wind parks with terrain specific patterns, GANs are capable of modeling these individualities also when faced with limite"
    },
    {
        "title": "Generative Adversarial Networks for Operational Scenario Planning of   Renewable Energy Farms: A Study on Wind and Photovoltaic",
        "text": "d data."
    },
    {
        "title": "Collaborative filtering via sparse Markov random fields",
        "text": "Recommender systems play a central role in providing individualized access to information and services. This paper focuses on collaborative filtering, an approach that exploits the shared structure among mind-liked users and similar items. In particular, w"
    },
    {
        "title": "Collaborative filtering via sparse Markov random fields",
        "text": "e focus on a formal probabilistic framework known as Markov random fields (MRF). We address the open problem of structure learning and introduce a sparsity-inducing algorithm to automatically estimate the interaction structures between users and between it"
    },
    {
        "title": "Collaborative filtering via sparse Markov random fields",
        "text": "ems. Item-item and user-user correlation networks are obtained as a by-product. Large-scale experiments on movie recommendation and date matching datasets demonstrate the power of the proposed method."
    },
    {
        "title": "Embedding-based Silhouette Community Detection",
        "text": "Mining complex data in the form of networks is of increasing interest in many scientific disciplines. Network communities correspond to densely connected subnetworks, and often represent key functional parts of real-world systems. In this work, we propose "
    },
    {
        "title": "Embedding-based Silhouette Community Detection",
        "text": "Silhouette Community Detection (SCD), an approach for detecting communities, based on clustering of network node embeddings, i.e. real valued representations of nodes derived from their neighborhoods. We investigate the performance of the proposed SCD appr"
    },
    {
        "title": "Embedding-based Silhouette Community Detection",
        "text": "oach on 234 synthetic networks, as well as on a real-life social network. Even though SCD is not based on any form of modularity optimization, it performs comparably or better than state-of-the-art community detection algorithms, such as the InfoMap and Lo"
    },
    {
        "title": "Embedding-based Silhouette Community Detection",
        "text": "uvain algorithms. Further, we demonstrate how SCD's outputs can be used along with domain ontologies in semantic subgroup discovery, yielding human-understandable explanations of communities detected in a real-life protein interaction network. Being embedd"
    },
    {
        "title": "Embedding-based Silhouette Community Detection",
        "text": "ing-based, SCD is widely applicable and can be tested out-of-the-box as part of many existing network learning and exploration pipelines."
    },
    {
        "title": "Controlling Robot Morphology from Incomplete Measurements",
        "text": "Mobile robots with complex morphology are essential for traversing rough terrains in Urban Search & Rescue missions (USAR). Since teleoperation of the complex morphology causes high cognitive load of the operator, the morphology is controlled autonomously."
    },
    {
        "title": "Controlling Robot Morphology from Incomplete Measurements",
        "text": " The autonomous control measures the robot state and surrounding terrain which is usually only partially observable, and thus the data are often incomplete. We marginalize the control over the missing measurements and evaluate an explicit safety condition."
    },
    {
        "title": "Controlling Robot Morphology from Incomplete Measurements",
        "text": " If the safety condition is violated, tactile terrain exploration by the body-mounted robotic arm gathers the missing data."
    },
    {
        "title": "Machine learning a manifold",
        "text": "We propose a simple method to identify a continuous Lie algebra symmetry in a dataset through regression by an artificial neural network. Our proposal takes advantage of the $ \\mathcal{O}(\\epsilon^2)$ scaling of the output variable under infinitesimal symm"
    },
    {
        "title": "Machine learning a manifold",
        "text": "etry transformations on the input variables. As symmetry transformations are generated post-training, the methodology does not rely on sampling of the full representation space or binning of the dataset, and the possibility of false identification is minim"
    },
    {
        "title": "Machine learning a manifold",
        "text": "ised. We demonstrate our method in the SU(3)-symmetric (non-) linear $\\Sigma$ model."
    },
    {
        "title": "Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in   Pretrained Embeddings",
        "text": "Given a small corpus $\\mathcal D_T$ pertaining to a limited set of focused topics, our goal is to train embeddings that accurately capture the sense of words in the topic in spite of the limited size of $\\mathcal D_T$. These embeddings may be used in vario"
    },
    {
        "title": "Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in   Pretrained Embeddings",
        "text": "us tasks involving $\\mathcal D_T$. A popular strategy in limited data settings is to adapt pre-trained embeddings $\\mathcal E$ trained on a large corpus. To correct for sense drift, fine-tuning, regularization, projection, and pivoting have been proposed r"
    },
    {
        "title": "Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in   Pretrained Embeddings",
        "text": "ecently. Among these, regularization informed by a word's corpus frequency performed well, but we improve upon it using a new regularizer based on the stability of its cooccurrence with other words. However, a thorough comparison across ten topics, spannin"
    },
    {
        "title": "Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in   Pretrained Embeddings",
        "text": "g three tasks, with standardized settings of hyper-parameters, reveals that even the best embedding adaptation strategies provide small gains beyond well-tuned baselines, which many earlier comparisons ignored. In a bold departure from adapting pretrained "
    },
    {
        "title": "Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in   Pretrained Embeddings",
        "text": "embeddings, we propose using $\\mathcal D_T$ to probe, attend to, and borrow fragments from any large, topic-rich source corpus (such as Wikipedia), which need not be the corpus used to pretrain embeddings. This step is made scalable and practical by suitab"
    },
    {
        "title": "Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in   Pretrained Embeddings",
        "text": "le indexing. We reach the surprising conclusion that even limited corpus augmentation is more useful than adapting embeddings, which suggests that non-dominant sense information may be irrevocably obliterated from pretrained embeddings and cannot be salvag"
    },
    {
        "title": "Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in   Pretrained Embeddings",
        "text": "ed by adaptation."
    },
    {
        "title": "Prototype-based Neural Network Layers: Incorporating Vector Quantization",
        "text": "Neural networks currently dominate the machine learning community and they do so for good reasons. Their accuracy on complex tasks such as image classification is unrivaled at the moment and with recent improvements they are reasonably easy to train. Never"
    },
    {
        "title": "Prototype-based Neural Network Layers: Incorporating Vector Quantization",
        "text": "theless, neural networks are lacking robustness and interpretability. Prototype-based vector quantization methods on the other hand are known for being robust and interpretable. For this reason, we propose techniques and strategies to merge both approaches"
    },
    {
        "title": "Prototype-based Neural Network Layers: Incorporating Vector Quantization",
        "text": ". This contribution will particularly highlight the similarities between them and outline how to construct a prototype-based classification layer for multilayer networks. Additionally, we provide an alternative, prototype-based, approach to the classical c"
    },
    {
        "title": "Prototype-based Neural Network Layers: Incorporating Vector Quantization",
        "text": "onvolution operation. Numerical results are not part of this report, instead the focus lays on establishing a strong theoretical framework. By publishing our framework and the respective theoretical considerations and justifications before finalizing our n"
    },
    {
        "title": "Prototype-based Neural Network Layers: Incorporating Vector Quantization",
        "text": "umerical experiments we hope to jump-start the incorporation of prototype-based learning in neural networks and vice versa."
    },
    {
        "title": "Anomaly Detection in a Digital Video Broadcasting System Using Timed   Automata",
        "text": "This paper focuses on detecting anomalies in a digital video broadcasting (DVB) system from providers' perspective. We learn a probabilistic deterministic real timed automaton profiling benign behavior of encryption control in the DVB control access system"
    },
    {
        "title": "Anomaly Detection in a Digital Video Broadcasting System Using Timed   Automata",
        "text": ". This profile is used as a one-class classifier. Anomalous items in a testing sequence are detected when the sequence is not accepted by the learned model."
    },
    {
        "title": "Faster Meta Update Strategy for Noise-Robust Deep Learning",
        "text": "It has been shown that deep neural networks are prone to overfitting on biased training data. Towards addressing this issue, meta-learning employs a meta model for correcting the training bias. Despite the promising performances, super slow training is cur"
    },
    {
        "title": "Faster Meta Update Strategy for Noise-Robust Deep Learning",
        "text": "rently the bottleneck in the meta learning approaches. In this paper, we introduce a novel Faster Meta Update Strategy (FaMUS) to replace the most expensive step in the meta gradient computation with a faster layer-wise approximation. We empirically find t"
    },
    {
        "title": "Faster Meta Update Strategy for Noise-Robust Deep Learning",
        "text": "hat FaMUS yields not only a reasonably accurate but also a low-variance approximation of the meta gradient. We conduct extensive experiments to verify the proposed method on two tasks. We show our method is able to save two-thirds of the training time whil"
    },
    {
        "title": "Faster Meta Update Strategy for Noise-Robust Deep Learning",
        "text": "e still maintaining the comparable or achieving even better generalization performance. In particular, our method achieves the state-of-the-art performance on both synthetic and realistic noisy labels, and obtains promising performance on long-tailed recog"
    },
    {
        "title": "Faster Meta Update Strategy for Noise-Robust Deep Learning",
        "text": "nition on standard benchmarks."
    },
    {
        "title": "Machine Learning for Yield Curve Feature Extraction: Application to   Illiquid Corporate Bonds (Preliminary Draft)",
        "text": "This paper studies the application of machine learning in extracting the market implied features from historical risk neutral corporate bond yields. We consider the example of a hypothetical illiquid fixed income market. After choosing a surrogate liquid m"
    },
    {
        "title": "Machine Learning for Yield Curve Feature Extraction: Application to   Illiquid Corporate Bonds (Preliminary Draft)",
        "text": "arket, we apply the Denoising Autoencoder algorithm from the field of computer vision and pattern recognition to learn the features of the missing yield parameters from the historically implied data of the instruments traded in the chosen liquid market. Th"
    },
    {
        "title": "Machine Learning for Yield Curve Feature Extraction: Application to   Illiquid Corporate Bonds (Preliminary Draft)",
        "text": "e results of the trained machine learning algorithm are compared with the outputs of a point in- time 2 dimensional interpolation algorithm known as the Thin Plate Spline. Finally, the performances of the two algorithms are compared."
    },
    {
        "title": "Long Timescale Credit Assignment in NeuralNetworks with External Memory",
        "text": "Credit assignment in traditional recurrent neural networks usually involves back-propagating through a long chain of tied weight matrices. The length of this chain scales linearly with the number of time-steps as the same network is run at each time-step. "
    },
    {
        "title": "Long Timescale Credit Assignment in NeuralNetworks with External Memory",
        "text": "This creates many problems, such as vanishing gradients, that have been well studied. In contrast, a NNEM's architecture recurrent activity doesn't involve a long chain of activity (though some architectures such as the NTM do utilize a traditional recurre"
    },
    {
        "title": "Long Timescale Credit Assignment in NeuralNetworks with External Memory",
        "text": "nt architecture as a controller). Rather, the externally stored embedding vectors are used at each time-step, but no messages are passed from previous time-steps. This means that vanishing gradients aren't a problem, as all of the necessary gradient paths "
    },
    {
        "title": "Long Timescale Credit Assignment in NeuralNetworks with External Memory",
        "text": "are short. However, these paths are extremely numerous (one per embedding vector in memory) and reused for a very long time (until it leaves the memory). Thus, the forward-pass information of each memory must be stored for the entire duration of the memory"
    },
    {
        "title": "Long Timescale Credit Assignment in NeuralNetworks with External Memory",
        "text": ". This is problematic as this additional storage far surpasses that of the actual memories, to the extent that large memories on infeasible to back-propagate through in high dimensional settings. One way to get around the need to hold onto forward-pass inf"
    },
    {
        "title": "Long Timescale Credit Assignment in NeuralNetworks with External Memory",
        "text": "ormation is to recalculate the forward-pass whenever gradient information is available. However, if the observations are too large to store in the domain of interest, direct reinstatement of a forward pass cannot occur. Instead, we rely on a learned autoen"
    },
    {
        "title": "Long Timescale Credit Assignment in NeuralNetworks with External Memory",
        "text": "coder to reinstate the observation, and then use the embedding network to recalculate the forward-pass. Since the recalculated embedding vector is unlikely to perfectly match the one stored in memory, we try out 2 approximations to utilize error gradient w"
    },
    {
        "title": "Long Timescale Credit Assignment in NeuralNetworks with External Memory",
        "text": ".r.t. the vector in memory."
    },
    {
        "title": "Interpretable Subgroup Discovery in Treatment Effect Estimation with   Application to Opioid Prescribing Guidelines",
        "text": "The dearth of prescribing guidelines for physicians is one key driver of the current opioid epidemic in the United States. In this work, we analyze medical and pharmaceutical claims data to draw insights on characteristics of patients who are more prone to"
    },
    {
        "title": "Interpretable Subgroup Discovery in Treatment Effect Estimation with   Application to Opioid Prescribing Guidelines",
        "text": " adverse outcomes after an initial synthetic opioid prescription. Toward this end, we propose a generative model that allows discovery from observational data of subgroups that demonstrate an enhanced or diminished causal effect due to treatment. Our appro"
    },
    {
        "title": "Interpretable Subgroup Discovery in Treatment Effect Estimation with   Application to Opioid Prescribing Guidelines",
        "text": "ach models these sub-populations as a mixture distribution, using sparsity to enhance interpretability, while jointly learning nonlinear predictors of the potential outcomes to better adjust for confounding. The approach leads to human-interpretable insigh"
    },
    {
        "title": "Interpretable Subgroup Discovery in Treatment Effect Estimation with   Application to Opioid Prescribing Guidelines",
        "text": "ts on discovered subgroups, improving the practical utility for decision support"
    },
    {
        "title": "One Student Knows All Experts Know: From Sparse to Dense",
        "text": "Human education system trains one student by multiple experts. Mixture-of-experts (MoE) is a powerful sparse architecture including multiple experts. However, sparse MoE model is hard to implement, easy to overfit, and not hardware-friendly. In this work, "
    },
    {
        "title": "One Student Knows All Experts Know: From Sparse to Dense",
        "text": "inspired by human education model, we propose a novel task, knowledge integration, to obtain a dense student model (OneS) as knowledgeable as one sparse MoE. We investigate this task by proposing a general training framework including knowledge gathering a"
    },
    {
        "title": "One Student Knows All Experts Know: From Sparse to Dense",
        "text": "nd knowledge distillation. Specifically, we first propose Singular Value Decomposition Knowledge Gathering (SVD-KG) to gather key knowledge from different pretrained experts. We then refine the dense student model by knowledge distillation to offset the no"
    },
    {
        "title": "One Student Knows All Experts Know: From Sparse to Dense",
        "text": "ise from gathering. On ImageNet, our OneS preserves $61.7\\%$ benefits from MoE. OneS can achieve $78.4\\%$ top-1 accuracy with only $15$M parameters. On four natural language processing datasets, OneS obtains $88.2\\%$ MoE benefits and outperforms SoTA by $5"
    },
    {
        "title": "One Student Knows All Experts Know: From Sparse to Dense",
        "text": "1.7\\%$ using the same architecture and training data. In addition, compared with the MoE counterpart, OneS can achieve $3.7 \\times$ inference speedup due to the hardware-friendly architecture."
    },
    {
        "title": "RANDGAN: Randomized Generative Adversarial Network for Detection of   COVID-19 in Chest X-ray",
        "text": "COVID-19 spread across the globe at an immense rate has left healthcare systems incapacitated to diagnose and test patients at the needed rate. Studies have shown promising results for detection of COVID-19 from viral bacterial pneumonia in chest X-rays. A"
    },
    {
        "title": "RANDGAN: Randomized Generative Adversarial Network for Detection of   COVID-19 in Chest X-ray",
        "text": "utomation of COVID-19 testing using medical images can speed up the testing process of patients where health care systems lack sufficient numbers of the reverse-transcription polymerase chain reaction (RT-PCR) tests. Supervised deep learning models such as"
    },
    {
        "title": "RANDGAN: Randomized Generative Adversarial Network for Detection of   COVID-19 in Chest X-ray",
        "text": " convolutional neural networks (CNN) need enough labeled data for all classes to correctly learn the task of detection. Gathering labeled data is a cumbersome task and requires time and resources which could further strain health care systems and radiologi"
    },
    {
        "title": "RANDGAN: Randomized Generative Adversarial Network for Detection of   COVID-19 in Chest X-ray",
        "text": "sts at the early stages of a pandemic such as COVID-19. In this study, we propose a randomized generative adversarial network (RANDGAN) that detects images of an unknown class (COVID-19) from known and labelled classes (Normal and Viral Pneumonia) without "
    },
    {
        "title": "RANDGAN: Randomized Generative Adversarial Network for Detection of   COVID-19 in Chest X-ray",
        "text": "the need for labels and training data from the unknown class of images (COVID-19). We used the largest publicly available COVID-19 chest X-ray dataset, COVIDx, which is comprised of Normal, Pneumonia, and COVID-19 images from multiple public databases. In "
    },
    {
        "title": "RANDGAN: Randomized Generative Adversarial Network for Detection of   COVID-19 in Chest X-ray",
        "text": "this work, we use transfer learning to segment the lungs in the COVIDx dataset. Next, we show why segmentation of the region of interest (lungs) is vital to correctly learn the task of classification, specifically in datasets that contain images from diffe"
    },
    {
        "title": "RANDGAN: Randomized Generative Adversarial Network for Detection of   COVID-19 in Chest X-ray",
        "text": "rent resources as it is the case for the COVIDx dataset. Finally, we show improved results in detection of COVID-19 cases using our generative model (RANDGAN) compared to conventional generative adversarial networks (GANs) for anomaly detection in medical "
    },
    {
        "title": "RANDGAN: Randomized Generative Adversarial Network for Detection of   COVID-19 in Chest X-ray",
        "text": "images, improving the area under the ROC curve from 0.71 to 0.77."
    },
    {
        "title": "Ontology-based n-ball Concept Embeddings Informing Few-shot Image   Classification",
        "text": "We propose a novel framework named ViOCE that integrates ontology-based background knowledge in the form of $n$-ball concept embeddings into a neural network based vision architecture. The approach consists of two components - converting symbolic knowledge"
    },
    {
        "title": "Ontology-based n-ball Concept Embeddings Informing Few-shot Image   Classification",
        "text": " of an ontology into continuous space by learning n-ball embeddings that capture properties of subsumption and disjointness, and guiding the training and inference of a vision model using the learnt embeddings. We evaluate ViOCE using the task of few-shot "
    },
    {
        "title": "Ontology-based n-ball Concept Embeddings Informing Few-shot Image   Classification",
        "text": "image classification, where it demonstrates superior performance on two standard benchmarks."
    },
    {
        "title": "Machine Learning-based Orchestration of Containers: A Taxonomy and   Future Directions",
        "text": "Containerization is a lightweight application virtualization technology, providing high environmental consistency, operating system distribution portability, and resource isolation. Existing mainstream cloud service providers have prevalently adopted conta"
    },
    {
        "title": "Machine Learning-based Orchestration of Containers: A Taxonomy and   Future Directions",
        "text": "iner technologies in their distributed system infrastructures for automated application management. To handle the automation of deployment, maintenance, autoscaling, and networking of containerized applications, container orchestration is proposed as an es"
    },
    {
        "title": "Machine Learning-based Orchestration of Containers: A Taxonomy and   Future Directions",
        "text": "sential research problem. However, the highly dynamic and diverse feature of cloud workloads and environments considerably raises the complexity of orchestration mechanisms. Machine learning algorithms are accordingly employed by container orchestration sy"
    },
    {
        "title": "Machine Learning-based Orchestration of Containers: A Taxonomy and   Future Directions",
        "text": "stems for behavior modelling and prediction of multi-dimensional performance metrics. Such insights could further improve the quality of resource provisioning decisions in response to the changing workloads under complex environments. In this paper, we pre"
    },
    {
        "title": "Machine Learning-based Orchestration of Containers: A Taxonomy and   Future Directions",
        "text": "sent a comprehensive literature review of existing machine learning-based container orchestration approaches. Detailed taxonomies are proposed to classify the current researches by their common features. Moreover, the evolution of machine learning-based co"
    },
    {
        "title": "Machine Learning-based Orchestration of Containers: A Taxonomy and   Future Directions",
        "text": "ntainer orchestration technologies from the year 2016 to 2021 has been designed based on objectives and metrics. A comparative analysis of the reviewed techniques is conducted according to the proposed taxonomies, with emphasis on their key characteristics"
    },
    {
        "title": "Machine Learning-based Orchestration of Containers: A Taxonomy and   Future Directions",
        "text": ". Finally, various open research challenges and potential future directions are highlighted."
    },
    {
        "title": "Machine Learning in Heliophysics and Space Weather Forecasting: A White   Paper of Findings and Recommendations",
        "text": "The authors of this white paper met on 16-17 January 2020 at the New Jersey Institute of Technology, Newark, NJ, for a 2-day workshop that brought together a group of heliophysicists, data providers, expert modelers, and computer/data scientists. Their obj"
    },
    {
        "title": "Machine Learning in Heliophysics and Space Weather Forecasting: A White   Paper of Findings and Recommendations",
        "text": "ective was to discuss critical developments and prospects of the application of machine and/or deep learning techniques for data analysis, modeling and forecasting in Heliophysics, and to shape a strategy for further developments in the field. The workshop"
    },
    {
        "title": "Machine Learning in Heliophysics and Space Weather Forecasting: A White   Paper of Findings and Recommendations",
        "text": " combined a set of plenary sessions featuring invited introductory talks interleaved with a set of open discussion sessions. The outcome of the discussion is encapsulated in this white paper that also features a top-level list of recommendations agreed by "
    },
    {
        "title": "Machine Learning in Heliophysics and Space Weather Forecasting: A White   Paper of Findings and Recommendations",
        "text": "participants."
    },
    {
        "title": "Longitudinal Support Vector Machines for High Dimensional Time Series",
        "text": "We consider the problem of learning a classifier from observed functional data. Here, each data-point takes the form of a single time-series and contains numerous features. Assuming that each such series comes with a binary label, the problem of learning t"
    },
    {
        "title": "Longitudinal Support Vector Machines for High Dimensional Time Series",
        "text": "o predict the label of a new coming time-series is considered. Hereto, the notion of {\\em margin} underlying the classical support vector machine is extended to the continuous version for such data. The longitudinal support vector machine is also a convex "
    },
    {
        "title": "Longitudinal Support Vector Machines for High Dimensional Time Series",
        "text": "optimization problem and its dual form is derived as well. Empirical results for specified cases with significance tests indicate the efficacy of this innovative algorithm for analyzing such long-term multivariate data."
    },
    {
        "title": "Unsupervised Transductive Domain Adaptation",
        "text": "Supervised learning with large scale labeled datasets and deep layered models has made a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers generalization issues under the presence of a domain shift between th"
    },
    {
        "title": "Unsupervised Transductive Domain Adaptation",
        "text": "e training and the test data distribution. In this regard, unsupervised domain adaptation algorithms have been proposed to directly address the domain shift problem. In this paper, we approach the problem from a transductive perspective. We incorporate the"
    },
    {
        "title": "Unsupervised Transductive Domain Adaptation",
        "text": " domain shift and the transductive target inference into our framework by jointly solving for an asymmetric similarity metric and the optimal transductive target label assignment. We also show that our model can easily be extended for deep feature learning"
    },
    {
        "title": "Unsupervised Transductive Domain Adaptation",
        "text": " in order to learn features which are discriminative in the target domain. Our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin"
    },
    {
        "title": "Unsupervised Transductive Domain Adaptation",
        "text": "."
    },
    {
        "title": "Learning Efficient Representation for Intrinsic Motivation",
        "text": "Mutual Information between agent Actions and environment States (MIAS) quantifies the influence of agent on its environment. Recently, it was found that the maximization of MIAS can be used as an intrinsic motivation for artificial agents. In literature, t"
    },
    {
        "title": "Learning Efficient Representation for Intrinsic Motivation",
        "text": "he term empowerment is used to represent the maximum of MIAS at a certain state. While empowerment has been shown to solve a broad range of reinforcement learning problems, its calculation in arbitrary dynamics is a challenging problem because it relies on"
    },
    {
        "title": "Learning Efficient Representation for Intrinsic Motivation",
        "text": " the estimation of mutual information. Existing approaches, which rely on sampling, are limited to low dimensional spaces, because high-confidence distribution-free lower bounds for mutual information require exponential number of samples. In this work, we"
    },
    {
        "title": "Learning Efficient Representation for Intrinsic Motivation",
        "text": " develop a novel approach for the estimation of empowerment in unknown dynamics from visual observation only, without the need to sample for MIAS. The core idea is to represent the relation between action sequences and future states using a stochastic dyna"
    },
    {
        "title": "Learning Efficient Representation for Intrinsic Motivation",
        "text": "mic model in latent space with a specific form. This allows us to efficiently compute empowerment with the \"Water-Filling\" algorithm from information theory. We construct this embedding with deep neural networks trained on a sophisticated objective functio"
    },
    {
        "title": "Learning Efficient Representation for Intrinsic Motivation",
        "text": "n. Our experimental results show that the designed embedding preserves information-theoretic properties of the original dynamics."
    },
    {
        "title": "Interactions in information spread: quantification and interpretation   using stochastic block models",
        "text": "In most real-world applications, it is seldom the case that a given observable evolves independently of its environment. In social networks, users' behavior results from the people they interact with, news in their feed, or trending topics. In natural lang"
    },
    {
        "title": "Interactions in information spread: quantification and interpretation   using stochastic block models",
        "text": "uage, the meaning of phrases emerges from the combination of words. In general medicine, a diagnosis is established on the basis of the interaction of symptoms. Here, we propose a new model, the Interactive Mixed Membership Stochastic Block Model (IMMSBM),"
    },
    {
        "title": "Interactions in information spread: quantification and interpretation   using stochastic block models",
        "text": " which investigates the role of interactions between entities (hashtags, words, memes, etc.) and quantifies their importance within the aforementioned corpora. We find that interactions play an important role in those corpora. In inference tasks, taking th"
    },
    {
        "title": "Interactions in information spread: quantification and interpretation   using stochastic block models",
        "text": "em into account leads to average relative changes with respect to non-interactive models of up to 150\\% in the probability of an outcome. Furthermore, their role greatly improves the predictive power of the model. Our findings suggest that neglecting inter"
    },
    {
        "title": "Interactions in information spread: quantification and interpretation   using stochastic block models",
        "text": "actions when modeling real-world phenomena might lead to incorrect conclusions being drawn."
    },
    {
        "title": "NeuPL: Neural Population Learning",
        "text": "Learning in strategy games (e.g. StarCraft, poker) requires the discovery of diverse policies. This is often achieved by iteratively training new policies against existing ones, growing a policy population that is robust to exploit. This iterative approach"
    },
    {
        "title": "NeuPL: Neural Population Learning",
        "text": " suffers from two issues in real-world games: a) under finite budget, approximate best-response operators at each iteration needs truncating, resulting in under-trained good-responses populating the population; b) repeated learning of basic skills at each "
    },
    {
        "title": "NeuPL: Neural Population Learning",
        "text": "iteration is wasteful and becomes intractable in the presence of increasingly strong opponents. In this work, we propose Neural Population Learning (NeuPL) as a solution to both issues. NeuPL offers convergence guarantees to a population of best-responses "
    },
    {
        "title": "NeuPL: Neural Population Learning",
        "text": "under mild assumptions. By representing a population of policies within a single conditional model, NeuPL enables transfer learning across policies. Empirically, we show the generality, improved performance and efficiency of NeuPL across several test domai"
    },
    {
        "title": "NeuPL: Neural Population Learning",
        "text": "ns. Most interestingly, we show that novel strategies become more accessible, not less, as the neural population expands."
    },
    {
        "title": "An FPGA-based Solution for Convolution Operation Acceleration",
        "text": "Hardware-based acceleration is an extensive attempt to facilitate many computationally-intensive mathematics operations. This paper proposes an FPGA-based architecture to accelerate the convolution operation - a complex and expensive computing step that ap"
    },
    {
        "title": "An FPGA-based Solution for Convolution Operation Acceleration",
        "text": "pears in many Convolutional Neural Network models. We target the design to the standard convolution operation, intending to launch the product as an edge-AI solution. The project's purpose is to produce an FPGA IP core that can process a convolutional laye"
    },
    {
        "title": "An FPGA-based Solution for Convolution Operation Acceleration",
        "text": "r at a time. System developers can deploy the IP core with various FPGA families by using Verilog HDL as the primary design language for the architecture. The experimental results show that our single computing core synthesized on a simple edge computing F"
    },
    {
        "title": "An FPGA-based Solution for Convolution Operation Acceleration",
        "text": "PGA board can offer 0.224 GOPS. When the board is fully utilized, 4.48 GOPS can be achieved."
    },
    {
        "title": "Error-feedback stochastic modeling strategy for time series forecasting   with convolutional neural networks",
        "text": "Despite the superiority of convolutional neural networks demonstrated in time series modeling and forecasting, it has not been fully explored on the design of the neural network architecture and the tuning of the hyper-parameters. Inspired by the increment"
    },
    {
        "title": "Error-feedback stochastic modeling strategy for time series forecasting   with convolutional neural networks",
        "text": "al construction strategy for building a random multilayer perceptron, we propose a novel Error-feedback Stochastic Modeling (ESM) strategy to construct a random Convolutional Neural Network (ESM-CNN) for time series forecasting task, which builds the netwo"
    },
    {
        "title": "Error-feedback stochastic modeling strategy for time series forecasting   with convolutional neural networks",
        "text": "rk architecture adaptively. The ESM strategy suggests that random filters and neurons of the error-feedback fully connected layer are incrementally added to steadily compensate the prediction error during the construction process, and then a filter selecti"
    },
    {
        "title": "Error-feedback stochastic modeling strategy for time series forecasting   with convolutional neural networks",
        "text": "on strategy is introduced to enable ESM-CNN to extract the different size of temporal features, providing helpful information at each iterative process for the prediction. The performance of ESM-CNN is justified on its prediction accuracy of one-step-ahead"
    },
    {
        "title": "Error-feedback stochastic modeling strategy for time series forecasting   with convolutional neural networks",
        "text": " and multi-step-ahead forecasting tasks respectively. Comprehensive experiments on both the synthetic and real-world datasets show that the proposed ESM-CNN not only outperforms the state-of-art random neural networks, but also exhibits stronger predictive"
    },
    {
        "title": "Error-feedback stochastic modeling strategy for time series forecasting   with convolutional neural networks",
        "text": " power and less computing overhead in comparison to trained state-of-art deep neural network models."
    },
    {
        "title": "Similarity Kernel and Clustering via Random Projection Forests",
        "text": "Similarity plays a fundamental role in many areas, including data mining, machine learning, statistics and various applied domains. Inspired by the success of ensemble methods and the flexibility of trees, we propose to learn a similarity kernel called rpf"
    },
    {
        "title": "Similarity Kernel and Clustering via Random Projection Forests",
        "text": "-kernel through random projection forests (rpForests). Our theoretical analysis reveals a highly desirable property of rpf-kernel: far-away (dissimilar) points have a low similarity value while nearby (similar) points would have a high similarity}, and the"
    },
    {
        "title": "Similarity Kernel and Clustering via Random Projection Forests",
        "text": " similarities have a native interpretation as the probability of points remaining in the same leaf nodes during the growth of rpForests. The learned rpf-kernel leads to an effective clustering algorithm--rpfCluster. On a wide variety of real and benchmark "
    },
    {
        "title": "Similarity Kernel and Clustering via Random Projection Forests",
        "text": "datasets, rpfCluster compares favorably to K-means clustering, spectral clustering and a state-of-the-art clustering ensemble algorithm--Cluster Forests. Our approach is simple to implement and readily adapt to the geometry of the underlying data. Given it"
    },
    {
        "title": "Similarity Kernel and Clustering via Random Projection Forests",
        "text": "s desirable theoretical property and competitive empirical performance when applied to clustering, we expect rpf-kernel to be applicable to many problems of an unsupervised nature or as a regularizer in some supervised or weakly supervised settings."
    },
    {
        "title": "Mapping the Landscape of Artificial Intelligence Applications against   COVID-19",
        "text": "COVID-19, the disease caused by the SARS-CoV-2 virus, has been declared a pandemic by the World Health Organization, which has reported over 18 million confirmed cases as of August 5, 2020. In this review, we present an overview of recent studies using Mac"
    },
    {
        "title": "Mapping the Landscape of Artificial Intelligence Applications against   COVID-19",
        "text": "hine Learning and, more broadly, Artificial Intelligence, to tackle many aspects of the COVID-19 crisis. We have identified applications that address challenges posed by COVID-19 at different scales, including: molecular, by identifying new or existing dru"
    },
    {
        "title": "Mapping the Landscape of Artificial Intelligence Applications against   COVID-19",
        "text": "gs for treatment; clinical, by supporting diagnosis and evaluating prognosis based on medical imaging and non-invasive measures; and societal, by tracking both the epidemic and the accompanying infodemic using multiple data sources. We also review datasets"
    },
    {
        "title": "Mapping the Landscape of Artificial Intelligence Applications against   COVID-19",
        "text": ", tools, and resources needed to facilitate Artificial Intelligence research, and discuss strategic considerations related to the operational implementation of multidisciplinary partnerships and open science. We highlight the need for international coopera"
    },
    {
        "title": "Mapping the Landscape of Artificial Intelligence Applications against   COVID-19",
        "text": "tion to maximize the potential of AI in this and future pandemics."
    },
    {
        "title": "Rerunning OCR: A Machine Learning Approach to Quality Assessment and   Enhancement Prediction",
        "text": "Iterating with new and improved OCR solutions enforces decision making when it comes to targeting the right candidates for reprocessing. This especially applies when the underlying data collection is of considerable size and rather diverse in terms of font"
    },
    {
        "title": "Rerunning OCR: A Machine Learning Approach to Quality Assessment and   Enhancement Prediction",
        "text": "s, languages, periods of publication and consequently OCR quality. This article captures the efforts of the National Library of Luxembourg to support those targeting decisions. They are crucial in order to guarantee low computational overhead and reduced q"
    },
    {
        "title": "Rerunning OCR: A Machine Learning Approach to Quality Assessment and   Enhancement Prediction",
        "text": "uality degradation risks, combined with a more quantifiable OCR improvement. In particular, this work explains the methodology of the library with respect to text block level quality assessment. Through extension of this technique, a regression model, that"
    },
    {
        "title": "Rerunning OCR: A Machine Learning Approach to Quality Assessment and   Enhancement Prediction",
        "text": " is able to take into account the enhancement potential of a new OCR engine, is also presented. They both mark promising approaches, especially for cultural institutions dealing with historical data of lower quality."
    },
    {
        "title": "Bayesian estimation from few samples: community detection and related   problems",
        "text": "We propose an efficient meta-algorithm for Bayesian estimation problems that is based on low-degree polynomials, semidefinite programming, and tensor decomposition. The algorithm is inspired by recent lower bound constructions for sum-of-squares and relate"
    },
    {
        "title": "Bayesian estimation from few samples: community detection and related   problems",
        "text": "d to the method of moments. Our focus is on sample complexity bounds that are as tight as possible (up to additive lower-order terms) and often achieve statistical thresholds or conjectured computational thresholds.   Our algorithm recovers the best known "
    },
    {
        "title": "Bayesian estimation from few samples: community detection and related   problems",
        "text": "bounds for community detection in the sparse stochastic block model, a widely-studied class of estimation problems for community detection in graphs. We obtain the first recovery guarantees for the mixed-membership stochastic block model (Airoldi et el.) i"
    },
    {
        "title": "Bayesian estimation from few samples: community detection and related   problems",
        "text": "n constant average degree graphs---up to what we conjecture to be the computational threshold for this model. We show that our algorithm exhibits a sharp computational threshold for the stochastic block model with multiple communities beyond the Kesten--St"
    },
    {
        "title": "Bayesian estimation from few samples: community detection and related   problems",
        "text": "igum bound---giving evidence that this task may require exponential time.   The basic strategy of our algorithm is strikingly simple: we compute the best-possible low-degree approximation for the moments of the posterior distribution of the parameters and "
    },
    {
        "title": "Bayesian estimation from few samples: community detection and related   problems",
        "text": "use a robust tensor decomposition algorithm to recover the parameters from these approximate posterior moments."
    },
    {
        "title": "Multiclass Learning with Simplex Coding",
        "text": "In this paper we discuss a novel framework for multiclass learning, defined by a suitable coding/decoding strategy, namely the simplex coding, that allows to generalize to multiple classes a relaxation approach commonly used in binary classification. In th"
    },
    {
        "title": "Multiclass Learning with Simplex Coding",
        "text": "is framework, a relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this setting it is possible to derive the first provably consistent regularized method with training/tuning comple"
    },
    {
        "title": "Multiclass Learning with Simplex Coding",
        "text": "xity which is independent to the number of classes. Tools from convex analysis are introduced that can be used beyond the scope of this paper."
    },
    {
        "title": "T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order   Tensor",
        "text": "Recent findings indicate that over-parametrization, while crucial for successfully training deep neural networks, also introduces large amounts of redundancy. Tensor methods have the potential to efficiently parametrize over-complete representations by lev"
    },
    {
        "title": "T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order   Tensor",
        "text": "eraging this redundancy. In this paper, we propose to fully parametrize Convolutional Neural Networks (CNNs) with a single high-order, low-rank tensor. Previous works on network tensorization have focused on parametrizing individual layers (convolutional o"
    },
    {
        "title": "T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order   Tensor",
        "text": "r fully connected) only, and perform the tensorization layer-by-layer separately. In contrast, we propose to jointly capture the full structure of a neural network by parametrizing it with a single high-order tensor, the modes of which represent each of th"
    },
    {
        "title": "T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order   Tensor",
        "text": "e architectural design parameters of the network (e.g. number of convolutional blocks, depth, number of stacks, input features, etc). This parametrization allows to regularize the whole network and drastically reduce the number of parameters. Our model is "
    },
    {
        "title": "T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order   Tensor",
        "text": "end-to-end trainable and the low-rank structure imposed on the weight tensor acts as an implicit regularization. We study the case of networks with rich structure, namely Fully Convolutional Networks (FCNs), which we propose to parametrize with a single 8t"
    },
    {
        "title": "T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order   Tensor",
        "text": "h-order tensor. We show that our approach can achieve superior performance with small compression rates, and attain high compression rates with negligible drop in accuracy for the challenging task of human pose estimation."
    },
    {
        "title": "The Influence of Dropout on Membership Inference in Differentially   Private Models",
        "text": "Differentially private models seek to protect the privacy of data the model is trained on, making it an important component of model security and privacy. At the same time, data scientists and machine learning engineers seek to use uncertainty quantificati"
    },
    {
        "title": "The Influence of Dropout on Membership Inference in Differentially   Private Models",
        "text": "on methods to ensure models are as useful and actionable as possible. We explore the tension between uncertainty quantification via dropout and privacy by conducting membership inference attacks against models with and without differential privacy. We find"
    },
    {
        "title": "The Influence of Dropout on Membership Inference in Differentially   Private Models",
        "text": " that models with large dropout slightly increases a model's risk to succumbing to membership inference attacks in all cases including in differentially private models."
    },
    {
        "title": "DANE: Domain Adaptive Network Embedding",
        "text": "Recent works reveal that network embedding techniques enable many machine learning models to handle diverse downstream tasks on graph structured data. However, as previous methods usually focus on learning embeddings for a single network, they can not lear"
    },
    {
        "title": "DANE: Domain Adaptive Network Embedding",
        "text": "n representations transferable on multiple networks. Hence, it is important to design a network embedding algorithm that supports downstream model transferring on different networks, known as domain adaptation. In this paper, we propose a novel Domain Adap"
    },
    {
        "title": "DANE: Domain Adaptive Network Embedding",
        "text": "tive Network Embedding framework, which applies graph convolutional network to learn transferable embeddings. In DANE, nodes from multiple networks are encoded to vectors via a shared set of learnable parameters so that the vectors share an aligned embeddi"
    },
    {
        "title": "DANE: Domain Adaptive Network Embedding",
        "text": "ng space. The distribution of embeddings on different networks are further aligned by adversarial learning regularization. In addition, DANE's advantage in learning transferable network embedding can be guaranteed theoretically. Extensive experiments refle"
    },
    {
        "title": "DANE: Domain Adaptive Network Embedding",
        "text": "ct that the proposed framework outperforms other state-of-the-art network embedding baselines in cross-network domain adaptation tasks."
    },
    {
        "title": "A brief history on Homomorphic learning: A privacy-focused approach to   machine learning",
        "text": "Cryptography and data science research grew exponential with the internet boom. Legacy encryption techniques force users to make a trade-off between usability, convenience, and security. Encryption makes valuable data inaccessible, as it needs to be decryp"
    },
    {
        "title": "A brief history on Homomorphic learning: A privacy-focused approach to   machine learning",
        "text": "ted each time to perform any operation. Billions of dollars could be saved, and millions of people could benefit from cryptography methods that don't compromise between usability, convenience, and security. Homomorphic encryption is one such paradigm that "
    },
    {
        "title": "A brief history on Homomorphic learning: A privacy-focused approach to   machine learning",
        "text": "allows running arbitrary operations on encrypted data. It enables us to run any sophisticated machine learning algorithm without access to the underlying raw data. Thus, homomorphic learning provides the ability to gain insights from sensitive data that ha"
    },
    {
        "title": "A brief history on Homomorphic learning: A privacy-focused approach to   machine learning",
        "text": "s been neglected due to various governmental and organization privacy rules.   In this paper, we trace back the ideas of homomorphic learning formally posed by Ronald L. Rivest and Len Alderman as \"Can we compute upon encrypted data?\" in their 1978 paper. "
    },
    {
        "title": "A brief history on Homomorphic learning: A privacy-focused approach to   machine learning",
        "text": "Then we gradually follow the ideas sprouting in the brilliant minds of Shafi Goldwasser, Kristin Lauter, Dan Bonch, Tomas Sander, Donald Beaver, and Craig Gentry to address that vital question. It took more than 30 years of collective effort to finally fin"
    },
    {
        "title": "A brief history on Homomorphic learning: A privacy-focused approach to   machine learning",
        "text": "d the answer \"yes\" to that important question."
    },
    {
        "title": "A Flexible Class of Dependence-aware Multi-Label Loss Functions",
        "text": "Multi-label classification is the task of assigning a subset of labels to a given query instance. For evaluating such predictions, the set of predicted labels needs to be compared to the ground-truth label set associated with that instance, and various los"
    },
    {
        "title": "A Flexible Class of Dependence-aware Multi-Label Loss Functions",
        "text": "s functions have been proposed for this purpose. In addition to assessing predictive accuracy, a key concern in this regard is to foster and to analyze a learner's ability to capture label dependencies. In this paper, we introduce a new class of loss funct"
    },
    {
        "title": "A Flexible Class of Dependence-aware Multi-Label Loss Functions",
        "text": "ions for multi-label classification, which overcome disadvantages of commonly used losses such as Hamming and subset 0/1. To this end, we leverage the mathematical framework of non-additive measures and integrals. Roughly speaking, a non-additive measure a"
    },
    {
        "title": "A Flexible Class of Dependence-aware Multi-Label Loss Functions",
        "text": "llows for modeling the importance of correct predictions of label subsets (instead of single labels), and thereby their impact on the overall evaluation, in a flexible way - by giving full importance to single labels and the entire label set, respectively,"
    },
    {
        "title": "A Flexible Class of Dependence-aware Multi-Label Loss Functions",
        "text": " Hamming and subset 0/1 are rather extreme in this regard. We present concrete instantiations of this class, which comprise Hamming and subset 0/1 as special cases, and which appear to be especially appealing from a modeling perspective. The assessment of "
    },
    {
        "title": "A Flexible Class of Dependence-aware Multi-Label Loss Functions",
        "text": "multi-label classifiers in terms of these losses is illustrated in an empirical study."
    },
    {
        "title": "Kinematics clustering enables head impact subtyping for better traumatic   brain injury prediction",
        "text": "Traumatic brain injury can be caused by various types of head impacts. However, due to different kinematic characteristics, many brain injury risk estimation models are not generalizable across the variety of impacts that humans may sustain. The current de"
    },
    {
        "title": "Kinematics clustering enables head impact subtyping for better traumatic   brain injury prediction",
        "text": "finitions of head impact subtypes are based on impact sources (e.g., football, traffic accident), which may not reflect the intrinsic kinematic similarities of impacts across the impact sources. To investigate the potential new definitions of impact subtyp"
    },
    {
        "title": "Kinematics clustering enables head impact subtyping for better traumatic   brain injury prediction",
        "text": "es based on kinematics, 3,161 head impacts from various sources including simulation, college football, mixed martial arts, and car racing were collected. We applied the K-means clustering to cluster the impacts on 16 standardized temporal features from he"
    },
    {
        "title": "Kinematics clustering enables head impact subtyping for better traumatic   brain injury prediction",
        "text": "ad rotation kinematics. Then, we developed subtype-specific ridge regression models for cumulative strain damage (using the threshold of 15%), which significantly improved the estimation accuracy compared with the baseline method which mixed impacts from d"
    },
    {
        "title": "Kinematics clustering enables head impact subtyping for better traumatic   brain injury prediction",
        "text": "ifferent sources and developed one model (R^2 from 0.7 to 0.9). To investigate the effect of kinematic features, we presented the top three critical features (maximum resultant angular acceleration, maximum angular acceleration along the z-axis, maximum li"
    },
    {
        "title": "Kinematics clustering enables head impact subtyping for better traumatic   brain injury prediction",
        "text": "near acceleration along the y-axis) based on regression accuracy and used logistic regression to find the critical points for each feature that partitioned the subtypes. This study enables researchers to define head impact subtypes in a data-driven manner,"
    },
    {
        "title": "Kinematics clustering enables head impact subtyping for better traumatic   brain injury prediction",
        "text": " which leads to more generalizable brain injury risk estimation."
    },
    {
        "title": "Joint Sensing Matrix and Sparsifying Dictionary Optimization for Tensor   Compressive Sensing",
        "text": "Tensor Compressive Sensing (TCS) is a multidimensional framework of Compressive Sensing (CS), and it is advantageous in terms of reducing the amount of storage, easing hardware implementations and preserving multidimensional structures of signals in compar"
    },
    {
        "title": "Joint Sensing Matrix and Sparsifying Dictionary Optimization for Tensor   Compressive Sensing",
        "text": "ison to a conventional CS system. In a TCS system, instead of using a random sensing matrix and a predefined dictionary, the average-case performance can be further improved by employing an optimized multidimensional sensing matrix and a learned multilinea"
    },
    {
        "title": "Joint Sensing Matrix and Sparsifying Dictionary Optimization for Tensor   Compressive Sensing",
        "text": "r sparsifying dictionary. In this paper, we propose a joint optimization approach of the sensing matrix and dictionary for a TCS system. For the sensing matrix design in TCS, an extended separable approach with a closed form solution and a novel iterative "
    },
    {
        "title": "Joint Sensing Matrix and Sparsifying Dictionary Optimization for Tensor   Compressive Sensing",
        "text": "non-separable method are proposed when the multilinear dictionary is fixed. In addition, a multidimensional dictionary learning method that takes advantages of the multidimensional structure is derived, and the influence of sensing matrices is taken into a"
    },
    {
        "title": "Joint Sensing Matrix and Sparsifying Dictionary Optimization for Tensor   Compressive Sensing",
        "text": "ccount in the learning process. A joint optimization is achieved via alternately iterating the optimization of the sensing matrix and dictionary. Numerical experiments using both synthetic data and real images demonstrate the superiority of the proposed ap"
    },
    {
        "title": "Joint Sensing Matrix and Sparsifying Dictionary Optimization for Tensor   Compressive Sensing",
        "text": "proaches."
    },
    {
        "title": "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction   in Structure-based Drug Discovery",
        "text": "Deep convolutional neural networks comprise a subclass of deep neural networks (DNN) with a constrained architecture that leverages the spatial and temporal structure of the domain they model. Convolutional networks achieve the best predictive performance "
    },
    {
        "title": "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction   in Structure-based Drug Discovery",
        "text": "in areas such as speech and image recognition by hierarchically composing simple local features into complex models. Although DNNs have been used in drug discovery for QSAR and ligand-based bioactivity predictions, none of these models have benefited from "
    },
    {
        "title": "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction   in Structure-based Drug Discovery",
        "text": "this powerful convolutional architecture. This paper introduces AtomNet, the first structure-based, deep convolutional neural network designed to predict the bioactivity of small molecules for drug discovery applications. We demonstrate how to apply the co"
    },
    {
        "title": "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction   in Structure-based Drug Discovery",
        "text": "nvolutional concepts of feature locality and hierarchical composition to the modeling of bioactivity and chemical interactions. In further contrast to existing DNN techniques, we show that AtomNet's application of local convolutional filters to structural "
    },
    {
        "title": "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction   in Structure-based Drug Discovery",
        "text": "target information successfully predicts new active molecules for targets with no previously known modulators. Finally, we show that AtomNet outperforms previous docking approaches on a diverse set of benchmarks by a large margin, achieving an AUC greater "
    },
    {
        "title": "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction   in Structure-based Drug Discovery",
        "text": "than 0.9 on 57.8% of the targets in the DUDE benchmark."
    },
    {
        "title": "Efficient Intrusion Detection Using Evidence Theory",
        "text": "Intrusion Detection Systems (IDS) are now an essential element when it comes to securing computers and networks. Despite the huge research efforts done in the field, handling sources' reliability remains an open issue. To address this problem, this paper p"
    },
    {
        "title": "Efficient Intrusion Detection Using Evidence Theory",
        "text": "roposes a novel contextual discounting method based on sources' reliability and their distinguishing ability between normal and abnormal behavior. Dempster-Shafer theory, a general framework for reasoning under uncertainty, is used to construct an evidenti"
    },
    {
        "title": "Efficient Intrusion Detection Using Evidence Theory",
        "text": "al classifier. The NSL-KDD dataset, a significantly revised and improved version of the existing KDDCUP'99 dataset, provides the basis for assessing the performance of our new detection approach. While giving comparable results on the KDDTest+ dataset, our"
    },
    {
        "title": "Efficient Intrusion Detection Using Evidence Theory",
        "text": " approach outperformed some other state-of-the-art methods on the KDDTest-21 dataset which is more challenging."
    },
    {
        "title": "Positive blood culture detection in time series data using a BiLSTM   network",
        "text": "The presence of bacteria or fungi in the bloodstream of patients is abnormal and can lead to life-threatening conditions. A computational model based on a bidirectional long short-term memory artificial neural network, is explored to assist doctors in the "
    },
    {
        "title": "Positive blood culture detection in time series data using a BiLSTM   network",
        "text": "intensive care unit to predict whether examination of blood cultures of patients will return positive. As input it uses nine monitored clinical parameters, presented as time series data, collected from 2177 ICU admissions at the Ghent University Hospital. "
    },
    {
        "title": "Positive blood culture detection in time series data using a BiLSTM   network",
        "text": "Our main goal is to determine if general machine learning methods and more specific, temporal models, can be used to create an early detection system. This preliminary research obtains an area of 71.95% under the precision recall curve, proving the potenti"
    },
    {
        "title": "Positive blood culture detection in time series data using a BiLSTM   network",
        "text": "al of temporal neural networks in this context."
    },
    {
        "title": "Mind the Nuisance: Gaussian Process Classification using Privileged   Noise",
        "text": "The learning with privileged information setting has recently attracted a lot of attention within the machine learning community, as it allows the integration of additional knowledge into the training process of a classifier, even when this comes in the fo"
    },
    {
        "title": "Mind the Nuisance: Gaussian Process Classification using Privileged   Noise",
        "text": "rm of a data modality that is not available at test time. Here, we show that privileged information can naturally be treated as noise in the latent function of a Gaussian Process classifier (GPC). That is, in contrast to the standard GPC setting, the laten"
    },
    {
        "title": "Mind the Nuisance: Gaussian Process Classification using Privileged   Noise",
        "text": "t function is not just a nuisance but a feature: it becomes a natural measure of confidence about the training data by modulating the slope of the GPC sigmoid likelihood function. Extensive experiments on public datasets show that the proposed GPC method u"
    },
    {
        "title": "Mind the Nuisance: Gaussian Process Classification using Privileged   Noise",
        "text": "sing privileged noise, called GPC+, improves over a standard GPC without privileged knowledge, and also over the current state-of-the-art SVM-based method, SVM+. Moreover, we show that advanced neural networks and deep learning methods can be compressed as"
    },
    {
        "title": "Mind the Nuisance: Gaussian Process Classification using Privileged   Noise",
        "text": " privileged information."
    },
    {
        "title": "Training Logistic Regression and SVM on 200GB Data Using b-Bit Minwise   Hashing and Comparisons with Vowpal Wabbit (VW)",
        "text": "We generated a dataset of 200 GB with 10^9 features, to test our recent b-bit minwise hashing algorithms for training very large-scale logistic regression and SVM. The results confirm our prior work that, compared with the VW hashing algorithm (which has t"
    },
    {
        "title": "Training Logistic Regression and SVM on 200GB Data Using b-Bit Minwise   Hashing and Comparisons with Vowpal Wabbit (VW)",
        "text": "he same variance as random projections), b-bit minwise hashing is substantially more accurate at the same storage. For example, with merely 30 hashed values per data point, b-bit minwise hashing can achieve similar accuracies as VW with 2^14 hashed values "
    },
    {
        "title": "Training Logistic Regression and SVM on 200GB Data Using b-Bit Minwise   Hashing and Comparisons with Vowpal Wabbit (VW)",
        "text": "per data point.   We demonstrate that the preprocessing cost of b-bit minwise hashing is roughly on the same order of magnitude as the data loading time. Furthermore, by using a GPU, the preprocessing cost can be reduced to a small fraction of the data loa"
    },
    {
        "title": "Training Logistic Regression and SVM on 200GB Data Using b-Bit Minwise   Hashing and Comparisons with Vowpal Wabbit (VW)",
        "text": "ding time.   Minwise hashing has been widely used in industry, at least in the context of search. One reason for its popularity is that one can efficiently simulate permutations by (e.g.,) universal hashing. In other words, there is no need to store the pe"
    },
    {
        "title": "Training Logistic Regression and SVM on 200GB Data Using b-Bit Minwise   Hashing and Comparisons with Vowpal Wabbit (VW)",
        "text": "rmutation matrix. In this paper, we empirically verify this practice, by demonstrating that even using the simplest 2-universal hashing does not degrade the learning performance."
    },
    {
        "title": "FINN: A Framework for Fast, Scalable Binarized Neural Network Inference",
        "text": "Research has shown that convolutional neural networks contain significant redundancy, and high classification accuracy can be obtained even when weights and activations are reduced from floating point to binary values. In this paper, we present FINN, a fra"
    },
    {
        "title": "FINN: A Framework for Fast, Scalable Binarized Neural Network Inference",
        "text": "mework for building fast and flexible FPGA accelerators using a flexible heterogeneous streaming architecture. By utilizing a novel set of optimizations that enable efficient mapping of binarized neural networks to hardware, we implement fully connected, c"
    },
    {
        "title": "FINN: A Framework for Fast, Scalable Binarized Neural Network Inference",
        "text": "onvolutional and pooling layers, with per-layer compute resources being tailored to user-provided throughput requirements. On a ZC706 embedded FPGA platform drawing less than 25 W total system power, we demonstrate up to 12.3 million image classifications "
    },
    {
        "title": "FINN: A Framework for Fast, Scalable Binarized Neural Network Inference",
        "text": "per second with 0.31 {\\mu}s latency on the MNIST dataset with 95.8% accuracy, and 21906 image classifications per second with 283 {\\mu}s latency on the CIFAR-10 and SVHN datasets with respectively 80.1% and 94.9% accuracy. To the best of our knowledge, our"
    },
    {
        "title": "FINN: A Framework for Fast, Scalable Binarized Neural Network Inference",
        "text": "s are the fastest classification rates reported to date on these benchmarks."
    },
    {
        "title": "Assessing the Impact: Does an Improvement to a Revenue Management System   Lead to an Improved Revenue?",
        "text": "Airlines and other industries have been making use of sophisticated Revenue Management Systems to maximize revenue for decades. While improving the different components of these systems has been the focus of numerous studies, estimating the impact of such "
    },
    {
        "title": "Assessing the Impact: Does an Improvement to a Revenue Management System   Lead to an Improved Revenue?",
        "text": "improvements on the revenue has been overlooked in the literature despite its practical importance. Indeed, quantifying the benefit of a change in a system serves as support for investment decisions. This is a challenging problem as it corresponds to the d"
    },
    {
        "title": "Assessing the Impact: Does an Improvement to a Revenue Management System   Lead to an Improved Revenue?",
        "text": "ifference between the generated value and the value that would have been generated keeping the system as before. The latter is not observable. Moreover, the expected impact can be small in relative value. In this paper, we cast the problem as counterfactua"
    },
    {
        "title": "Assessing the Impact: Does an Improvement to a Revenue Management System   Lead to an Improved Revenue?",
        "text": "l prediction of unobserved revenue. The impact on revenue is then the difference between the observed and the estimated revenue. The originality of this work lies in the innovative application of econometric methods proposed for macroeconomic applications "
    },
    {
        "title": "Assessing the Impact: Does an Improvement to a Revenue Management System   Lead to an Improved Revenue?",
        "text": "to a new problem setting. Broadly applicable, the approach benefits from only requiring revenue data observed for origin-destination pairs in the network of the airline at each day, before and after a change in the system is applied. We report results usin"
    },
    {
        "title": "Assessing the Impact: Does an Improvement to a Revenue Management System   Lead to an Improved Revenue?",
        "text": "g real large-scale data from Air Canada. We compare a deep neural network counterfactual predictions model with econometric models. They achieve respectively 1% and 1.1% of error on the counterfactual revenue predictions, and allow to accurately estimate s"
    },
    {
        "title": "Assessing the Impact: Does an Improvement to a Revenue Management System   Lead to an Improved Revenue?",
        "text": "mall impacts (in the order of 2%)."
    },
    {
        "title": "Near-optimal control of dynamical systems with neural ordinary differential equations",
        "text": "Optimal control problems naturally arise in many scientific applications where one wishes to steer a dynamical system from a certain initial state $\\mathbf{x}_0$ to a desired target state $\\mathbf{x}^*$ in finite time $T$. Recent advances in deep learning "
    },
    {
        "title": "Near-optimal control of dynamical systems with neural ordinary differential equations",
        "text": "and neural network-based optimization have contributed to the development of methods that can help solve control problems involving high-dimensional dynamical systems. In particular, the framework of neural ordinary differential equations (neural ODEs) pro"
    },
    {
        "title": "Near-optimal control of dynamical systems with neural ordinary differential equations",
        "text": "vides an efficient means to iteratively approximate continuous time control functions associated with analytically intractable and computationally demanding control tasks. Although neural ODE controllers have shown great potential in solving complex contro"
    },
    {
        "title": "Near-optimal control of dynamical systems with neural ordinary differential equations",
        "text": "l problems, the understanding of the effects of hyperparameters such as network structure and optimizers on learning performance is still very limited. Our work aims at addressing some of these knowledge gaps to conduct efficient hyperparameter optimizatio"
    },
    {
        "title": "Near-optimal control of dynamical systems with neural ordinary differential equations",
        "text": "n. To this end, we first analyze how truncated and non-truncated backpropagation through time affect runtime performance and the ability of neural networks to learn optimal control functions. Using analytical and numerical methods, we then study the role o"
    },
    {
        "title": "Near-optimal control of dynamical systems with neural ordinary differential equations",
        "text": "f parameter initializations, optimizers, and neural-network architecture. Finally, we connect our results to the ability of neural ODE controllers to implicitly regularize control energy."
    },
    {
        "title": "Pruning has a disparate impact on model accuracy",
        "text": "Network pruning is a widely-used compression technique that is able to significantly scale down overparameterized models with minimal loss of accuracy. This paper shows that pruning may create or exacerbate disparate impacts. The paper sheds light on the f"
    },
    {
        "title": "Pruning has a disparate impact on model accuracy",
        "text": "actors to cause such disparities, suggesting differences in gradient norms and distance to decision boundary across groups to be responsible for this critical issue. It analyzes these factors in detail, providing both theoretical and empirical support, and"
    },
    {
        "title": "Pruning has a disparate impact on model accuracy",
        "text": " proposes a simple, yet effective, solution that mitigates the disparate impacts caused by pruning."
    },
    {
        "title": "Solving Visual Object Ambiguities when Pointing: An Unsupervised   Learning Approach",
        "text": "Whenever we are addressing a specific object or refer to a certain spatial location, we are using referential or deictic gestures usually accompanied by some verbal description. Especially pointing gestures are necessary to dissolve ambiguities in a scene "
    },
    {
        "title": "Solving Visual Object Ambiguities when Pointing: An Unsupervised   Learning Approach",
        "text": "and they are of crucial importance when verbal communication may fail due to environmental conditions or when two persons simply do not speak the same language. With the currently increasing advances of humanoid robots and their future integration in domes"
    },
    {
        "title": "Solving Visual Object Ambiguities when Pointing: An Unsupervised   Learning Approach",
        "text": "tic domains, the development of gesture interfaces complementing human-robot interaction scenarios is of substantial interest. The implementation of an intuitive gesture scenario is still challenging because both the pointing intention and the correspondin"
    },
    {
        "title": "Solving Visual Object Ambiguities when Pointing: An Unsupervised   Learning Approach",
        "text": "g object have to be correctly recognized in real-time. The demand increases when considering pointing gestures in a cluttered environment, as is the case in households. Also, humans perform pointing in many different ways and those variations have to be ca"
    },
    {
        "title": "Solving Visual Object Ambiguities when Pointing: An Unsupervised   Learning Approach",
        "text": "ptured. Research in this field often proposes a set of geometrical computations which do not scale well with the number of gestures and objects, use specific markers or a predefined set of pointing directions. In this paper, we propose an unsupervised lear"
    },
    {
        "title": "Solving Visual Object Ambiguities when Pointing: An Unsupervised   Learning Approach",
        "text": "ning approach to model the distribution of pointing gestures using a growing-when-required (GWR) network. We introduce an interaction scenario with a humanoid robot and define so-called ambiguity classes. Our implementation for the hand and object detectio"
    },
    {
        "title": "Solving Visual Object Ambiguities when Pointing: An Unsupervised   Learning Approach",
        "text": "n is independent of any markers or skeleton models, thus it can be easily reproduced. Our evaluation comparing a baseline computer vision approach with our GWR model shows that the pointing-object association is well learned even in cases of ambiguities re"
    },
    {
        "title": "Solving Visual Object Ambiguities when Pointing: An Unsupervised   Learning Approach",
        "text": "sulting from close object proximity."
    },
    {
        "title": "Counterfactual Reasoning and Learning Systems",
        "text": "This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select ch"
    },
    {
        "title": "Counterfactual Reasoning and Learning Systems",
        "text": "anges that improve both the short-term and long-term performance of such systems. This work is illustrated by experiments carried out on the ad placement system associated with the Bing search engine."
    },
    {
        "title": "Constrained Low-Rank Learning Using Least Squares-Based Regularization",
        "text": "Low-rank learning has attracted much attention recently due to its efficacy in a rich variety of real-world tasks, e.g., subspace segmentation and image categorization. Most low-rank methods are incapable of capturing low-dimensional subspace for supervise"
    },
    {
        "title": "Constrained Low-Rank Learning Using Least Squares-Based Regularization",
        "text": "d learning tasks, e.g., classification and regression. This paper aims to learn both the discriminant low-rank representation (LRR) and the robust projecting subspace in a supervised manner. To achieve this goal, we cast the problem into a constrained rank"
    },
    {
        "title": "Constrained Low-Rank Learning Using Least Squares-Based Regularization",
        "text": " minimization framework by adopting the least squares regularization. Naturally, the data label structure tends to resemble that of the corresponding low-dimensional representation, which is derived from the robust subspace projection of clean data by low-"
    },
    {
        "title": "Constrained Low-Rank Learning Using Least Squares-Based Regularization",
        "text": "rank learning. Moreover, the low-dimensional representation of original data can be paired with some informative structure by imposing an appropriate constraint, e.g., Laplacian regularizer. Therefore, we propose a novel constrained LRR method. The objecti"
    },
    {
        "title": "Constrained Low-Rank Learning Using Least Squares-Based Regularization",
        "text": "ve function is formulated as a constrained nuclear norm minimization problem, which can be solved by the inexact augmented Lagrange multiplier algorithm. Extensive experiments on image classification, human pose estimation, and robust face recovery have co"
    },
    {
        "title": "Constrained Low-Rank Learning Using Least Squares-Based Regularization",
        "text": "nfirmed the superiority of our method."
    },
    {
        "title": "Explore the Knowledge contained in Network Weights to Obtain Sparse   Neural Networks",
        "text": "Sparse neural networks are important for achieving better generalization and enhancing computation efficiency. This paper proposes a novel learning approach to obtain sparse fully connected layers in neural networks (NNs) automatically. We design a switche"
    },
    {
        "title": "Explore the Knowledge contained in Network Weights to Obtain Sparse   Neural Networks",
        "text": "r neural network (SNN) to optimize the structure of the task neural network (TNN). The SNN takes the weights of the TNN as the inputs and its outputs are used to switch the connections of TNN. In this way, the knowledge contained in the weights of TNN is e"
    },
    {
        "title": "Explore the Knowledge contained in Network Weights to Obtain Sparse   Neural Networks",
        "text": "xplored to determine the importance of each connection and the structure of TNN consequently. The SNN and TNN are learned alternately with stochastic gradient descent (SGD) optimization, targeting at a common objective. After learning, we achieve the optim"
    },
    {
        "title": "Explore the Knowledge contained in Network Weights to Obtain Sparse   Neural Networks",
        "text": "al structure and the optimal parameters of the TNN simultaneously. In order to evaluate the proposed approach, we conduct image classification experiments on various network structures and datasets. The network structures include LeNet, ResNet18, ResNet34,"
    },
    {
        "title": "Explore the Knowledge contained in Network Weights to Obtain Sparse   Neural Networks",
        "text": " VggNet16 and MobileNet. The datasets include MNIST, CIFAR10 and CIFAR100. The experimental results show that our approach can stably lead to sparse and well-performing fully connected layers in NNs."
    },
    {
        "title": "Vulnerability Under Adversarial Machine Learning: Bias or Variance?",
        "text": "Prior studies have unveiled the vulnerability of the deep neural networks in the context of adversarial machine learning, leading to great recent attention into this area. One interesting question that has yet to be fully explored is the bias-variance rela"
    },
    {
        "title": "Vulnerability Under Adversarial Machine Learning: Bias or Variance?",
        "text": "tionship of adversarial machine learning, which can potentially provide deeper insights into this behaviour. The notion of bias and variance is one of the main approaches to analyze and evaluate the generalization and reliability of a machine learning mode"
    },
    {
        "title": "Vulnerability Under Adversarial Machine Learning: Bias or Variance?",
        "text": "l. Although it has been extensively used in other machine learning models, it is not well explored in the field of deep learning and it is even less explored in the area of adversarial machine learning.   In this study, we investigate the effect of adversa"
    },
    {
        "title": "Vulnerability Under Adversarial Machine Learning: Bias or Variance?",
        "text": "rial machine learning on the bias and variance of a trained deep neural network and analyze how adversarial perturbations can affect the generalization of a network. We derive the bias-variance trade-off for both classification and regression applications "
    },
    {
        "title": "Vulnerability Under Adversarial Machine Learning: Bias or Variance?",
        "text": "based on two main loss functions: (i) mean squared error (MSE), and (ii) cross-entropy. Furthermore, we perform quantitative analysis with both simulated and real data to empirically evaluate consistency with the derived bias-variance tradeoffs. Our analys"
    },
    {
        "title": "Vulnerability Under Adversarial Machine Learning: Bias or Variance?",
        "text": "is sheds light on why the deep neural networks have poor performance under adversarial perturbation from a bias-variance point of view and how this type of perturbation would change the performance of a network. Moreover, given these new theoretical findin"
    },
    {
        "title": "Vulnerability Under Adversarial Machine Learning: Bias or Variance?",
        "text": "gs, we introduce a new adversarial machine learning algorithm with lower computational complexity than well-known adversarial machine learning strategies (e.g., PGD) while providing a high success rate in fooling deep neural networks in lower perturbation "
    },
    {
        "title": "Vulnerability Under Adversarial Machine Learning: Bias or Variance?",
        "text": "magnitudes."
    },
    {
        "title": "Challenging the Semi-Supervised VAE Framework for Text Classification",
        "text": "Semi-Supervised Variational Autoencoders (SSVAEs) are widely used models for data efficient learning. In this paper, we question the adequacy of the standard design of sequence SSVAEs for the task of text classification as we exhibit two sources of overcom"
    },
    {
        "title": "Challenging the Semi-Supervised VAE Framework for Text Classification",
        "text": "plexity for which we provide simplifications. These simplifications to SSVAEs preserve their theoretical soundness while providing a number of practical advantages in the semi-supervised setup where the result of training is a text classifier. These simpli"
    },
    {
        "title": "Challenging the Semi-Supervised VAE Framework for Text Classification",
        "text": "fications are the removal of (i) the Kullback-Liebler divergence from its objective and (ii) the fully unobserved latent variable from its probabilistic model. These changes relieve users from choosing a prior for their latent variables, make the model sma"
    },
    {
        "title": "Challenging the Semi-Supervised VAE Framework for Text Classification",
        "text": "ller and faster, and allow for a better flow of information into the latent variables. We compare the simplified versions to standard SSVAEs on 4 text classification tasks. On top of the above-mentioned simplification, experiments show a speed-up of 26%, w"
    },
    {
        "title": "Challenging the Semi-Supervised VAE Framework for Text Classification",
        "text": "hile keeping equivalent classification scores. The code to reproduce our experiments is public."
    },
    {
        "title": "LPMNet: Latent Part Modification and Generation for 3D Point Clouds",
        "text": "In this paper, we focus on latent modification and generation of 3D point cloud object models with respect to their semantic parts. Different to the existing methods which use separate networks for part generation and assembly, we propose a single end-to-e"
    },
    {
        "title": "LPMNet: Latent Part Modification and Generation for 3D Point Clouds",
        "text": "nd Autoencoder model that can handle generation and modification of both semantic parts, and global shapes. The proposed method supports part exchange between 3D point cloud models and composition by different parts to form new models by directly editing l"
    },
    {
        "title": "LPMNet: Latent Part Modification and Generation for 3D Point Clouds",
        "text": "atent representations. This holistic approach does not need part-based training to learn part representations and does not introduce any extra loss besides the standard reconstruction loss. The experiments demonstrate the robustness of the proposed method "
    },
    {
        "title": "LPMNet: Latent Part Modification and Generation for 3D Point Clouds",
        "text": "with different object categories and varying number of points. The method can generate new models by integration of generative models such as GANs and VAEs and can work with unannotated point clouds by integration of a segmentation module."
    },
    {
        "title": "Mitigating Adversarial Attacks by Distributing Different Copies to   Different Users",
        "text": "Machine learning models are vulnerable to adversarial attacks. In this paper, we consider the scenario where a model is to be distributed to many users, among which a malicious user attempts to attack another user. The malicious user probes its unique copy"
    },
    {
        "title": "Mitigating Adversarial Attacks by Distributing Different Copies to   Different Users",
        "text": " of the model to search for adversarial samples, presenting found samples to the victim's model in order to replicate the attack. By distributing different copies of the model to different users, we can mitigate such attacks wherein adversarial samples fou"
    },
    {
        "title": "Mitigating Adversarial Attacks by Distributing Different Copies to   Different Users",
        "text": "nd on one copy would not work on another copy. We propose a flexible parameter rewriting method that directly modifies the model's parameters. This method does not require training and is able to generate a large number of copies, where each copy induces d"
    },
    {
        "title": "Mitigating Adversarial Attacks by Distributing Different Copies to   Different Users",
        "text": "ifferent sets of adversarial samples. Experimentation studies show that our approach can significantly mitigate the attacks while retaining high accuracy."
    },
    {
        "title": "Spatio-Temporal Inception Graph Convolutional Networks for   Skeleton-Based Action Recognition",
        "text": "Skeleton-based human action recognition has attracted much attention with the prevalence of accessible depth sensors. Recently, graph convolutional networks (GCNs) have been widely used for this task due to their powerful capability to model graph data. Th"
    },
    {
        "title": "Spatio-Temporal Inception Graph Convolutional Networks for   Skeleton-Based Action Recognition",
        "text": "e topology of the adjacency graph is a key factor for modeling the correlations of the input skeletons. Thus, previous methods mainly focus on the design/learning of the graph topology. But once the topology is learned, only a single-scale feature and one "
    },
    {
        "title": "Spatio-Temporal Inception Graph Convolutional Networks for   Skeleton-Based Action Recognition",
        "text": "transformation exist in each layer of the networks. Many insights, such as multi-scale information and multiple sets of transformations, that have been proven to be very effective in convolutional neural networks (CNNs), have not been investigated in GCNs."
    },
    {
        "title": "Spatio-Temporal Inception Graph Convolutional Networks for   Skeleton-Based Action Recognition",
        "text": " The reason is that, due to the gap between graph-structured skeleton data and conventional image/video data, it is very challenging to embed these insights into GCNs. To overcome this gap, we reinvent the split-transform-merge strategy in GCNs for skeleto"
    },
    {
        "title": "Spatio-Temporal Inception Graph Convolutional Networks for   Skeleton-Based Action Recognition",
        "text": "n sequence processing. Specifically, we design a simple and highly modularized graph convolutional network architecture for skeleton-based action recognition. Our network is constructed by repeating a building block that aggregates multi-granularity inform"
    },
    {
        "title": "Spatio-Temporal Inception Graph Convolutional Networks for   Skeleton-Based Action Recognition",
        "text": "ation from both the spatial and temporal paths. Extensive experiments demonstrate that our network outperforms state-of-the-art methods by a significant margin with only 1/5 of the parameters and 1/10 of the FLOPs. Code is available at https://github.com/y"
    },
    {
        "title": "Spatio-Temporal Inception Graph Convolutional Networks for   Skeleton-Based Action Recognition",
        "text": "ellowtownhz/STIGCN."
    },
    {
        "title": "Learning to Grasp from a Single Demonstration",
        "text": "Learning-based approaches for robotic grasping using visual sensors typically require collecting a large size dataset, either manually labeled or by many trial and errors of a robotic manipulator in the real or simulated world. We propose a simpler learnin"
    },
    {
        "title": "Learning to Grasp from a Single Demonstration",
        "text": "g-from-demonstration approach that is able to detect the object to grasp from merely a single demonstration using a convolutional neural network we call GraspNet. In order to increase robustness and decrease the training time even further, we leverage data"
    },
    {
        "title": "Learning to Grasp from a Single Demonstration",
        "text": " from previous demonstrations to quickly fine-tune a GrapNet for each new demonstration. We present some preliminary results on a grasping experiment with the Franka Panda cobot for which we can train a GraspNet with only hundreds of train iterations."
    },
    {
        "title": "Learning on Attribute-Missing Graphs",
        "text": "Graphs with complete node attributes have been widely explored recently. While in practice, there is a graph where attributes of only partial nodes could be available and those of the others might be entirely missing. This attribute-missing graph is relate"
    },
    {
        "title": "Learning on Attribute-Missing Graphs",
        "text": "d to numerous real-world applications and there are limited studies investigating the corresponding learning problems. Existing graph learning methods including the popular GNN cannot provide satisfied learning performance since they are not specified for "
    },
    {
        "title": "Learning on Attribute-Missing Graphs",
        "text": "attribute-missing graphs. Thereby, designing a new GNN for these graphs is a burning issue to the graph learning community. In this paper, we make a shared-latent space assumption on graphs and develop a novel distribution matching based GNN called structu"
    },
    {
        "title": "Learning on Attribute-Missing Graphs",
        "text": "re-attribute transformer (SAT) for attribute-missing graphs. SAT leverages structures and attributes in a decoupled scheme and achieves the joint distribution modeling of structures and attributes by distribution matching techniques. It could not only perf"
    },
    {
        "title": "Learning on Attribute-Missing Graphs",
        "text": "orm the link prediction task but also the newly introduced node attribute completion task. Furthermore, practical measures are introduced to quantify the performance of node attribute completion. Extensive experiments on seven real-world datasets indicate "
    },
    {
        "title": "Learning on Attribute-Missing Graphs",
        "text": "SAT shows better performance than other methods on both link prediction and node attribute completion tasks. Codes and data are available online: https://github.com/xuChenSJTU/SAT-master-online"
    },
    {
        "title": "Deep Learning for Computational Chemistry",
        "text": "The rise and fall of artificial neural networks is well documented in the scientific literature of both computer science and computational chemistry. Yet almost two decades later, we are now seeing a resurgence of interest in deep learning, a machine learn"
    },
    {
        "title": "Deep Learning for Computational Chemistry",
        "text": "ing algorithm based on multilayer neural networks. Within the last few years, we have seen the transformative impact of deep learning in many domains, particularly in speech recognition and computer vision, to the extent that the majority of expert practit"
    },
    {
        "title": "Deep Learning for Computational Chemistry",
        "text": "ioners in those field are now regularly eschewing prior established models in favor of deep learning models. In this review, we provide an introductory overview into the theory of deep neural networks and their unique properties that distinguish them from "
    },
    {
        "title": "Deep Learning for Computational Chemistry",
        "text": "traditional machine learning algorithms used in cheminformatics. By providing an overview of the variety of emerging applications of deep neural networks, we highlight its ubiquity and broad applicability to a wide range of challenges in the field, includi"
    },
    {
        "title": "Deep Learning for Computational Chemistry",
        "text": "ng QSAR, virtual screening, protein structure prediction, quantum chemistry, materials design and property prediction. In reviewing the performance of deep neural networks, we observed a consistent outperformance against non-neural networks state-of-the-ar"
    },
    {
        "title": "Deep Learning for Computational Chemistry",
        "text": "t models across disparate research topics, and deep neural network based models often exceeded the \"glass ceiling\" expectations of their respective tasks. Coupled with the maturity of GPU-accelerated computing for training deep neural networks and the expo"
    },
    {
        "title": "Deep Learning for Computational Chemistry",
        "text": "nential growth of chemical data on which to train these networks on, we anticipate that deep learning algorithms will be a valuable tool for computational chemistry."
    },
    {
        "title": "A Relational Gradient Descent Algorithm For Support Vector Machine   Training",
        "text": "We consider gradient descent like algorithms for Support Vector Machine (SVM) training when the data is in relational form. The gradient of the SVM objective can not be efficiently computed by known techniques as it suffers from the ``subtraction problem''"
    },
    {
        "title": "A Relational Gradient Descent Algorithm For Support Vector Machine   Training",
        "text": ". We first show that the subtraction problem can not be surmounted by showing that computing any constant approximation of the gradient of the SVM objective function is $\\#P$-hard, even for acyclic joins. We, however, circumvent the subtraction problem by "
    },
    {
        "title": "A Relational Gradient Descent Algorithm For Support Vector Machine   Training",
        "text": "restricting our attention to stable instances, which intuitively are instances where a nearly optimal solution remains nearly optimal if the points are perturbed slightly. We give an efficient algorithm that computes a ``pseudo-gradient'' that guarantees c"
    },
    {
        "title": "A Relational Gradient Descent Algorithm For Support Vector Machine   Training",
        "text": "onvergence for stable instances at a rate comparable to that achieved by using the actual gradient. We believe that our results suggest that this sort of stability the analysis would likely yield useful insight in the context of designing algorithms on rel"
    },
    {
        "title": "A Relational Gradient Descent Algorithm For Support Vector Machine   Training",
        "text": "ational data for other learning problems in which the subtraction problem arises."
    },
    {
        "title": "From Caesar Cipher to Unsupervised Learning: A New Method for Classifier   Parameter Estimation",
        "text": "Many important classification problems, such as object classification, speech recognition, and machine translation, have been tackled by the supervised learning paradigm in the past, where training corpora of parallel input-output pairs are required with h"
    },
    {
        "title": "From Caesar Cipher to Unsupervised Learning: A New Method for Classifier   Parameter Estimation",
        "text": "igh cost. To remove the need for the parallel training corpora has practical significance for real-world applications, and it is one of the main goals of unsupervised learning. Recently, encouraging progress in unsupervised learning for solving such classi"
    },
    {
        "title": "From Caesar Cipher to Unsupervised Learning: A New Method for Classifier   Parameter Estimation",
        "text": "fication problems has been made and the nature of the challenges has been clarified. In this article, we review this progress and disseminate a class of promising new methods to facilitate understanding the methods for machine learning researchers. In part"
    },
    {
        "title": "From Caesar Cipher to Unsupervised Learning: A New Method for Classifier   Parameter Estimation",
        "text": "icular, we emphasize the key information that enables the success of unsupervised learning - the sequential statistics as the distributional prior in the labels. Exploitation of such sequential statistics makes it possible to estimate parameters of classif"
    },
    {
        "title": "From Caesar Cipher to Unsupervised Learning: A New Method for Classifier   Parameter Estimation",
        "text": "iers without the need of paired input-output data.   In this paper, we first introduce the concept of Caesar Cipher and its decryption, which motivated the construction of the novel loss function for unsupervised learning we use throughout the paper. Then "
    },
    {
        "title": "From Caesar Cipher to Unsupervised Learning: A New Method for Classifier   Parameter Estimation",
        "text": "we use a simple but representative binary classification task as an example to derive and describe the unsupervised learning algorithm in a step-by-step, easy-to-understand fashion. We include two cases, one with Bigram language model as the sequential sta"
    },
    {
        "title": "From Caesar Cipher to Unsupervised Learning: A New Method for Classifier   Parameter Estimation",
        "text": "tistics for use in unsupervised parameter estimation, and another with a simpler Unigram language model. For both cases, detailed derivation steps for the learning algorithm are included. Further, a summary table compares computational steps of the two cas"
    },
    {
        "title": "From Caesar Cipher to Unsupervised Learning: A New Method for Classifier   Parameter Estimation",
        "text": "es in executing the unsupervised learning algorithm for learning binary classifiers."
    },
    {
        "title": "Signed Laplacian Deep Learning with Adversarial Augmentation for   Improved Mammography Diagnosis",
        "text": "Computer-aided breast cancer diagnosis in mammography is limited by inadequate data and the similarity between benign and cancerous masses. To address this, we propose a signed graph regularized deep neural network with adversarial augmentation, named \\tex"
    },
    {
        "title": "Signed Laplacian Deep Learning with Adversarial Augmentation for   Improved Mammography Diagnosis",
        "text": "tsc{DiagNet}. Firstly, we use adversarial learning to generate positive and negative mass-contained mammograms for each mass class. After that, a signed similarity graph is built upon the expanded data to further highlight the discrimination. Finally, a de"
    },
    {
        "title": "Signed Laplacian Deep Learning with Adversarial Augmentation for   Improved Mammography Diagnosis",
        "text": "ep convolutional neural network is trained by jointly optimizing the signed graph regularization and classification loss. Experiments show that the \\textsc{DiagNet} framework outperforms the state-of-the-art in breast mass diagnosis in mammography."
    },
    {
        "title": "Learn Quasi-stationary Distributions of Finite State Markov Chain",
        "text": "We propose a reinforcement learning (RL) approach to compute the expression of quasi-stationary distribution. Based on the fixed-point formulation of quasi-stationary distribution, we minimize the KL-divergence of two Markovian path distributions induced b"
    },
    {
        "title": "Learn Quasi-stationary Distributions of Finite State Markov Chain",
        "text": "y the candidate distribution and the true target distribution. To solve this challenging minimization problem by gradient descent, we apply the reinforcement learning technique by introducing the reward and value functions. We derive the corresponding poli"
    },
    {
        "title": "Learn Quasi-stationary Distributions of Finite State Markov Chain",
        "text": "cy gradient theorem and design an actor-critic algorithm to learn the optimal solution and the value function. The numerical examples of finite state Markov chain are tested to demonstrate the new method."
    },
    {
        "title": "Principled Deep Neural Network Training through Linear Programming",
        "text": "Deep learning has received much attention lately due to the impressive empirical performance achieved by training algorithms. Consequently, a need for a better theoretical understanding of these problems has become more evident in recent years. In this wor"
    },
    {
        "title": "Principled Deep Neural Network Training through Linear Programming",
        "text": "k, using a unified framework, we show that there exists a polyhedron which encodes simultaneously all possible deep neural network training problems that can arise from a given architecture, activation functions, loss function, and sample-size. Notably, th"
    },
    {
        "title": "Principled Deep Neural Network Training through Linear Programming",
        "text": "e size of the polyhedral representation depends only linearly on the sample-size, and a better dependency on several other network parameters is unlikely (assuming $P\\neq NP$). Additionally, we use our polyhedral representation to obtain new and better com"
    },
    {
        "title": "Principled Deep Neural Network Training through Linear Programming",
        "text": "putational complexity results for training problems of well-known neural network architectures. Our results provide a new perspective on training problems through the lens of polyhedral theory and reveal a strong structure arising from these problems."
    },
    {
        "title": "Nearly Optimal Linear Convergence of Stochastic Primal-Dual Methods for   Linear Programming",
        "text": "There is a recent interest on first-order methods for linear programming (LP). In this paper,we propose a stochastic algorithm using variance reduction and restarts for solving sharp primal-dual problems such as LP. We show that the proposed stochastic met"
    },
    {
        "title": "Nearly Optimal Linear Convergence of Stochastic Primal-Dual Methods for   Linear Programming",
        "text": "hod exhibits a linear convergence rate for solving sharp instances with a high probability. In addition, we propose an efficient coordinate-based stochastic oracle for unconstrained bilinear problems, which has $\\mathcal O(1)$ per iteration cost and improv"
    },
    {
        "title": "Nearly Optimal Linear Convergence of Stochastic Primal-Dual Methods for   Linear Programming",
        "text": "es the complexity of the existing deterministic and stochastic algorithms. Finally, we show that the obtained linear convergence rate is nearly optimal (upto $\\log$ terms) for a wide class of stochastic primal dual methods."
    },
    {
        "title": "GANs for Medical Image Analysis",
        "text": "Generative Adversarial Networks (GANs) and their extensions have carved open many exciting ways to tackle well known and challenging medical image analysis problems such as medical image de-noising, reconstruction, segmentation, data simulation, detection "
    },
    {
        "title": "GANs for Medical Image Analysis",
        "text": "or classification. Furthermore, their ability to synthesize images at unprecedented levels of realism also gives hope that the chronic scarcity of labeled data in the medical field can be resolved with the help of these generative models. In this review pa"
    },
    {
        "title": "GANs for Medical Image Analysis",
        "text": "per, a broad overview of recent literature on GANs for medical applications is given, the shortcomings and opportunities of the proposed methods are thoroughly discussed and potential future work is elaborated. We review the most relevant papers published "
    },
    {
        "title": "GANs for Medical Image Analysis",
        "text": "until the submission date. For quick access, important details such as the underlying method, datasets and performance are tabulated. An interactive visualization which categorizes all papers to keep the review alive, is available at http://livingreview.in"
    },
    {
        "title": "GANs for Medical Image Analysis",
        "text": ".tum.de/GANs_for_Medical_Applications."
    },
    {
        "title": "Hamiltonian neural networks for solving equations of motion",
        "text": "There has been a wave of interest in applying machine learning to study dynamical systems. We present a Hamiltonian neural network that solves the differential equations that govern dynamical systems. This is an equation-driven machine learning method wher"
    },
    {
        "title": "Hamiltonian neural networks for solving equations of motion",
        "text": "e the optimization process of the network depends solely on the predicted functions without using any ground truth data. The model learns solutions that satisfy, up to an arbitrarily small error, Hamilton's equations and, therefore, conserve the Hamiltonia"
    },
    {
        "title": "Hamiltonian neural networks for solving equations of motion",
        "text": "n invariants. The choice of an appropriate activation function drastically improves the predictability of the network. Moreover, an error analysis is derived and states that the numerical errors depend on the overall network performance. The Hamiltonian ne"
    },
    {
        "title": "Hamiltonian neural networks for solving equations of motion",
        "text": "twork is then employed to solve the equations for the nonlinear oscillator and the chaotic Henon-Heiles dynamical system. In both systems, a symplectic Euler integrator requires two orders more evaluation points than the Hamiltonian network in order to ach"
    },
    {
        "title": "Hamiltonian neural networks for solving equations of motion",
        "text": "ieve the same order of the numerical error in the predicted phase space trajectories."
    },
    {
        "title": "Analyzing Human-Human Interactions: A Survey",
        "text": "Many videos depict people, and it is their interactions that inform us of their activities, relation to one another and the cultural and social setting. With advances in human action recognition, researchers have begun to address the automated recognition "
    },
    {
        "title": "Analyzing Human-Human Interactions: A Survey",
        "text": "of these human-human interactions from video. The main challenges stem from dealing with the considerable variation in recording setting, the appearance of the people depicted and the coordinated performance of their interaction. This survey provides a sum"
    },
    {
        "title": "Analyzing Human-Human Interactions: A Survey",
        "text": "mary of these challenges and datasets to address these, followed by an in-depth discussion of relevant vision-based recognition and detection methods. We focus on recent, promising work based on deep learning and convolutional neural networks (CNNs). Final"
    },
    {
        "title": "Analyzing Human-Human Interactions: A Survey",
        "text": "ly, we outline directions to overcome the limitations of the current state-of-the-art to analyze and, eventually, understand social human actions."
    },
    {
        "title": "Contrastive Learning with Hard Negative Samples",
        "text": "How can you sample good negative examples for contrastive learning? We argue that, as with metric learning, contrastive learning of representations benefits from hard negative samples (i.e., points that are difficult to distinguish from an anchor point). T"
    },
    {
        "title": "Contrastive Learning with Hard Negative Samples",
        "text": "he key challenge toward using hard negatives is that contrastive methods must remain unsupervised, making it infeasible to adopt existing negative sampling strategies that use true similarity information. In response, we develop a new family of unsupervise"
    },
    {
        "title": "Contrastive Learning with Hard Negative Samples",
        "text": "d sampling methods for selecting hard negative samples where the user can control the hardness. A limiting case of this sampling results in a representation that tightly clusters each class, and pushes different classes as far apart as possible. The propos"
    },
    {
        "title": "Contrastive Learning with Hard Negative Samples",
        "text": "ed method improves downstream performance across multiple modalities, requires only few additional lines of code to implement, and introduces no computational overhead."
    },
    {
        "title": "Enhancing Unsupervised Anomaly Detection with Score-Guided Network",
        "text": "Anomaly detection plays a crucial role in various real-world applications, including healthcare and finance systems. Owing to the limited number of anomaly labels in these complex systems, unsupervised anomaly detection methods have attracted great attenti"
    },
    {
        "title": "Enhancing Unsupervised Anomaly Detection with Score-Guided Network",
        "text": "on in recent years. Two major challenges faced by the existing unsupervised methods are: (i) distinguishing between normal and abnormal data in the transition field, where normal and abnormal data are highly mixed together; (ii) defining an effective metri"
    },
    {
        "title": "Enhancing Unsupervised Anomaly Detection with Score-Guided Network",
        "text": "c to maximize the gap between normal and abnormal data in a hypothesis space, which is built by a representation learner. To that end, this work proposes a novel scoring network with a score-guided regularization to learn and enlarge the anomaly score disp"
    },
    {
        "title": "Enhancing Unsupervised Anomaly Detection with Score-Guided Network",
        "text": "arities between normal and abnormal data. With such score-guided strategy, the representation learner can gradually learn more informative representation during the model training stage, especially for the samples in the transition field. We next propose a"
    },
    {
        "title": "Enhancing Unsupervised Anomaly Detection with Score-Guided Network",
        "text": " score-guided autoencoder (SG-AE), incorporating the scoring network into an autoencoder framework for anomaly detection, as well as other three state-of-the-art models, to further demonstrate the effectiveness and transferability of the design. Extensive "
    },
    {
        "title": "Enhancing Unsupervised Anomaly Detection with Score-Guided Network",
        "text": "experiments on both synthetic and real-world datasets demonstrate the state-of-the-art performance of these score-guided models (SGMs)."
    },
    {
        "title": "Auditing and Achieving Intersectional Fairness in Classification   Problems",
        "text": "Machine learning algorithms are extensively used to make increasingly more consequential decisions about people, so achieving optimal predictive performance can no longer be the only focus. A particularly important consideration is fairness with respect to"
    },
    {
        "title": "Auditing and Achieving Intersectional Fairness in Classification   Problems",
        "text": " race, gender, or any other sensitive attribute. This paper studies intersectional fairness, where intersections of multiple sensitive attributes are considered. Prior research has mainly focused on fairness with respect to a single sensitive attribute, wi"
    },
    {
        "title": "Auditing and Achieving Intersectional Fairness in Classification   Problems",
        "text": "th intersectional fairness being comparatively less studied despite its critical importance for the safety of modern machine learning systems. We present a comprehensive framework for auditing and achieving intersectional fairness in classification problem"
    },
    {
        "title": "Auditing and Achieving Intersectional Fairness in Classification   Problems",
        "text": "s: we define a suite of metrics to assess intersectional fairness in the data or model outputs by extending known single-attribute fairness metrics, and propose methods for robustly estimating them even when some intersectional subgroups are underrepresent"
    },
    {
        "title": "Auditing and Achieving Intersectional Fairness in Classification   Problems",
        "text": "ed. Furthermore, we develop post-processing techniques to mitigate any detected intersectional bias in a classification model. Our techniques do not rely on any assumptions regarding the underlying model and preserve predictive performance at a guaranteed "
    },
    {
        "title": "Auditing and Achieving Intersectional Fairness in Classification   Problems",
        "text": "level of fairness. Finally, we give guidance on a practical implementation, showing how the proposed methods perform on a real-world dataset."
    },
    {
        "title": "Active online learning in the binary perceptron problem",
        "text": "The binary perceptron is the simplest artificial neural network formed by $N$ input units and one output unit, with the neural states and the synaptic weights all restricted to $\\pm 1$ values. The task in the teacher--student scenario is to infer the hidde"
    },
    {
        "title": "Active online learning in the binary perceptron problem",
        "text": "n weight vector by training on a set of labeled patterns. Previous efforts on the passive learning mode have shown that learning from independent random patterns is quite inefficient. Here we consider the active online learning mode in which the student de"
    },
    {
        "title": "Active online learning in the binary perceptron problem",
        "text": "signs every new Ising training pattern. We demonstrate that it is mathematically possible to achieve perfect (error-free) inference using only $N$ designed training patterns, but this is computationally unfeasible for large systems. We then investigate two"
    },
    {
        "title": "Active online learning in the binary perceptron problem",
        "text": " Bayesian statistical designing protocols, which require $2.3 N$ and $1.9 N$ training patterns, respectively, to achieve error-free inference. If the training patterns are instead designed through deductive reasoning, perfect inference is achieved using $N"
    },
    {
        "title": "Active online learning in the binary perceptron problem",
        "text": "\\!+\\!\\log_{2}\\!N$ samples. The performance gap between Bayesian and deductive designing strategies may be shortened in future work by taking into account the possibility of ergodicity breaking in the version space of the binary perceptron."
    },
    {
        "title": "Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data",
        "text": "Corrupted labels and class imbalance are commonly encountered in practically collected training data, which easily leads to over-fitting of deep neural networks (DNNs). Existing approaches alleviate these issues by adopting a sample re-weighting strategy, "
    },
    {
        "title": "Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data",
        "text": "which is to re-weight sample by designing weighting function. However, it is only applicable for training data containing only either one type of data biases. In practice, however, biased samples with corrupted labels and of tailed classes commonly co-exis"
    },
    {
        "title": "Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data",
        "text": "t in training data. How to handle them simultaneously is a key but under-explored problem. In this paper, we find that these two types of biased samples, though have similar transient loss, have distinguishable trend and characteristics in loss curves, whi"
    },
    {
        "title": "Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data",
        "text": "ch could provide valuable priors for sample weight assignment. Motivated by this, we delve into the loss curves and propose a novel probe-and-allocate training strategy: In the probing stage, we train the network on the whole biased training data without i"
    },
    {
        "title": "Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data",
        "text": "ntervention, and record the loss curve of each sample as an additional attribute; In the allocating stage, we feed the resulting attribute to a newly designed curve-perception network, named CurveNet, to learn to identify the bias type of each sample and a"
    },
    {
        "title": "Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data",
        "text": "ssign proper weights through meta-learning adaptively. The training speed of meta learning also blocks its application. To solve it, we propose a method named skip layer meta optimization (SLMO) to accelerate training speed by skipping the bottom layers. E"
    },
    {
        "title": "Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data",
        "text": "xtensive synthetic and real experiments well validate the proposed method, which achieves state-of-the-art performance on multiple challenging benchmarks."
    },
    {
        "title": "retina-VAE: Variationally Decoding the Spectrum of Macular Disease",
        "text": "In this paper, we seek a clinically-relevant latent code for representing the spectrum of macular disease. Towards this end, we construct retina-VAE, a variational autoencoder-based model that accepts a patient profile vector (pVec) as input. The pVec comp"
    },
    {
        "title": "retina-VAE: Variationally Decoding the Spectrum of Macular Disease",
        "text": "onents include clinical exam findings and demographic information. We evaluate the model on a subspectrum of the retinal maculopathies, in particular, exudative age-related macular degeneration, central serous chorioretinopathy, and polypoidal choroidal va"
    },
    {
        "title": "retina-VAE: Variationally Decoding the Spectrum of Macular Disease",
        "text": "sculopathy. For these three maculopathies, a database of 3000 6-dimensional pVecs (1000 each) was synthetically generated based on known disease statistics in the literature. The database was then used to train the VAE and generate latent vector representa"
    },
    {
        "title": "retina-VAE: Variationally Decoding the Spectrum of Macular Disease",
        "text": "tions. We found training performance to be best for a 3-dimensional latent vector architecture compared to 2 or 4 dimensional latents. Additionally, for the 3D latent architecture, we discovered that the resulting latent vectors were strongly clustered spo"
    },
    {
        "title": "retina-VAE: Variationally Decoding the Spectrum of Macular Disease",
        "text": "ntaneously into one of 14 clusters. Kmeans was then used only to identify members of each cluster and to inspect cluster properties. These clusters suggest underlying disease subtypes which may potentially respond better or worse to particular pharmaceutic"
    },
    {
        "title": "retina-VAE: Variationally Decoding the Spectrum of Macular Disease",
        "text": "al treatments such as anti-vascular endothelial growth factor variants. The retina-VAE framework will potentially yield new fundamental insights into the mechanisms and manifestations of disease. And will potentially facilitate the development of personali"
    },
    {
        "title": "retina-VAE: Variationally Decoding the Spectrum of Macular Disease",
        "text": "zed pharmaceuticals and gene therapies."
    },
    {
        "title": "Sparse LMS via Online Linearized Bregman Iteration",
        "text": "We propose a version of least-mean-square (LMS) algorithm for sparse system identification. Our algorithm called online linearized Bregman iteration (OLBI) is derived from minimizing the cumulative prediction error squared along with an l1-l2 norm regulari"
    },
    {
        "title": "Sparse LMS via Online Linearized Bregman Iteration",
        "text": "zer. By systematically treating the non-differentiable regularizer we arrive at a simple two-step iteration. We demonstrate that OLBI is bias free and compare its operation with existing sparse LMS algorithms by rederiving them in the online convex optimiz"
    },
    {
        "title": "Sparse LMS via Online Linearized Bregman Iteration",
        "text": "ation framework. We perform convergence analysis of OLBI for white input signals and derive theoretical expressions for both the steady state and instantaneous mean square deviations (MSD). We demonstrate numerically that OLBI improves the performance of L"
    },
    {
        "title": "Sparse LMS via Online Linearized Bregman Iteration",
        "text": "MS type algorithms for signals generated from sparse tap weights."
    },
    {
        "title": "CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot   MultiBox Detector",
        "text": "Due to the success of Bidirectional Encoder Representations from Transformers (BERT) in natural language process (NLP), the multi-head attention transformer has been more and more prevalent in computer-vision researches (CV). However, it still remains a ch"
    },
    {
        "title": "CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot   MultiBox Detector",
        "text": "allenge for researchers to put forward complex tasks such as vision detection and semantic segmentation. Although multiple Transformer-Based architectures like DETR and ViT-FRCNN have been proposed to complete object detection task, they inevitably decreas"
    },
    {
        "title": "CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot   MultiBox Detector",
        "text": "es discrimination accuracy and brings down computational efficiency caused by the enormous learning parameters and heavy computational complexity incurred by the traditional self-attention operation. In order to alleviate these issues, we present a novel o"
    },
    {
        "title": "CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot   MultiBox Detector",
        "text": "bject detection architecture, named Convolutional vision Transformer Based Attentive Single Shot MultiBox Detector (CvT-ASSD), that built on the top of Convolutional vision Transormer (CvT) with the efficient Attentive Single Shot MultiBox Detector (ASSD)."
    },
    {
        "title": "CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot   MultiBox Detector",
        "text": " We provide comprehensive empirical evidence showing that our model CvT-ASSD can leads to good system efficiency and performance while being pretrained on large-scale detection datasets such as PASCAL VOC and MS COCO. Code has been released on public githu"
    },
    {
        "title": "CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot   MultiBox Detector",
        "text": "b repository at https://github.com/albert-jin/CvT-ASSD."
    },
    {
        "title": "SMU: smooth activation function for deep networks using smoothing   maximum technique",
        "text": "Deep learning researchers have a keen interest in proposing two new novel activation functions which can boost network performance. A good choice of activation function can have significant consequences in improving network performance. A handcrafted activ"
    },
    {
        "title": "SMU: smooth activation function for deep networks using smoothing   maximum technique",
        "text": "ation is the most common choice in neural network models. ReLU is the most common choice in the deep learning community due to its simplicity though ReLU has some serious drawbacks. In this paper, we have proposed a new novel activation function based on a"
    },
    {
        "title": "SMU: smooth activation function for deep networks using smoothing   maximum technique",
        "text": "pproximation of known activation functions like Leaky ReLU, and we call this function Smooth Maximum Unit (SMU). Replacing ReLU by SMU, we have got 6.22% improvement in the CIFAR100 dataset with the ShuffleNet V2 model."
    },
    {
        "title": "Learning Long Term Dependencies via Fourier Recurrent Units",
        "text": "It is a known fact that training recurrent neural networks for tasks that have long term dependencies is challenging. One of the main reasons is the vanishing or exploding gradient problem, which prevents gradient information from propagating to early laye"
    },
    {
        "title": "Learning Long Term Dependencies via Fourier Recurrent Units",
        "text": "rs. In this paper we propose a simple recurrent architecture, the Fourier Recurrent Unit (FRU), that stabilizes the gradients that arise in its training while giving us stronger expressive power. Specifically, FRU summarizes the hidden states $h^{(t)}$ alo"
    },
    {
        "title": "Learning Long Term Dependencies via Fourier Recurrent Units",
        "text": "ng the temporal dimension with Fourier basis functions. This allows gradients to easily reach any layer due to FRU's residual learning structure and the global support of trigonometric functions. We show that FRU has gradient lower and upper bounds indepen"
    },
    {
        "title": "Learning Long Term Dependencies via Fourier Recurrent Units",
        "text": "dent of temporal dimension. We also show the strong expressivity of sparse Fourier basis, from which FRU obtains its strong expressive power. Our experimental study also demonstrates that with fewer parameters the proposed architecture outperforms other re"
    },
    {
        "title": "Learning Long Term Dependencies via Fourier Recurrent Units",
        "text": "current architectures on many tasks."
    },
    {
        "title": "Text to Image Generation with Semantic-Spatial Aware GAN",
        "text": "Text-to-image synthesis (T2I) aims to generate photo-realistic images which are semantically consistent with the text descriptions. Existing methods are usually built upon conditional generative adversarial networks (GANs) and initialize an image from nois"
    },
    {
        "title": "Text to Image Generation with Semantic-Spatial Aware GAN",
        "text": "e with sentence embedding, and then refine the features with fine-grained word embedding iteratively. A close inspection of their generated images reveals a major limitation: even though the generated image holistically matches the description, individual "
    },
    {
        "title": "Text to Image Generation with Semantic-Spatial Aware GAN",
        "text": "image regions or parts of somethings are often not recognizable or consistent with words in the sentence, e.g. \"a white crown\". To address this problem, we propose a novel framework Semantic-Spatial Aware GAN for synthesizing images from input text. Concre"
    },
    {
        "title": "Text to Image Generation with Semantic-Spatial Aware GAN",
        "text": "tely, we introduce a simple and effective Semantic-Spatial Aware block, which (1) learns semantic-adaptive transformation conditioned on text to effectively fuse text features and image features, and (2) learns a semantic mask in a weakly-supervised way th"
    },
    {
        "title": "Text to Image Generation with Semantic-Spatial Aware GAN",
        "text": "at depends on the current text-image fusion process in order to guide the transformation spatially. Experiments on the challenging COCO and CUB bird datasets demonstrate the advantage of our method over the recent state-of-the-art approaches, regarding bot"
    },
    {
        "title": "Text to Image Generation with Semantic-Spatial Aware GAN",
        "text": "h visual fidelity and alignment with input text description."
    },
    {
        "title": "Regularization by Misclassification in ReLU Neural Networks",
        "text": "We study the implicit bias of ReLU neural networks trained by a variant of SGD where at each step, the label is changed with probability $p$ to a random label (label smoothing being a close variant of this procedure). Our experiments demonstrate that label"
    },
    {
        "title": "Regularization by Misclassification in ReLU Neural Networks",
        "text": " noise propels the network to a sparse solution in the following sense: for a typical input, a small fraction of neurons are active, and the firing pattern of the hidden layers is sparser. In fact, for some instances, an appropriate amount of label noise d"
    },
    {
        "title": "Regularization by Misclassification in ReLU Neural Networks",
        "text": "oes not only sparsify the network but further reduces the test error. We then turn to the theoretical analysis of such sparsification mechanisms, focusing on the extremal case of $p=1$. We show that in this case, the network withers as anticipated from exp"
    },
    {
        "title": "Regularization by Misclassification in ReLU Neural Networks",
        "text": "eriments, but surprisingly, in different ways that depend on the learning rate and the presence of bias, with either weights vanishing or neurons ceasing to fire."
    },
    {
        "title": "Conjugate-Computation Variational Inference : Converting Variational   Inference in Non-Conjugate Models to Inferences in Conjugate Models",
        "text": "Variational inference is computationally challenging in models that contain both conjugate and non-conjugate terms. Methods specifically designed for conjugate models, even though computationally efficient, find it difficult to deal with non-conjugate term"
    },
    {
        "title": "Conjugate-Computation Variational Inference : Converting Variational   Inference in Non-Conjugate Models to Inferences in Conjugate Models",
        "text": "s. On the other hand, stochastic-gradient methods can handle the non-conjugate terms but they usually ignore the conjugate structure of the model which might result in slow convergence. In this paper, we propose a new algorithm called Conjugate-computation"
    },
    {
        "title": "Conjugate-Computation Variational Inference : Converting Variational   Inference in Non-Conjugate Models to Inferences in Conjugate Models",
        "text": " Variational Inference (CVI) which brings the best of the two worlds together -- it uses conjugate computations for the conjugate terms and employs stochastic gradients for the rest. We derive this algorithm by using a stochastic mirror-descent method in t"
    },
    {
        "title": "Conjugate-Computation Variational Inference : Converting Variational   Inference in Non-Conjugate Models to Inferences in Conjugate Models",
        "text": "he mean-parameter space, and then expressing each gradient step as a variational inference in a conjugate model. We demonstrate our algorithm's applicability to a large class of models and establish its convergence. Our experimental results show that our m"
    },
    {
        "title": "Conjugate-Computation Variational Inference : Converting Variational   Inference in Non-Conjugate Models to Inferences in Conjugate Models",
        "text": "ethod converges much faster than the methods that ignore the conjugate structure of the model."
    },
    {
        "title": "Towards Compact ConvNets via Structure-Sparsity Regularized Filter   Pruning",
        "text": "The success of convolutional neural networks (CNNs) in computer vision applications has been accompanied by a significant increase of computation and memory costs, which prohibits its usage on resource-limited environments such as mobile or embedded device"
    },
    {
        "title": "Towards Compact ConvNets via Structure-Sparsity Regularized Filter   Pruning",
        "text": "s. To this end, the research of CNN compression has recently become emerging. In this paper, we propose a novel filter pruning scheme, termed structured sparsity regularization (SSR), to simultaneously speedup the computation and reduce the memory overhead"
    },
    {
        "title": "Towards Compact ConvNets via Structure-Sparsity Regularized Filter   Pruning",
        "text": " of CNNs, which can be well supported by various off-the-shelf deep learning libraries. Concretely, the proposed scheme incorporates two different regularizers of structured sparsity into the original objective function of filter pruning, which fully coord"
    },
    {
        "title": "Towards Compact ConvNets via Structure-Sparsity Regularized Filter   Pruning",
        "text": "inates the global outputs and local pruning operations to adaptively prune filters. We further propose an Alternative Updating with Lagrange Multipliers (AULM) scheme to efficiently solve its optimization. AULM follows the principle of ADMM and alternates "
    },
    {
        "title": "Towards Compact ConvNets via Structure-Sparsity Regularized Filter   Pruning",
        "text": "between promoting the structured sparsity of CNNs and optimizing the recognition loss, which leads to a very efficient solver (2.5x to the most recent work that directly solves the group sparsity-based regularization). Moreover, by imposing the structured "
    },
    {
        "title": "Towards Compact ConvNets via Structure-Sparsity Regularized Filter   Pruning",
        "text": "sparsity, the online inference is extremely memory-light, since the number of filters and the output feature maps are simultaneously reduced. The proposed scheme has been deployed to a variety of state-of-the-art CNN structures including LeNet, AlexNet, VG"
    },
    {
        "title": "Towards Compact ConvNets via Structure-Sparsity Regularized Filter   Pruning",
        "text": "G, ResNet and GoogLeNet over different datasets. Quantitative results demonstrate that the proposed scheme achieves superior performance over the state-of-the-art methods. We further demonstrate the proposed compression scheme for the task of transfer lear"
    },
    {
        "title": "Towards Compact ConvNets via Structure-Sparsity Regularized Filter   Pruning",
        "text": "ning, including domain adaptation and object detection, which also show exciting performance gains over the state-of-the-arts."
    },
    {
        "title": "Reinforcement Learning with Competitive Ensembles of   Information-Constrained Primitives",
        "text": "Reinforcement learning agents that operate in diverse and complex environments can benefit from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a "
    },
    {
        "title": "Reinforcement Learning with Competitive Ensembles of   Information-Constrained Primitives",
        "text": "policy into lower-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. However, the meta-policy must still produce appropriate decisions in all states. In this work, we propose a policy "
    },
    {
        "title": "Reinforcement Learning with Competitive Ensembles of   Information-Constrained Primitives",
        "text": "design that decomposes into primitives, similarly to hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide for themselves whether they wish to act in the current state. We use an information-theoretic"
    },
    {
        "title": "Reinforcement Learning with Competitive Ensembles of   Information-Constrained Primitives",
        "text": " mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision and the primitive that requests the most information about the current state acts in the world. The primiti"
    },
    {
        "title": "Reinforcement Learning with Competitive Ensembles of   Information-Constrained Primitives",
        "text": "ves are regularized to use as little information as possible, which leads to natural competition and specialization. We experimentally demonstrate that this policy architecture improves over both flat and hierarchical policies in terms of generalization."
    },
    {
        "title": "Continuous-variable neural-network quantum states and the quantum rotor   model",
        "text": "We initiate the study of neural-network quantum state algorithms for analyzing continuous-variable lattice quantum systems in first quantization. A simple family of continuous-variable trial wavefunctons is introduced which naturally generalizes the restri"
    },
    {
        "title": "Continuous-variable neural-network quantum states and the quantum rotor   model",
        "text": "cted Boltzmann machine (RBM) wavefunction introduced for analyzing quantum spin systems. By virtue of its simplicity, the same variational Monte Carlo training algorithms that have been developed for ground state determination and time evolution of spin sy"
    },
    {
        "title": "Continuous-variable neural-network quantum states and the quantum rotor   model",
        "text": "stems have natural analogues in the continuum. We offer a proof of principle demonstration in the context of ground state determination of a stoquastic quantum rotor Hamiltonian. Results are compared against those obtained from partial differential equatio"
    },
    {
        "title": "Continuous-variable neural-network quantum states and the quantum rotor   model",
        "text": "n (PDE) based scalable eigensolvers. This study serves as a benchmark against which future investigation of continuous-variable neural quantum states can be compared, and points to the need to consider deep network architectures and more sophisticated trai"
    },
    {
        "title": "Continuous-variable neural-network quantum states and the quantum rotor   model",
        "text": "ning algorithms."
    },
    {
        "title": "A Fast Algorithm for PAC Combinatorial Pure Exploration",
        "text": "We consider the problem of Combinatorial Pure Exploration (CPE), which deals with finding a combinatorial set or arms with a high reward, when the rewards of individual arms are unknown in advance and must be estimated using arm pulls. Previous algorithms "
    },
    {
        "title": "A Fast Algorithm for PAC Combinatorial Pure Exploration",
        "text": "for this problem, while obtaining sample complexity reductions in many cases, are highly computationally intensive, thus making them impractical even for mildly large problems. In this work, we propose a new CPE algorithm in the PAC setting, which is compu"
    },
    {
        "title": "A Fast Algorithm for PAC Combinatorial Pure Exploration",
        "text": "tationally light weight, and so can easily be applied to problems with tens of thousands of arms. This is achieved since the proposed algorithm requires a very small number of combinatorial oracle calls. The algorithm is based on successive acceptance of a"
    },
    {
        "title": "A Fast Algorithm for PAC Combinatorial Pure Exploration",
        "text": "rms, along with elimination which is based on the combinatorial structure of the problem. We provide sample complexity guarantees for our algorithm, and demonstrate in experiments its usefulness on large problems, whereas previous algorithms are impractica"
    },
    {
        "title": "A Fast Algorithm for PAC Combinatorial Pure Exploration",
        "text": "l to run on problems of even a few dozen arms. The code for the algorithms and experiments is provided at https://github.com/noabdavid/csale."
    },
    {
        "title": "Federated Learning via Inexact ADMM",
        "text": "One of the crucial issues in federated learning is how to develop efficient optimization algorithms. Most of the current ones require full devices participation and/or impose strong assumptions for convergence. Different from the widely-used gradient desce"
    },
    {
        "title": "Federated Learning via Inexact ADMM",
        "text": "nt-based algorithms, this paper develops an inexact alternating direction method of multipliers (ADMM), which is both computation and communication-efficient, capable of combating the stragglers' effect, and convergent under mild conditions."
    },
    {
        "title": "New directions for surrogate models and differentiable programming for   High Energy Physics detector simulation",
        "text": "The computational cost for high energy physics detector simulation in future experimental facilities is going to exceed the current available resources. To overcome this challenge, new ideas on surrogate models using machine learning methods are being expl"
    },
    {
        "title": "New directions for surrogate models and differentiable programming for   High Energy Physics detector simulation",
        "text": "ored to replace computationally expensive components. Additionally, differentiable programming has been proposed as a complementary approach, providing controllable and scalable simulation routines. In this document, new and ongoing efforts for surrogate m"
    },
    {
        "title": "New directions for surrogate models and differentiable programming for   High Energy Physics detector simulation",
        "text": "odels and differential programming applied to detector simulation are discussed in the context of the 2021 Particle Physics Community Planning Exercise (`Snowmass')."
    },
    {
        "title": "What You See is What You Classify: Black Box Attributions",
        "text": "An important step towards explaining deep image classifiers lies in the identification of image regions that contribute to individual class scores in the model's output. However, doing this accurately is a difficult task due to the black-box nature of such"
    },
    {
        "title": "What You See is What You Classify: Black Box Attributions",
        "text": " networks. Most existing approaches find such attributions either using activations and gradients or by repeatedly perturbing the input. We instead address this challenge by training a second deep network, the Explainer, to predict attributions for a pre-t"
    },
    {
        "title": "What You See is What You Classify: Black Box Attributions",
        "text": "rained black-box classifier, the Explanandum. These attributions are in the form of masks that only show the classifier-relevant parts of an image, masking out the rest. Our approach produces sharper and more boundary-precise masks when compared to the sal"
    },
    {
        "title": "What You See is What You Classify: Black Box Attributions",
        "text": "iency maps generated by other methods. Moreover, unlike most existing approaches, ours is capable of directly generating very distinct class-specific masks. Finally, the proposed method is very efficient for inference since it only takes a single forward p"
    },
    {
        "title": "What You See is What You Classify: Black Box Attributions",
        "text": "ass through the Explainer to generate all class-specific masks. We show that our attributions are superior to established methods both visually and quantitatively, by evaluating them on the PASCAL VOC-2007 and Microsoft COCO-2014 datasets."
    },
    {
        "title": "Dueling Bandits with Dependent Arms",
        "text": "We study dueling bandits with weak utility-based regret when preferences over arms have a total order and carry observable feature vectors. The order is assumed to be determined by these feature vectors, an unknown preference vector, and a known utility fu"
    },
    {
        "title": "Dueling Bandits with Dependent Arms",
        "text": "nction. This structure introduces dependence between preferences for pairs of arms, and allows learning about the preference over one pair of arms from the preference over another pair of arms. We propose an algorithm for this setting called Comparing The "
    },
    {
        "title": "Dueling Bandits with Dependent Arms",
        "text": "Best (CTB), which we show has constant expected cumulative weak utility-based regret. We provide a Bayesian interpretation for CTB, an implementation appropriate for a small number of arms, and an alternate implementation for many arms that can be used whe"
    },
    {
        "title": "Dueling Bandits with Dependent Arms",
        "text": "n the input parameters satisfy a decomposability condition. We demonstrate through numerical experiments that CTB with appropriate input parameters outperforms all benchmarks considered."
    },
    {
        "title": "Training Neural Networks is $\\exists\\mathbb R$-complete",
        "text": "Given a neural network, training data, and a threshold, it was known that it is NP-hard to find weights for the neural network such that the total error is below the threshold. We determine the algorithmic complexity of this fundamental problem precisely, "
    },
    {
        "title": "Training Neural Networks is $\\exists\\mathbb R$-complete",
        "text": "by showing that it is $\\exists\\mathbb R$-complete. This means that the problem is equivalent, up to polynomial-time reductions, to deciding whether a system of polynomial equations and inequalities with integer coefficients and real unknowns has a solution"
    },
    {
        "title": "Training Neural Networks is $\\exists\\mathbb R$-complete",
        "text": ". If, as widely expected, $\\exists\\mathbb R$ is strictly larger than NP, our work implies that the problem of training neural networks is not even in NP.   Neural networks are usually trained using some variation of backpropagation. The result of this pape"
    },
    {
        "title": "Training Neural Networks is $\\exists\\mathbb R$-complete",
        "text": "r offers an explanation why techniques commonly used to solve big instances of NP-complete problems seem not to be of use for this task. Examples of such techniques are SAT solvers, IP solvers, local search, dynamic programming, to name a few general ones."
    },
    {
        "title": "Graph Dynamical Networks for Unsupervised Learning of Atomic Scale   Dynamics in Materials",
        "text": "Understanding the dynamical processes that govern the performance of functional materials is essential for the design of next generation materials to tackle global energy and environmental challenges. Many of these processes involve the dynamics of individ"
    },
    {
        "title": "Graph Dynamical Networks for Unsupervised Learning of Atomic Scale   Dynamics in Materials",
        "text": "ual atoms or small molecules in condensed phases, e.g. lithium ions in electrolytes, water molecules in membranes, molten atoms at interfaces, etc., which are difficult to understand due to the complexity of local environments. In this work, we develop gra"
    },
    {
        "title": "Graph Dynamical Networks for Unsupervised Learning of Atomic Scale   Dynamics in Materials",
        "text": "ph dynamical networks, an unsupervised learning approach for understanding atomic scale dynamics in arbitrary phases and environments from molecular dynamics simulations. We show that important dynamical information can be learned for various multi-compone"
    },
    {
        "title": "Graph Dynamical Networks for Unsupervised Learning of Atomic Scale   Dynamics in Materials",
        "text": "nt amorphous material systems, which is difficult to obtain otherwise. With the large amounts of molecular dynamics data generated everyday in nearly every aspect of materials design, this approach provides a broadly useful, automated tool to understand at"
    },
    {
        "title": "Graph Dynamical Networks for Unsupervised Learning of Atomic Scale   Dynamics in Materials",
        "text": "omic scale dynamics in material systems."
    },
    {
        "title": "Logarithmic Pruning is All You Need",
        "text": "The Lottery Ticket Hypothesis is a conjecture that every large neural network contains a subnetwork that, when trained in isolation, achieves comparable performance to the large network. An even stronger conjecture has been proven recently: Every sufficien"
    },
    {
        "title": "Logarithmic Pruning is All You Need",
        "text": "tly overparameterized network contains a subnetwork that, at random initialization, but without training, achieves comparable accuracy to the trained large network. This latter result, however, relies on a number of strong assumptions and guarantees a poly"
    },
    {
        "title": "Logarithmic Pruning is All You Need",
        "text": "nomial factor on the size of the large network compared to the target function. In this work, we remove the most limiting assumptions of this previous work while providing significantly tighter bounds:the overparameterized network only needs a logarithmic "
    },
    {
        "title": "Logarithmic Pruning is All You Need",
        "text": "factor (in all variables but depth) number of neurons per weight of the target subnetwork."
    },
    {
        "title": "Over-The-Air Federated Learning under Byzantine Attacks",
        "text": "Federated learning (FL) is a promising solution to enable many AI applications, where sensitive datasets from distributed clients are needed for collaboratively training a global model. FL allows the clients to participate in the training phase, governed b"
    },
    {
        "title": "Over-The-Air Federated Learning under Byzantine Attacks",
        "text": "y a central server, without sharing their local data. One of the main challenges of FL is the communication overhead, where the model updates of the participating clients are sent to the central server at each global training round. Over-the-air computatio"
    },
    {
        "title": "Over-The-Air Federated Learning under Byzantine Attacks",
        "text": "n (AirComp) has been recently proposed to alleviate the communication bottleneck where the model updates are sent simultaneously over the multiple-access channel. However, simple averaging of the model updates via AirComp makes the learning process vulnera"
    },
    {
        "title": "Over-The-Air Federated Learning under Byzantine Attacks",
        "text": "ble to random or intended modifications of the local model updates of some Byzantine clients. In this paper, we propose a transmission and aggregation framework to reduce the effect of such attacks while preserving the benefits of AirComp for FL. For the p"
    },
    {
        "title": "Over-The-Air Federated Learning under Byzantine Attacks",
        "text": "roposed robust approach, the central server divides the participating clients randomly into groups and allocates a transmission time slot for each group. The updates of the different groups are then aggregated using a robust aggregation technique. We exten"
    },
    {
        "title": "Over-The-Air Federated Learning under Byzantine Attacks",
        "text": "d our approach to handle the case of non-i.i.d. local data, where a resampling step is added before robust aggregation. We analyze the convergence of the proposed approach for both cases of i.i.d. and non-i.i.d. data and demonstrate that the proposed algor"
    },
    {
        "title": "Over-The-Air Federated Learning under Byzantine Attacks",
        "text": "ithm converges at a linear rate to a neighborhood of the optimal solution. Experiments on real datasets are provided to confirm the robustness of the proposed approach."
    },
    {
        "title": "Multi-Task Learning as a Bargaining Game",
        "text": "In Multi-task learning (MTL), a joint model is trained to simultaneously make predictions for several tasks. Joint training reduces computation costs and improves data efficiency; however, since the gradients of these different tasks may conflict, training"
    },
    {
        "title": "Multi-Task Learning as a Bargaining Game",
        "text": " a joint model for MTL often yields lower performance than its corresponding single-task counterparts. A common method for alleviating this issue is to combine per-task gradients into a joint update direction using a particular heuristic. In this paper, we"
    },
    {
        "title": "Multi-Task Learning as a Bargaining Game",
        "text": " propose viewing the gradients combination step as a bargaining game, where tasks negotiate to reach an agreement on a joint direction of parameter update. Under certain assumptions, the bargaining problem has a unique solution, known as the Nash Bargainin"
    },
    {
        "title": "Multi-Task Learning as a Bargaining Game",
        "text": "g Solution, which we propose to use as a principled approach to multi-task learning. We describe a new MTL optimization procedure, Nash-MTL, and derive theoretical guarantees for its convergence. Empirically, we show that Nash-MTL achieves state-of-the-art"
    },
    {
        "title": "Multi-Task Learning as a Bargaining Game",
        "text": " results on multiple MTL benchmarks in various domains."
    },
    {
        "title": "Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble   of Autoencoders",
        "text": "A fall is an abnormal activity that occurs rarely, so it is hard to collect real data for falls. It is, therefore, difficult to use supervised learning methods to automatically detect falls. Another challenge in using machine learning methods to automatica"
    },
    {
        "title": "Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble   of Autoencoders",
        "text": "lly detect falls is the choice of engineered features. In this paper, we propose to use an ensemble of autoencoders to extract features from different channels of wearable sensor data trained only on normal activities. We show that the traditional approach"
    },
    {
        "title": "Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble   of Autoencoders",
        "text": " of choosing a threshold as the maximum of the reconstruction error on the training normal data is not the right way to identify unseen falls. We propose two methods for automatic tightening of reconstruction error from only the normal activities for bette"
    },
    {
        "title": "Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble   of Autoencoders",
        "text": "r identification of unseen falls. We present our results on two activity recognition datasets and show the efficacy of our proposed method against traditional autoencoder models and two standard one-class classification methods."
    },
    {
        "title": "A Deep Learning Based Chatbot for Campus Psychological Therapy",
        "text": "In this paper, we propose Evebot, an innovative, sequence to sequence (Seq2seq) based, fully generative conversational system for the diagnosis of negative emotions and prevention of depression through positively suggestive responses. The system consists o"
    },
    {
        "title": "A Deep Learning Based Chatbot for Campus Psychological Therapy",
        "text": "f an assembly of deep-learning based models, including Bi-LSTM based model for detecting negative emotions of users and obtaining psychological counselling related corpus for training the chatbot, anti-language sequence to sequence neural network, and maxi"
    },
    {
        "title": "A Deep Learning Based Chatbot for Campus Psychological Therapy",
        "text": "mum mutual information (MMI) model. As adolescents are reluctant to show their negative emotions in physical interaction, traditional methods of emotion analysis and comforting methods may not work. Therefore, this system puts emphasis on using virtual pla"
    },
    {
        "title": "A Deep Learning Based Chatbot for Campus Psychological Therapy",
        "text": "tform to detect signs of depression or anxiety, channel adolescents' stress and mood, and thus prevent the emergence of mental illness. We launched the integrated chatbot system onto an online platform for real-world campus applications. Through a one-mont"
    },
    {
        "title": "A Deep Learning Based Chatbot for Campus Psychological Therapy",
        "text": "h user study, we observe better results in the increase in positivity than other public chatbots in the control group."
    },
    {
        "title": "Learning to Optimize Energy Efficiency in Energy Harvesting Wireless   Sensor Networks",
        "text": "We study wireless power transmission by an energy source to multiple energy harvesting nodes with the aim to maximize the energy efficiency. The source transmits energy to the nodes using one of the available power levels in each time slot and the nodes tr"
    },
    {
        "title": "Learning to Optimize Energy Efficiency in Energy Harvesting Wireless   Sensor Networks",
        "text": "ansmit information back to the energy source using the harvested energy. The source does not have any channel state information and it only knows whether a received codeword from a given node was successfully decoded or not. With this limited information, "
    },
    {
        "title": "Learning to Optimize Energy Efficiency in Energy Harvesting Wireless   Sensor Networks",
        "text": "the source has to learn the optimal power level that maximizes the energy efficiency of the network. We model the problem as a stochastic Multi-Armed Bandits problem and develop an Upper Confidence Bound based algorithm, which learns the optimal transmit p"
    },
    {
        "title": "Learning to Optimize Energy Efficiency in Energy Harvesting Wireless   Sensor Networks",
        "text": "ower of the energy source that maximizes the energy efficiency. Numerical results validate the performance guarantees of the proposed algorithm and show significant gains compared to the benchmark schemes."
    },
    {
        "title": "Classification Under Misspecification: Halfspaces, Generalized Linear   Models, and Connections to Evolvability",
        "text": "In this paper we revisit some classic problems on classification under misspecification. In particular, we study the problem of learning halfspaces under Massart noise with rate $\\eta$. In a recent work, Diakonikolas, Goulekakis, and Tzamos resolved a long"
    },
    {
        "title": "Classification Under Misspecification: Halfspaces, Generalized Linear   Models, and Connections to Evolvability",
        "text": "-standing problem by giving the first efficient algorithm for learning to accuracy $\\eta + \\epsilon$ for any $\\epsilon > 0$. However, their algorithm outputs a complicated hypothesis, which partitions space into $\\text{poly}(d,1/\\epsilon)$ regions. Here we"
    },
    {
        "title": "Classification Under Misspecification: Halfspaces, Generalized Linear   Models, and Connections to Evolvability",
        "text": " give a much simpler algorithm and in the process resolve a number of outstanding open questions:   (1) We give the first proper learner for Massart halfspaces that achieves $\\eta + \\epsilon$. We also give improved bounds on the sample complexity achievabl"
    },
    {
        "title": "Classification Under Misspecification: Halfspaces, Generalized Linear   Models, and Connections to Evolvability",
        "text": "e by polynomial time algorithms.   (2) Based on (1), we develop a blackbox knowledge distillation procedure to convert an arbitrarily complex classifier to an equally good proper classifier.   (3) By leveraging a simple but overlooked connection to evolvab"
    },
    {
        "title": "Classification Under Misspecification: Halfspaces, Generalized Linear   Models, and Connections to Evolvability",
        "text": "ility, we show any SQ algorithm requires super-polynomially many queries to achieve $\\mathsf{OPT} + \\epsilon$.   Moreover we study generalized linear models where $\\mathbb{E}[Y|\\mathbf{X}] = \\sigma(\\langle \\mathbf{w}^*, \\mathbf{X}\\rangle)$ for any odd, mon"
    },
    {
        "title": "Classification Under Misspecification: Halfspaces, Generalized Linear   Models, and Connections to Evolvability",
        "text": "otone, and Lipschitz function $\\sigma$. This family includes the previously mentioned halfspace models as a special case, but is much richer and includes other fundamental models like logistic regression. We introduce a challenging new corruption model tha"
    },
    {
        "title": "Classification Under Misspecification: Halfspaces, Generalized Linear   Models, and Connections to Evolvability",
        "text": "t generalizes Massart noise, and give a general algorithm for learning in this setting. Our algorithms are based on a small set of core recipes for learning to classify in the presence of misspecification.   Finally we study our algorithm for learning half"
    },
    {
        "title": "Classification Under Misspecification: Halfspaces, Generalized Linear   Models, and Connections to Evolvability",
        "text": "spaces under Massart noise empirically and find that it exhibits some appealing fairness properties."
    },
    {
        "title": "Localized Vision-Language Matching for Open-vocabulary Object Detection",
        "text": "In this work, we propose an open-world object detection method that, based on image-caption pairs, learns to detect novel object classes along with a given set of known classes. It is a two-stage training approach that first uses a location-guided image-ca"
    },
    {
        "title": "Localized Vision-Language Matching for Open-vocabulary Object Detection",
        "text": "ption matching technique to learn class labels for both novel and known classes in a weakly-supervised manner and second specializes the model for the object detection task using known class annotations. We show that a simple language model fits better tha"
    },
    {
        "title": "Localized Vision-Language Matching for Open-vocabulary Object Detection",
        "text": "n a large contextualized language model for detecting novel objects. Moreover, we introduce a consistency-regularization technique to better exploit image-caption pair information. Our method compares favorably to existing open-world detection approaches w"
    },
    {
        "title": "Localized Vision-Language Matching for Open-vocabulary Object Detection",
        "text": "hile being data-efficient."
    },
    {
        "title": "Network Implosion: Effective Model Compression for ResNets via Static   Layer Pruning and Retraining",
        "text": "Residual Networks with convolutional layers are widely used in the field of machine learning. Since they effectively extract features from input data by stacking multiple layers, they can achieve high accuracy in many applications. However, the stacking of"
    },
    {
        "title": "Network Implosion: Effective Model Compression for ResNets via Static   Layer Pruning and Retraining",
        "text": " many layers raises their computation costs. To address this problem, we propose Network Implosion, it erases multiple layers from Residual Networks without degrading accuracy. Our key idea is to introduce a priority term that identifies the importance of "
    },
    {
        "title": "Network Implosion: Effective Model Compression for ResNets via Static   Layer Pruning and Retraining",
        "text": "a layer; we can select unimportant layers according to the priority and erase them after the training. In addition, we retrain the networks to avoid critical drops in accuracy after layer erasure. A theoretical assessment reveals that our erasure and retra"
    },
    {
        "title": "Network Implosion: Effective Model Compression for ResNets via Static   Layer Pruning and Retraining",
        "text": "ining scheme can erase layers without accuracy drop, and achieve higher accuracy than is possible with training from scratch. Our experiments show that Network Implosion can, for classification on Cifar-10/100 and ImageNet, reduce the number of layers by 2"
    },
    {
        "title": "Network Implosion: Effective Model Compression for ResNets via Static   Layer Pruning and Retraining",
        "text": "4.00 to 42.86 percent without any drop in accuracy."
    },
    {
        "title": "Neural Multi-Scale Self-Supervised Registration for Echocardiogram Dense   Tracking",
        "text": "Echocardiography has become routinely used in the diagnosis of cardiomyopathy and abnormal cardiac blood flow. However, manually measuring myocardial motion and cardiac blood flow from echocardiogram is time-consuming and error-prone. Computer algorithms t"
    },
    {
        "title": "Neural Multi-Scale Self-Supervised Registration for Echocardiogram Dense   Tracking",
        "text": "hat can automatically track and quantify myocardial motion and cardiac blood flow are highly sought after, but have not been very successful due to noise and high variability of echocardiography. In this work, we propose a neural multi-scale self-supervise"
    },
    {
        "title": "Neural Multi-Scale Self-Supervised Registration for Echocardiogram Dense   Tracking",
        "text": "d registration (NMSR) method for automated myocardial and cardiac blood flow dense tracking. NMSR incorporates two novel components: 1) utilizing a deep neural net to parameterize the velocity field between two image frames, and 2) optimizing the parameter"
    },
    {
        "title": "Neural Multi-Scale Self-Supervised Registration for Echocardiogram Dense   Tracking",
        "text": "s of the neural net in a sequential multi-scale fashion to account for large variations within the velocity field. Experiments demonstrate that NMSR yields significantly better registration accuracy than state-of-the-art methods, such as advanced normaliza"
    },
    {
        "title": "Neural Multi-Scale Self-Supervised Registration for Echocardiogram Dense   Tracking",
        "text": "tion tools (ANTs) and VoxelMorph, for both myocardial and cardiac blood flow dense tracking. Our approach promises to provide a fully automated method for fast and accurate analyses of echocardiograms."
    },
    {
        "title": "Learning to solve geometric construction problems from images",
        "text": "We describe a purely image-based method for finding geometric constructions with a ruler and compass in the Euclidea geometric game. The method is based on adapting the Mask R-CNN state-of-the-art image processing neural architecture and adding a tree-base"
    },
    {
        "title": "Learning to solve geometric construction problems from images",
        "text": "d search procedure to it. In a supervised setting, the method learns to solve all 68 kinds of geometric construction problems from the first six level packs of Euclidea with an average 92% accuracy. When evaluated on new kinds of problems, the method can s"
    },
    {
        "title": "Learning to solve geometric construction problems from images",
        "text": "olve 31 of the 68 kinds of Euclidea problems. We believe that this is the first time that a purely image-based learning has been trained to solve geometric construction problems of this difficulty."
    },
    {
        "title": "COBRA: Context-aware Bernoulli Neural Networks for Reputation Assessment",
        "text": "Trust and reputation management (TRM) plays an increasingly important role in large-scale online environments such as multi-agent systems (MAS) and the Internet of Things (IoT). One main objective of TRM is to achieve accurate trust assessment of entities "
    },
    {
        "title": "COBRA: Context-aware Bernoulli Neural Networks for Reputation Assessment",
        "text": "such as agents or IoT service providers. However, this encounters an accuracy-privacy dilemma as we identify in this paper, and we propose a framework called Context-aware Bernoulli Neural Network based Reputation Assessment (COBRA) to address this challen"
    },
    {
        "title": "COBRA: Context-aware Bernoulli Neural Networks for Reputation Assessment",
        "text": "ge. COBRA encapsulates agent interactions or transactions, which are prone to privacy leak, in machine learning models, and aggregates multiple such models using a Bernoulli neural network to predict a trust score for an agent. COBRA preserves agent privac"
    },
    {
        "title": "COBRA: Context-aware Bernoulli Neural Networks for Reputation Assessment",
        "text": "y and retains interaction contexts via the machine learning models, and achieves more accurate trust prediction than a fully-connected neural network alternative. COBRA is also robust to security attacks by agents who inject fake machine learning models; n"
    },
    {
        "title": "COBRA: Context-aware Bernoulli Neural Networks for Reputation Assessment",
        "text": "otably, it is resistant to the 51-percent attack. The performance of COBRA is validated by our experiments using a real dataset, and by our simulations, where we also show that COBRA outperforms other state-of-the-art TRM systems."
    },
    {
        "title": "Secure Machine Learning in the Cloud Using One Way Scrambling by   Deconvolution",
        "text": "Cloud-based machine learning services (CMLS) enable organizations to take advantage of advanced models that are pre-trained on large quantities of data. The main shortcoming of using these services, however, is the difficulty of keeping the transmitted dat"
    },
    {
        "title": "Secure Machine Learning in the Cloud Using One Way Scrambling by   Deconvolution",
        "text": "a private and secure. Asymmetric encryption requires the data to be decrypted in the cloud, while Homomorphic encryption is often too slow and difficult to implement. We propose One Way Scrambling by Deconvolution (OWSD), a deconvolution-based scrambling f"
    },
    {
        "title": "Secure Machine Learning in the Cloud Using One Way Scrambling by   Deconvolution",
        "text": "ramework that offers the advantages of Homomorphic encryption at a fraction of the computational overhead. Extensive evaluation on multiple image datasets demonstrates OWSD's ability to achieve near-perfect classification performance when the output vector"
    },
    {
        "title": "Secure Machine Learning in the Cloud Using One Way Scrambling by   Deconvolution",
        "text": " of the CMLS is sufficiently large. Additionally, we provide empirical analysis of the robustness of our approach."
    },
    {
        "title": "Exploring Topic-Metadata Relationships with the STM: A Bayesian Approach",
        "text": "Topic models such as the Structural Topic Model (STM) estimate latent topical clusters within text. An important step in many topic modeling applications is to explore relationships between the discovered topical structure and metadata associated with the "
    },
    {
        "title": "Exploring Topic-Metadata Relationships with the STM: A Bayesian Approach",
        "text": "text documents. Methods used to estimate such relationships must take into account that the topical structure is not directly observed, but instead being estimated itself. The authors of the STM, for instance, perform repeated OLS regressions of sampled to"
    },
    {
        "title": "Exploring Topic-Metadata Relationships with the STM: A Bayesian Approach",
        "text": "pic proportions on metadata covariates by using a Monte Carlo sampling technique known as the method of composition. In this paper, we propose two improvements: first, we replace OLS with more appropriate Beta regression. Second, we suggest a fully Bayesia"
    },
    {
        "title": "Exploring Topic-Metadata Relationships with the STM: A Bayesian Approach",
        "text": "n approach instead of the current blending of frequentist and Bayesian methods. We demonstrate our improved methodology by exploring relationships between Twitter posts by German members of parliament (MPs) and different metadata covariates."
    },
    {
        "title": "FAR: A General Framework for Attributional Robustness",
        "text": "Attribution maps are popular tools for explaining neural networks predictions. By assigning an importance value to each input dimension that represents its impact towards the outcome, they give an intuitive explanation of the decision process. However, rec"
    },
    {
        "title": "FAR: A General Framework for Attributional Robustness",
        "text": "ent work has discovered vulnerability of these maps to imperceptible adversarial changes, which can prove critical in safety-relevant domains such as healthcare. Therefore, we define a novel generic framework for attributional robustness (FAR) as general p"
    },
    {
        "title": "FAR: A General Framework for Attributional Robustness",
        "text": "roblem formulation for training models with robust attributions. This framework consist of a generic regularization term and training objective that minimize the maximal dissimilarity of attribution maps in a local neighbourhood of the input. We show that "
    },
    {
        "title": "FAR: A General Framework for Attributional Robustness",
        "text": "FAR is a generalized, less constrained formulation of currently existing training methods. We then propose two new instantiations of this framework, AAT and AdvAAT, that directly optimize for both robust attributions and predictions. Experiments performed "
    },
    {
        "title": "FAR: A General Framework for Attributional Robustness",
        "text": "on widely used vision datasets show that our methods perform better or comparably to current ones in terms of attributional robustness while being more generally applicable. We finally show that our methods mitigate undesired dependencies between attributi"
    },
    {
        "title": "FAR: A General Framework for Attributional Robustness",
        "text": "onal robustness and some training and estimation parameters, which seem to critically affect other competitor methods."
    },
    {
        "title": "Set-based Meta-Interpolation for Few-Task Meta-Learning",
        "text": "Meta-learning approaches enable machine learning systems to adapt to new tasks given few examples by leveraging knowledge from related tasks. However, a large number of meta-training tasks are still required for generalization to unseen tasks during meta-t"
    },
    {
        "title": "Set-based Meta-Interpolation for Few-Task Meta-Learning",
        "text": "esting, which introduces a critical bottleneck for real-world problems that come with only few tasks, due to various reasons including the difficulty and cost of constructing tasks. Recently, several task augmentation methods have been proposed to tackle t"
    },
    {
        "title": "Set-based Meta-Interpolation for Few-Task Meta-Learning",
        "text": "his issue using domain-specific knowledge to design augmentation techniques to densify the meta-training task distribution. However, such reliance on domain-specific knowledge renders these methods inapplicable to other domains. While Manifold Mixup based "
    },
    {
        "title": "Set-based Meta-Interpolation for Few-Task Meta-Learning",
        "text": "task augmentation methods are domain-agnostic, we empirically find them ineffective on non-image domains. To tackle these limitations, we propose a novel domain-agnostic task augmentation method, Meta-Interpolation, which utilizes expressive neural set fun"
    },
    {
        "title": "Set-based Meta-Interpolation for Few-Task Meta-Learning",
        "text": "ctions to densify the meta-training task distribution using bilevel optimization. We empirically validate the efficacy of Meta-Interpolation on eight datasets spanning across various domains such as image classification, molecule property prediction, text "
    },
    {
        "title": "Set-based Meta-Interpolation for Few-Task Meta-Learning",
        "text": "classification and speech recognition. Experimentally, we show that Meta-Interpolation consistently outperforms all the relevant baselines. Theoretically, we prove that task interpolation with the set function regularizes the meta-learner to improve genera"
    },
    {
        "title": "Set-based Meta-Interpolation for Few-Task Meta-Learning",
        "text": "lization."
    },
    {
        "title": "PyHard: a novel tool for generating hardness embeddings to support   data-centric analysis",
        "text": "For building successful Machine Learning (ML) systems, it is imperative to have high quality data and well tuned learning models. But how can one assess the quality of a given dataset? And how can the strengths and weaknesses of a model on a dataset be rev"
    },
    {
        "title": "PyHard: a novel tool for generating hardness embeddings to support   data-centric analysis",
        "text": "ealed? Our new tool PyHard employs a methodology known as Instance Space Analysis (ISA) to produce a hardness embedding of a dataset relating the predictive performance of multiple ML models to estimated instance hardness meta-features. This space is built"
    },
    {
        "title": "PyHard: a novel tool for generating hardness embeddings to support   data-centric analysis",
        "text": " so that observations are distributed linearly regarding how hard they are to classify. The user can visually interact with this embedding in multiple ways and obtain useful insights about data and algorithmic performance along the individual observations "
    },
    {
        "title": "PyHard: a novel tool for generating hardness embeddings to support   data-centric analysis",
        "text": "of the dataset. We show in a COVID prognosis dataset how this analysis supported the identification of pockets of hard observations that challenge ML models and are therefore worth closer inspection, and the delineation of regions of strengths and weakness"
    },
    {
        "title": "PyHard: a novel tool for generating hardness embeddings to support   data-centric analysis",
        "text": "es of ML models."
    },
    {
        "title": "Generating News Headlines with Recurrent Neural Networks",
        "text": "We describe an application of an encoder-decoder recurrent neural network with LSTM units and attention to generating headlines from the text of news articles. We find that the model is quite effective at concisely paraphrasing news articles. Furthermore, "
    },
    {
        "title": "Generating News Headlines with Recurrent Neural Networks",
        "text": "we study how the neural network decides which input words to pay attention to, and specifically we identify the function of the different neurons in a simplified attention mechanism. Interestingly, our simplified attention mechanism performs better that th"
    },
    {
        "title": "Generating News Headlines with Recurrent Neural Networks",
        "text": "e more complex attention mechanism on a held out set of articles."
    },
    {
        "title": "Natural language processing to identify lupus nephritis phenotype in   electronic health records",
        "text": "Systemic lupus erythematosus (SLE) is a rare autoimmune disorder characterized by an unpredictable course of flares and remission with diverse manifestations. Lupus nephritis, one of the major disease manifestations of SLE for organ damage and mortality, i"
    },
    {
        "title": "Natural language processing to identify lupus nephritis phenotype in   electronic health records",
        "text": "s a key component of lupus classification criteria. Accurately identifying lupus nephritis in electronic health records (EHRs) would therefore benefit large cohort observational studies and clinical trials where characterization of the patient population i"
    },
    {
        "title": "Natural language processing to identify lupus nephritis phenotype in   electronic health records",
        "text": "s critical for recruitment, study design, and analysis. Lupus nephritis can be recognized through procedure codes and structured data, such as laboratory tests. However, other critical information documenting lupus nephritis, such as histologic reports fro"
    },
    {
        "title": "Natural language processing to identify lupus nephritis phenotype in   electronic health records",
        "text": "m kidney biopsies and prior medical history narratives, require sophisticated text processing to mine information from pathology reports and clinical notes. In this study, we developed algorithms to identify lupus nephritis with and without natural languag"
    },
    {
        "title": "Natural language processing to identify lupus nephritis phenotype in   electronic health records",
        "text": "e processing (NLP) using EHR data. We developed four algorithms: a rule-based algorithm using only structured data (baseline algorithm) and three algorithms using different NLP models. The three NLP models are based on regularized logistic regression and u"
    },
    {
        "title": "Natural language processing to identify lupus nephritis phenotype in   electronic health records",
        "text": "se different sets of features including positive mention of concept unique identifiers (CUIs), number of appearances of CUIs, and a mixture of three components respectively. The baseline algorithm and the best performed NLP algorithm were external validate"
    },
    {
        "title": "Natural language processing to identify lupus nephritis phenotype in   electronic health records",
        "text": "d on a dataset from Vanderbilt University Medical Center (VUMC). Our best performing NLP model incorporating features from both structured data, regular expression concepts, and mapped CUIs improved F measure in both the NMEDW (0.41 vs 0.79) and VUMC (0.62"
    },
    {
        "title": "Natural language processing to identify lupus nephritis phenotype in   electronic health records",
        "text": " vs 0.96) datasets compared to the baseline lupus nephritis algorithm."
    },
    {
        "title": "Exact multiplicative updates for convolutional $\\beta$-NMF in 2D",
        "text": "In this paper, we extend the $\\beta$-CNMF to two dimensions and derive exact multiplicative updates for its factors. The new updates generalize and correct the nonnegative matrix factor deconvolution previously proposed by Schmidt and M{\\o}rup. We show by "
    },
    {
        "title": "Exact multiplicative updates for convolutional $\\beta$-NMF in 2D",
        "text": "simulation that the updates lead to a monotonically decreasing $\\beta$-divergence in terms of the mean and the standard deviation and that the corresponding convergence curves are consistent across the most common values for $\\beta$."
    },
    {
        "title": "Markov Chain Monte Carlo for Continuous-Time Switching Dynamical Systems",
        "text": "Switching dynamical systems are an expressive model class for the analysis of time-series data. As in many fields within the natural and engineering sciences, the systems under study typically evolve continuously in time, it is natural to consider continuo"
    },
    {
        "title": "Markov Chain Monte Carlo for Continuous-Time Switching Dynamical Systems",
        "text": "us-time model formulations consisting of switching stochastic differential equations governed by an underlying Markov jump process. Inference in these types of models is however notoriously difficult, and tractable computational schemes are rare. In this w"
    },
    {
        "title": "Markov Chain Monte Carlo for Continuous-Time Switching Dynamical Systems",
        "text": "ork, we propose a novel inference algorithm utilizing a Markov Chain Monte Carlo approach. The presented Gibbs sampler allows to efficiently obtain samples from the exact continuous-time posterior processes. Our framework naturally enables Bayesian paramet"
    },
    {
        "title": "Markov Chain Monte Carlo for Continuous-Time Switching Dynamical Systems",
        "text": "er estimation, and we also include an estimate for the diffusion covariance, which is oftentimes assumed fixed in stochastic differential equation models. We evaluate our framework under the modeling assumption and compare it against an existing variationa"
    },
    {
        "title": "Markov Chain Monte Carlo for Continuous-Time Switching Dynamical Systems",
        "text": "l inference approach."
    },
    {
        "title": "A Hierarchical Block Distance Model for Ultra Low-Dimensional Graph   Representations",
        "text": "Graph Representation Learning (GRL) has become central for characterizing structures of complex networks and performing tasks such as link prediction, node classification, network reconstruction, and community detection. Whereas numerous generative GRL mod"
    },
    {
        "title": "A Hierarchical Block Distance Model for Ultra Low-Dimensional Graph   Representations",
        "text": "els have been proposed, many approaches have prohibitive computational requirements hampering large-scale network analysis, fewer are able to explicitly account for structure emerging at multiple scales, and only a few explicitly respect important network "
    },
    {
        "title": "A Hierarchical Block Distance Model for Ultra Low-Dimensional Graph   Representations",
        "text": "properties such as homophily and transitivity. This paper proposes a novel scalable graph representation learning method named the Hierarchical Block Distance Model (HBDM). The HBDM imposes a multiscale block structure akin to stochastic block modeling (SB"
    },
    {
        "title": "A Hierarchical Block Distance Model for Ultra Low-Dimensional Graph   Representations",
        "text": "M) and accounts for homophily and transitivity by accurately approximating the latent distance model (LDM) throughout the inferred hierarchy. The HBDM naturally accommodates unipartite, directed, and bipartite networks whereas the hierarchy is designed to "
    },
    {
        "title": "A Hierarchical Block Distance Model for Ultra Low-Dimensional Graph   Representations",
        "text": "ensure linearithmic time and space complexity enabling the analysis of very large-scale networks. We evaluate the performance of the HBDM on massive networks consisting of millions of nodes. Importantly, we find that the proposed HBDM framework significant"
    },
    {
        "title": "A Hierarchical Block Distance Model for Ultra Low-Dimensional Graph   Representations",
        "text": "ly outperforms recent scalable approaches in all considered downstream tasks. Surprisingly, we observe superior performance even imposing ultra-low two-dimensional embeddings facilitating accurate direct and hierarchical-aware network visualization and int"
    },
    {
        "title": "A Hierarchical Block Distance Model for Ultra Low-Dimensional Graph   Representations",
        "text": "erpretation."
    },
    {
        "title": "Low-rank optimization for distance matrix completion",
        "text": "This paper addresses the problem of low-rank distance matrix completion. This problem amounts to recover the missing entries of a distance matrix when the dimension of the data embedding space is possibly unknown but small compared to the number of conside"
    },
    {
        "title": "Low-rank optimization for distance matrix completion",
        "text": "red data points. The focus is on high-dimensional problems. We recast the considered problem into an optimization problem over the set of low-rank positive semidefinite matrices and propose two efficient algorithms for low-rank distance matrix completion. "
    },
    {
        "title": "Low-rank optimization for distance matrix completion",
        "text": "In addition, we propose a strategy to determine the dimension of the embedding space. The resulting algorithms scale to high-dimensional problems and monotonically converge to a global solution of the problem. Finally, numerical experiments illustrate the "
    },
    {
        "title": "Low-rank optimization for distance matrix completion",
        "text": "good performance of the proposed algorithms on benchmarks."
    },
    {
        "title": "Exploiting Position Bias for Robust Aspect Sentiment Classification",
        "text": "Aspect sentiment classification (ASC) aims at determining sentiments expressed towards different aspects in a sentence. While state-of-the-art ASC models have achieved remarkable performance, they are recently shown to suffer from the issue of robustness. "
    },
    {
        "title": "Exploiting Position Bias for Robust Aspect Sentiment Classification",
        "text": "Particularly in two common scenarios: when domains of test and training data are different (out-of-domain scenario) or test data is adversarially perturbed (adversarial scenario), ASC models may attend to irrelevant words and neglect opinion expressions th"
    },
    {
        "title": "Exploiting Position Bias for Robust Aspect Sentiment Classification",
        "text": "at truly describe diverse aspects. To tackle the challenge, in this paper, we hypothesize that position bias (i.e., the words closer to a concerning aspect would carry a higher degree of importance) is crucial for building more robust ASC models by reducin"
    },
    {
        "title": "Exploiting Position Bias for Robust Aspect Sentiment Classification",
        "text": "g the probability of mis-attending. Accordingly, we propose two mechanisms for capturing position bias, namely position-biased weight and position-biased dropout, which can be flexibly injected into existing models to enhance representations for classifica"
    },
    {
        "title": "Exploiting Position Bias for Robust Aspect Sentiment Classification",
        "text": "tion. Experiments conducted on out-of-domain and adversarial datasets demonstrate that our proposed approaches largely improve the robustness and effectiveness of current models."
    },
    {
        "title": "In a Nutshell, the Human Asked for This: Latent Goals for Following   Temporal Specifications",
        "text": "We address the problem of building agents whose goal is to learn to execute out-of distribution (OOD) multi-task instructions expressed in temporal logic (TL) by using deep reinforcement learning (DRL). Recent works provided evidence that the agent's neura"
    },
    {
        "title": "In a Nutshell, the Human Asked for This: Latent Goals for Following   Temporal Specifications",
        "text": "l architecture is a key feature when DRL agents are learning to solve OOD tasks in TL. Yet, the studies on this topic are still in their infancy. In this work, we propose a new deep learning configuration with inductive biases that lead agents to generate "
    },
    {
        "title": "In a Nutshell, the Human Asked for This: Latent Goals for Following   Temporal Specifications",
        "text": "latent representations of their current goal, yielding a stronger generalization performance. We use these latent-goal networks within a neuro-symbolic framework that executes multi-task formally-defined instructions and contrast the performance of the pro"
    },
    {
        "title": "In a Nutshell, the Human Asked for This: Latent Goals for Following   Temporal Specifications",
        "text": "posed neural networks against employing different state-of-the-art (SOTA) architectures when generalizing to unseen instructions in OOD environments."
    },
    {
        "title": "Recursive Sketches for Modular Deep Learning",
        "text": "We present a mechanism to compute a sketch (succinct summary) of how a complex modular deep network processes its inputs. The sketch summarizes essential information about the inputs and outputs of the network and can be used to quickly identify key compon"
    },
    {
        "title": "Recursive Sketches for Modular Deep Learning",
        "text": "ents and summary statistics of the inputs. Furthermore, the sketch is recursive and can be unrolled to identify sub-components of these components and so forth, capturing a potentially complicated DAG structure. These sketches erase gracefully; even if we "
    },
    {
        "title": "Recursive Sketches for Modular Deep Learning",
        "text": "erase a fraction of the sketch at random, the remainder still retains the `high-weight' information present in the original sketch. The sketches can also be organized in a repository to implicitly form a `knowledge graph'; it is possible to quickly retriev"
    },
    {
        "title": "Recursive Sketches for Modular Deep Learning",
        "text": "e sketches in the repository that are related to a sketch of interest; arranged in this fashion, the sketches can also be used to learn emerging concepts by looking for new clusters in sketch space. Finally, in the scenario where we want to learn a ground "
    },
    {
        "title": "Recursive Sketches for Modular Deep Learning",
        "text": "truth deep network, we show that augmenting input/output pairs with these sketches can theoretically make it easier to do so."
    },
    {
        "title": "AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning",
        "text": "Multi-task learning is an open and challenging problem in computer vision. The typical way of conducting multi-task learning with deep neural networks is either through handcrafted schemes that share all initial layers and branch out at an adhoc point, or "
    },
    {
        "title": "AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning",
        "text": "through separate task-specific networks with an additional feature sharing/fusion mechanism. Unlike existing methods, we propose an adaptive sharing approach, called AdaShare, that decides what to share across which tasks to achieve the best recognition ac"
    },
    {
        "title": "AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning",
        "text": "curacy, while taking resource efficiency into account. Specifically, our main idea is to learn the sharing pattern through a task-specific policy that selectively chooses which layers to execute for a given task in the multi-task network. We efficiently op"
    },
    {
        "title": "AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning",
        "text": "timize the task-specific policy jointly with the network weights, using standard back-propagation. Experiments on several challenging and diverse benchmark datasets with a variable number of tasks well demonstrate the efficacy of our approach over state-of"
    },
    {
        "title": "AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning",
        "text": "-the-art methods. Project page: https://cs-people.bu.edu/sunxm/AdaShare/project.html."
    },
    {
        "title": "EigenGP: Sparse Gaussian process models with data-dependent   eigenfunctions",
        "text": "Gaussian processes (GPs) provide a nonparametric representation of functions. However, classical GP inference suffers from high computational cost and it is difficult to design nonstationary GP priors in practice. In this paper, we propose a sparse Gaussia"
    },
    {
        "title": "EigenGP: Sparse Gaussian process models with data-dependent   eigenfunctions",
        "text": "n process model, EigenGP, based on the Karhunen-Loeve (KL) expansion of a GP prior. We use the Nystrom approximation to obtain data dependent eigenfunctions and select these eigenfunctions by evidence maximization. This selection reduces the number of eige"
    },
    {
        "title": "EigenGP: Sparse Gaussian process models with data-dependent   eigenfunctions",
        "text": "nfunctions in our model and provides a nonstationary covariance function. To handle nonlinear likelihoods, we develop an efficient expectation propagation (EP) inference algorithm, and couple it with expectation maximization for eigenfunction selection. Be"
    },
    {
        "title": "EigenGP: Sparse Gaussian process models with data-dependent   eigenfunctions",
        "text": "cause the eigenfunctions of a Gaussian kernel are associated with clusters of samples - including both the labeled and unlabeled - selecting relevant eigenfunctions enables EigenGP to conduct semi-supervised learning. Our experimental results demonstrate i"
    },
    {
        "title": "EigenGP: Sparse Gaussian process models with data-dependent   eigenfunctions",
        "text": "mproved predictive performance of EigenGP over alternative state-of-the-art sparse GP and semisupervised learning methods for regression, classification, and semisupervised classification."
    },
    {
        "title": "Generating Sentences Using a Dynamic Canvas",
        "text": "We introduce the Attentive Unsupervised Text (W)riter (AUTR), which is a word level generative model for natural language. It uses a recurrent neural network with a dynamic attention and canvas memory mechanism to iteratively construct sentences. By viewin"
    },
    {
        "title": "Generating Sentences Using a Dynamic Canvas",
        "text": "g the state of the memory at intermediate stages and where the model is placing its attention, we gain insight into how it constructs sentences. We demonstrate that AUTR learns a meaningful latent representation for each sentence, and achieves competitive "
    },
    {
        "title": "Generating Sentences Using a Dynamic Canvas",
        "text": "log-likelihood lower bounds whilst being computationally efficient. It is effective at generating and reconstructing sentences, as well as imputing missing words."
    },
    {
        "title": "Towards Group Learning: Distributed Weighting of Experts",
        "text": "Aggregating signals from a collection of noisy sources is a fundamental problem in many domains including crowd-sourcing, multi-agent planning, sensor networks, signal processing, voting, ensemble learning, and federated learning. The core question is how "
    },
    {
        "title": "Towards Group Learning: Distributed Weighting of Experts",
        "text": "to aggregate signals from multiple sources (e.g. experts) in order to reveal an underlying ground truth. While a full answer depends on the type of signal, correlation of signals, and desired output, a problem common to all of these applications is that of"
    },
    {
        "title": "Towards Group Learning: Distributed Weighting of Experts",
        "text": " differentiating sources based on their quality and weighting them accordingly. It is often assumed that this differentiation and aggregation is done by a single, accurate central mechanism or agent (e.g. judge). We complicate this model in two ways. First"
    },
    {
        "title": "Towards Group Learning: Distributed Weighting of Experts",
        "text": ", we investigate the setting with both a single judge, and one with multiple judges. Second, given this multi-agent interaction of judges, we investigate various constraints on the judges' reporting space. We build on known results for the optimal weightin"
    },
    {
        "title": "Towards Group Learning: Distributed Weighting of Experts",
        "text": "g of experts and prove that an ensemble of sub-optimal mechanisms can perform optimally under certain conditions. We then show empirically that the ensemble approximates the performance of the optimal mechanism under a broader range of conditions."
    },
    {
        "title": "Learning Physics-Informed Neural Networks without Stacked   Back-propagation",
        "text": "Physics-Informed Neural Network (PINN) has become a commonly used machine learning approach to solve partial differential equations (PDE). But, facing high-dimensional second-order PDE problems, PINN will suffer from severe scalability issues since its los"
    },
    {
        "title": "Learning Physics-Informed Neural Networks without Stacked   Back-propagation",
        "text": "s includes second-order derivatives, the computational cost of which will grow along with the dimension during stacked back-propagation. In this paper, we develop a novel approach that can significantly accelerate the training of Physics-Informed Neural Ne"
    },
    {
        "title": "Learning Physics-Informed Neural Networks without Stacked   Back-propagation",
        "text": "tworks. In particular, we parameterize the PDE solution by the Gaussian smoothed model and show that, derived from Stein's Identity, the second-order derivatives can be efficiently calculated without back-propagation. We further discuss the model capacity "
    },
    {
        "title": "Learning Physics-Informed Neural Networks without Stacked   Back-propagation",
        "text": "and provide variance reduction methods to address key limitations in the derivative estimation. Experimental results show that our proposed method can achieve competitive error compared to standard PINN training but is two orders of magnitude faster."
    },
    {
        "title": "Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert   Spaces",
        "text": "Transfer operators such as the Perron--Frobenius or Koopman operator play an important role in the global analysis of complex dynamical systems. The eigenfunctions of these operators can be used to detect metastable sets, to project the dynamics onto the d"
    },
    {
        "title": "Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert   Spaces",
        "text": "ominant slow processes, or to separate superimposed signals. We extend transfer operator theory to reproducing kernel Hilbert spaces and show that these operators are related to Hilbert space representations of conditional distributions, known as condition"
    },
    {
        "title": "Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert   Spaces",
        "text": "al mean embeddings in the machine learning community. Moreover, numerical methods to compute empirical estimates of these embeddings are akin to data-driven methods for the approximation of transfer operators such as extended dynamic mode decomposition and"
    },
    {
        "title": "Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert   Spaces",
        "text": " its variants. One main benefit of the presented kernel-based approaches is that these methods can be applied to any domain where a similarity measure given by a kernel is available. We illustrate the results with the aid of guiding examples and highlight "
    },
    {
        "title": "Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert   Spaces",
        "text": "potential applications in molecular dynamics as well as video and text data analysis."
    },
    {
        "title": "Automatic Model Monitoring for Data Streams",
        "text": "Detecting concept drift is a well known problem that affects production systems. However, two important issues that are frequently not addressed in the literature are 1) the detection of drift when the labels are not immediately available; and 2) the autom"
    },
    {
        "title": "Automatic Model Monitoring for Data Streams",
        "text": "atic generation of explanations to identify possible causes for the drift. For example, a fraud detection model in online payments could show a drift due to a hot sale item (with an increase in false positives) or due to a true fraud attack (with an increa"
    },
    {
        "title": "Automatic Model Monitoring for Data Streams",
        "text": "se in false negatives) before labels are available. In this paper we propose SAMM, an automatic model monitoring system for data streams. SAMM detects concept drift using a time and space efficient unsupervised streaming algorithm and it generates alarm re"
    },
    {
        "title": "Automatic Model Monitoring for Data Streams",
        "text": "ports with a summary of the events and features that are important to explain it. SAMM was evaluated in five real world fraud detection datasets, each spanning periods up to eight months and totaling more than 22 million online transactions. We evaluated S"
    },
    {
        "title": "Automatic Model Monitoring for Data Streams",
        "text": "AMM using human feedback from domain experts, by sending them 100 reports generated by the system. Our results show that SAMM is able to detect anomalous events in a model life cycle that are considered useful by the domain experts. Given these results, SA"
    },
    {
        "title": "Automatic Model Monitoring for Data Streams",
        "text": "MM will be rolled out in a next version of Feedzai's Fraud Detection solution."
    },
    {
        "title": "Improving Sampling from Generative Autoencoders with Markov Chains",
        "text": "We focus on generative autoencoders, such as variational or adversarial autoencoders, which jointly learn a generative model alongside an inference model. Generative autoencoders are those which are trained to softly enforce a prior on the latent distribut"
    },
    {
        "title": "Improving Sampling from Generative Autoencoders with Markov Chains",
        "text": "ion learned by the inference model. We call the distribution to which the inference model maps observed samples, the learned latent distribution, which may not be consistent with the prior. We formulate a Markov chain Monte Carlo (MCMC) sampling process, e"
    },
    {
        "title": "Improving Sampling from Generative Autoencoders with Markov Chains",
        "text": "quivalent to iteratively decoding and encoding, which allows us to sample from the learned latent distribution. Since, the generative model learns to map from the learned latent distribution, rather than the prior, we may use MCMC to improve the quality of"
    },
    {
        "title": "Improving Sampling from Generative Autoencoders with Markov Chains",
        "text": " samples drawn from the generative model, especially when the learned latent distribution is far from the prior. Using MCMC sampling, we are able to reveal previously unseen differences between generative autoencoders trained either with or without a denoi"
    },
    {
        "title": "Improving Sampling from Generative Autoencoders with Markov Chains",
        "text": "sing criterion."
    },
    {
        "title": "The Recurrent Neural Tangent Kernel",
        "text": "The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architectur"
    },
    {
        "title": "The Recurrent Neural Tangent Kernel",
        "text": "e remains to be kernelized, namely, the recurrent neural network (RNN). In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of overparametrized RNNs. A key property of the RNTK shou"
    },
    {
        "title": "The Recurrent Neural Tangent Kernel",
        "text": "ld greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic"
    },
    {
        "title": "The Recurrent Neural Tangent Kernel",
        "text": " and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets."
    },
    {
        "title": "Stochastic Contrastive Learning",
        "text": "While state-of-the-art contrastive Self-Supervised Learning (SSL) models produce results competitive with their supervised counterparts, they lack the ability to infer latent variables. In contrast, prescribed latent variable (LV) models enable attributing"
    },
    {
        "title": "Stochastic Contrastive Learning",
        "text": " uncertainty, inducing task specific compression, and in general allow for more interpretable representations. In this work, we introduce LV approximations to large scale contrastive SSL models. We demonstrate that this addition improves downstream perform"
    },
    {
        "title": "Stochastic Contrastive Learning",
        "text": "ance (resulting in 96.42% and 77.49% test top-1 fine-tuned performance on CIFAR10 and ImageNet respectively with a ResNet50) as well as producing highly compressed representations (588x reduction) that are useful for interpretability, classification and re"
    },
    {
        "title": "Stochastic Contrastive Learning",
        "text": "gression downstream tasks."
    },
    {
        "title": "Sinogram upsampling using Primal-Dual UNet for undersampled CT and   radial MRI reconstruction",
        "text": "CT and MRI are two widely used clinical imaging modalities for non-invasive diagnosis. However, both of these modalities come with certain problems. CT uses harmful ionising radiation, and MRI suffers from slow acquisition speed. Both problems can be tackl"
    },
    {
        "title": "Sinogram upsampling using Primal-Dual UNet for undersampled CT and   radial MRI reconstruction",
        "text": "ed by undersampling, such as sparse sampling. However, such undersampled data leads to lower resolution and introduces artefacts. Several techniques, including deep learning based methods, have been proposed to reconstruct such data. However, the undersamp"
    },
    {
        "title": "Sinogram upsampling using Primal-Dual UNet for undersampled CT and   radial MRI reconstruction",
        "text": "led reconstruction problem for these two modalities was always considered as two different problems and tackled separately by different research works. This paper proposes a unified solution for both sparse CT and undersampled radial MRI reconstruction, ac"
    },
    {
        "title": "Sinogram upsampling using Primal-Dual UNet for undersampled CT and   radial MRI reconstruction",
        "text": "hieved by applying Fourier transform-based pre-processing on the radial MRI and then reconstructing both modalities using sinogram upsampling combined with filtered back-projection. The Primal-Dual network is a deep learning based method for reconstructing"
    },
    {
        "title": "Sinogram upsampling using Primal-Dual UNet for undersampled CT and   radial MRI reconstruction",
        "text": " sparsely-sampled CT data. This paper introduces Primal-Dual UNet, which improves the Primal-Dual network in terms of accuracy and reconstruction speed. The proposed method resulted in an average SSIM of 0.932 while performing sparse CT reconstruction for "
    },
    {
        "title": "Sinogram upsampling using Primal-Dual UNet for undersampled CT and   radial MRI reconstruction",
        "text": "fan-beam geometry with a sparsity level of 16, achieving a statistically significant improvement over the previous model, which resulted in 0.919. Furthermore, the proposed model resulted in 0.903 and 0.957 average SSIM while reconstructing undersampled br"
    },
    {
        "title": "Sinogram upsampling using Primal-Dual UNet for undersampled CT and   radial MRI reconstruction",
        "text": "ain and abdominal MRI data with an acceleration factor of 16 - statistically significant improvements over the original model, which resulted in 0.867 and 0.949. Finally, this paper shows that the proposed network not only improves the overall image qualit"
    },
    {
        "title": "Sinogram upsampling using Primal-Dual UNet for undersampled CT and   radial MRI reconstruction",
        "text": "y, but also improves the image quality for the regions-of-interest; as well as generalises better in presence of a needle."
    },
    {
        "title": "How Much Reading Does Reading Comprehension Require? A Critical   Investigation of Popular Benchmarks",
        "text": "Many recent papers address reading comprehension, where examples consist of (question, passage, answer) tuples. Presumably, a model must combine information from both questions and passages to predict corresponding answers. However, despite intense interes"
    },
    {
        "title": "How Much Reading Does Reading Comprehension Require? A Critical   Investigation of Popular Benchmarks",
        "text": "t in the topic, with hundreds of published papers vying for leaderboard dominance, basic questions about the difficulty of many popular benchmarks remain unanswered. In this paper, we establish sensible baselines for the bAbI, SQuAD, CBT, CNN, and Who-did-"
    },
    {
        "title": "How Much Reading Does Reading Comprehension Require? A Critical   Investigation of Popular Benchmarks",
        "text": "What datasets, finding that question- and passage-only models often perform surprisingly well. On $14$ out of $20$ bAbI tasks, passage-only models achieve greater than $50\\%$ accuracy, sometimes matching the full model. Interestingly, while CBT provides $2"
    },
    {
        "title": "How Much Reading Does Reading Comprehension Require? A Critical   Investigation of Popular Benchmarks",
        "text": "0$-sentence stories only the last is needed for comparably accurate prediction. By comparison, SQuAD and CNN appear better-constructed."
    },
    {
        "title": "MCP: Learning Composable Hierarchical Control with Multiplicative   Compositional Policies",
        "text": "Humans are able to perform a myriad of sophisticated tasks by drawing upon skills acquired through prior experience. For autonomous agents to have this capability, they must be able to extract reusable skills from past experience that can be recombined in "
    },
    {
        "title": "MCP: Learning Composable Hierarchical Control with Multiplicative   Compositional Policies",
        "text": "new ways for subsequent tasks. Furthermore, when controlling complex high-dimensional morphologies, such as humanoid bodies, tasks often require coordination of multiple skills simultaneously. Learning discrete primitives for every combination of skills qu"
    },
    {
        "title": "MCP: Learning Composable Hierarchical Control with Multiplicative   Compositional Policies",
        "text": "ickly becomes prohibitive. Composable primitives that can be recombined to create a large variety of behaviors can be more suitable for modeling this combinatorial explosion. In this work, we propose multiplicative compositional policies (MCP), a method fo"
    },
    {
        "title": "MCP: Learning Composable Hierarchical Control with Multiplicative   Compositional Policies",
        "text": "r learning reusable motor skills that can be composed to produce a range of complex behaviors. Our method factorizes an agent's skills into a collection of primitives, where multiple primitives can be activated simultaneously via multiplicative composition"
    },
    {
        "title": "MCP: Learning Composable Hierarchical Control with Multiplicative   Compositional Policies",
        "text": ". This flexibility allows the primitives to be transferred and recombined to elicit new behaviors as necessary for novel tasks. We demonstrate that MCP is able to extract composable skills for highly complex simulated characters from pre-training tasks, su"
    },
    {
        "title": "MCP: Learning Composable Hierarchical Control with Multiplicative   Compositional Policies",
        "text": "ch as motion imitation, and then reuse these skills to solve challenging continuous control tasks, such as dribbling a soccer ball to a goal, and picking up an object and transporting it to a target location."
    },
    {
        "title": "CNN-based Dual-Chain Models for Knowledge Graph Learning",
        "text": "Knowledge graph learning plays a critical role in integrating domain specific knowledge bases when deploying machine learning and data mining models in practice. Existing methods on knowledge graph learning primarily focus on modeling the relations among e"
    },
    {
        "title": "CNN-based Dual-Chain Models for Knowledge Graph Learning",
        "text": "ntities as translations among the relations and entities, and many of these methods are not able to handle zero-shot problems, when new entities emerge. In this paper, we present a new convolutional neural network (CNN)-based dual-chain model. Different fr"
    },
    {
        "title": "CNN-based Dual-Chain Models for Knowledge Graph Learning",
        "text": "om translation based methods, in our model, interactions among relations and entities are directly captured via CNN over their embeddings. Moreover, a secondary chain of learning is conducted simultaneously to incorporate additional information and to enab"
    },
    {
        "title": "CNN-based Dual-Chain Models for Knowledge Graph Learning",
        "text": "le better performance. We also present an extension of this model, which incorporates descriptions of entities and learns a second set of entity embeddings from the descriptions. As a result, the extended model is able to effectively handle zero-shot probl"
    },
    {
        "title": "CNN-based Dual-Chain Models for Knowledge Graph Learning",
        "text": "ems. We conducted comprehensive experiments, comparing our methods with 15 methods on 8 benchmark datasets. Extensive experimental results demonstrate that our proposed methods achieve or outperform the state-of-the-art results on knowledge graph learning,"
    },
    {
        "title": "CNN-based Dual-Chain Models for Knowledge Graph Learning",
        "text": " and outperform other methods on zero-shot problems. In addition, our methods applied to real-world biomedical data are able to produce results that conform to expert domain knowledge."
    },
    {
        "title": "Outcome-Driven Dynamic Refugee Assignment with Allocation Balancing",
        "text": "This study proposes two new dynamic assignment algorithms to match refugees and asylum seekers to geographic localities within a host country. The first, currently implemented in a multi-year pilot in Switzerland, seeks to maximize the average predicted em"
    },
    {
        "title": "Outcome-Driven Dynamic Refugee Assignment with Allocation Balancing",
        "text": "ployment level (or any measured outcome of interest) of refugees through a minimum-discord online assignment algorithm. Although the proposed algorithm achieves near-optimal expected employment compared to the hindsight-optimal solution (and improves upon "
    },
    {
        "title": "Outcome-Driven Dynamic Refugee Assignment with Allocation Balancing",
        "text": "the status quo procedure by about 40%), it results in a periodically imbalanced allocation to the localities over time. This leads to undesirable workload inefficiencies for resettlement resources and agents. To address this problem, the second algorithm b"
    },
    {
        "title": "Outcome-Driven Dynamic Refugee Assignment with Allocation Balancing",
        "text": "alances the goal of improving refugee outcomes with the desire for an even allocation over time. The performance of the proposed methods is illustrated using real refugee resettlement data from a large resettlement agency in the United States. On this data"
    },
    {
        "title": "Outcome-Driven Dynamic Refugee Assignment with Allocation Balancing",
        "text": "set, we find that the allocation balancing algorithm can achieve near-perfect balance over time with only a small loss in expected employment compared to the pure employment-maximizing algorithm. In addition, the allocation balancing algorithm offers a num"
    },
    {
        "title": "Outcome-Driven Dynamic Refugee Assignment with Allocation Balancing",
        "text": "ber of ancillary benefits compared to pure outcome-maximization, including robustness to unknown arrival flows and greater exploration."
    },
    {
        "title": "On the Oracle Complexity of Higher-Order Smooth Non-Convex Finite-Sum   Optimization",
        "text": "We prove lower bounds for higher-order methods in smooth non-convex finite-sum optimization. Our contribution is threefold: We first show that a deterministic algorithm cannot profit from the finite-sum structure of the objective, and that simulating a pth"
    },
    {
        "title": "On the Oracle Complexity of Higher-Order Smooth Non-Convex Finite-Sum   Optimization",
        "text": "-order regularized method on the whole function by constructing exact gradient information is optimal up to constant factors. We further show lower bounds for randomized algorithms and compare them with the best known upper bounds. To address some gaps bet"
    },
    {
        "title": "On the Oracle Complexity of Higher-Order Smooth Non-Convex Finite-Sum   Optimization",
        "text": "ween the bounds, we propose a new second-order smoothness assumption that can be seen as an analogue of the first-order mean-squared smoothness assumption. We prove that it is sufficient to ensure state-of-the-art convergence guarantees, while allowing for"
    },
    {
        "title": "On the Oracle Complexity of Higher-Order Smooth Non-Convex Finite-Sum   Optimization",
        "text": " a sharper lower bound."
    },
    {
        "title": "Distributed Training of Deep Neural Networks with Theoretical Analysis:   Under SSP Setting",
        "text": "We propose a distributed approach to train deep neural networks (DNNs), which has guaranteed convergence theoretically and great scalability empirically: close to 6 times faster on instance of ImageNet data set when run with 6 machines. The proposed scheme"
    },
    {
        "title": "Distributed Training of Deep Neural Networks with Theoretical Analysis:   Under SSP Setting",
        "text": " is close to optimally scalable in terms of number of machines, and guaranteed to converge to the same optima as the undistributed setting. The convergence and scalability of the distributed setting is shown empirically across different datasets (TIMIT and"
    },
    {
        "title": "Distributed Training of Deep Neural Networks with Theoretical Analysis:   Under SSP Setting",
        "text": " ImageNet) and machine learning tasks (image classification and phoneme extraction). The convergence analysis provides novel insights into this complex learning scheme, including: 1) layerwise convergence, and 2) convergence of the weights in probability."
    },
    {
        "title": "Maximum Density Divergence for Domain Adaptation",
        "text": "Unsupervised domain adaptation addresses the problem of transferring knowledge from a well-labeled source domain to an unlabeled target domain where the two domains have distinctive data distributions. Thus, the essence of domain adaptation is to mitigate "
    },
    {
        "title": "Maximum Density Divergence for Domain Adaptation",
        "text": "the distribution divergence between the two domains. The state-of-the-art methods practice this very idea by either conducting adversarial training or minimizing a metric which defines the distribution gaps. In this paper, we propose a new domain adaptatio"
    },
    {
        "title": "Maximum Density Divergence for Domain Adaptation",
        "text": "n method named Adversarial Tight Match (ATM) which enjoys the benefits of both adversarial training and metric learning. Specifically, at first, we propose a novel distance loss, named Maximum Density Divergence (MDD), to quantify the distribution divergen"
    },
    {
        "title": "Maximum Density Divergence for Domain Adaptation",
        "text": "ce. MDD minimizes the inter-domain divergence (\"match\" in ATM) and maximizes the intra-class density (\"tight\" in ATM). Then, to address the equilibrium challenge issue in adversarial domain adaptation, we consider leveraging the proposed MDD into adversari"
    },
    {
        "title": "Maximum Density Divergence for Domain Adaptation",
        "text": "al domain adaptation framework. At last, we tailor the proposed MDD as a practical learning loss and report our ATM. Both empirical evaluation and theoretical analysis are reported to verify the effectiveness of the proposed method. The experimental result"
    },
    {
        "title": "Maximum Density Divergence for Domain Adaptation",
        "text": "s on four benchmarks, both classical and large-scale, show that our method is able to achieve new state-of-the-art performance on most evaluations. Codes and datasets used in this paper are available at {\\it github.com/lijin118/ATM}."
    },
    {
        "title": "maskGRU: Tracking Small Objects in the Presence of Large Background   Motions",
        "text": "We propose a recurrent neural network-based spatio-temporal framework named maskGRU for the detection and tracking of small objects in videos. While there have been many developments in the area of object tracking in recent years, tracking a small moving o"
    },
    {
        "title": "maskGRU: Tracking Small Objects in the Presence of Large Background   Motions",
        "text": "bject amid other moving objects and actors (such as a ball amid moving players in sports footage) continues to be a difficult task. Existing spatio-temporal networks, such as convolutional Gated Recurrent Units (convGRUs), are difficult to train and have t"
    },
    {
        "title": "maskGRU: Tracking Small Objects in the Presence of Large Background   Motions",
        "text": "rouble accurately tracking small objects under such conditions. To overcome these difficulties, we developed the maskGRU framework that uses a weighted sum of the internal hidden state produced by a convGRU and a 3-channel mask of the tracked object's pred"
    },
    {
        "title": "maskGRU: Tracking Small Objects in the Presence of Large Background   Motions",
        "text": "icted bounding box as the hidden state to be used at the next time step of the underlying convGRU. We believe the technique of incorporating a mask into the hidden state through a weighted sum has two benefits: controlling the effect of exploding gradients"
    },
    {
        "title": "maskGRU: Tracking Small Objects in the Presence of Large Background   Motions",
        "text": " and introducing an attention-like mechanism into the network by indicating where in the previous video frame the object is located. Our experiments show that maskGRU outperforms convGRU at tracking objects that are small relative to the video resolution e"
    },
    {
        "title": "maskGRU: Tracking Small Objects in the Presence of Large Background   Motions",
        "text": "ven in the presence of other moving objects."
    },
    {
        "title": "From Language to Programs: Bridging Reinforcement Learning and Maximum   Marginal Likelihood",
        "text": "Our goal is to learn a semantic parser that maps natural language utterances into executable programs when only indirect supervision is available: examples are labeled with the correct execution result, but not the program itself. Consequently, we must sea"
    },
    {
        "title": "From Language to Programs: Bridging Reinforcement Learning and Maximum   Marginal Likelihood",
        "text": "rch the space of programs for those that output the correct result, while not being misled by spurious programs: incorrect programs that coincidentally output the correct result. We connect two common learning paradigms, reinforcement learning (RL) and max"
    },
    {
        "title": "From Language to Programs: Bridging Reinforcement Learning and Maximum   Marginal Likelihood",
        "text": "imum marginal likelihood (MML), and then present a new learning algorithm that combines the strengths of both. The new algorithm guards against spurious programs by combining the systematic search traditionally employed in MML with the randomized explorati"
    },
    {
        "title": "From Language to Programs: Bridging Reinforcement Learning and Maximum   Marginal Likelihood",
        "text": "on of RL, and by updating parameters such that probability is spread more evenly across consistent programs. We apply our learning algorithm to a new neural semantic parser and show significant gains over existing state-of-the-art results on a recent conte"
    },
    {
        "title": "From Language to Programs: Bridging Reinforcement Learning and Maximum   Marginal Likelihood",
        "text": "xt-dependent semantic parsing task."
    },
    {
        "title": "Systematic Assessment of Hyperdimensional Computing for Epileptic   Seizure Detection",
        "text": "Hyperdimensional computing is a promising novel paradigm for low-power embedded machine learning. It has been applied on different biomedical applications, and particularly on epileptic seizure detection. Unfortunately, due to differences in data preparati"
    },
    {
        "title": "Systematic Assessment of Hyperdimensional Computing for Epileptic   Seizure Detection",
        "text": "on, segmentation, encoding strategies, and performance metrics, results are hard to compare, which makes building upon that knowledge difficult. Thus, the main goal of this work is to perform a systematic assessment of the HD computing framework for the de"
    },
    {
        "title": "Systematic Assessment of Hyperdimensional Computing for Epileptic   Seizure Detection",
        "text": "tection of epileptic seizures, comparing different feature approaches mapped to HD vectors. More precisely, we test two previously implemented features as well as several novel approaches with HD computing on epileptic seizure detection. We evaluate them i"
    },
    {
        "title": "Systematic Assessment of Hyperdimensional Computing for Epileptic   Seizure Detection",
        "text": "n a comparable way, i.e., with the same preprocessing setup, and with the identical performance measures. We use two different datasets in order to assess the generalizability of our conclusions. The systematic assessment involved three primary aspects rel"
    },
    {
        "title": "Systematic Assessment of Hyperdimensional Computing for Epileptic   Seizure Detection",
        "text": "evant for potential wearable implementations: 1) detection performance, 2) memory requirements, and 3) computational complexity. Our analysis shows a significant difference in detection performance between approaches, but also that the ones with the highes"
    },
    {
        "title": "Systematic Assessment of Hyperdimensional Computing for Epileptic   Seizure Detection",
        "text": "t performance might not be ideal for wearable applications due to their high memory or computational requirements. Furthermore, we evaluate a post-processing strategy to adjust the predictions to the dynamics of epileptic seizures, showing that performance"
    },
    {
        "title": "Systematic Assessment of Hyperdimensional Computing for Epileptic   Seizure Detection",
        "text": " is significantly improved in all the approaches and also that after post-processing, differences in performance are much smaller between approaches."
    },
    {
        "title": "HEU Emotion: A Large-scale Database for Multi-modal Emotion Recognition   in the Wild",
        "text": "The study of affective computing in the wild setting is underpinned by databases. Existing multimodal emotion databases in the real-world conditions are few and small, with a limited number of subjects and expressed in a single language. To meet this requi"
    },
    {
        "title": "HEU Emotion: A Large-scale Database for Multi-modal Emotion Recognition   in the Wild",
        "text": "rement, we collected, annotated, and prepared to release a new natural state video database (called HEU Emotion). HEU Emotion contains a total of 19,004 video clips, which is divided into two parts according to the data source. The first part contains vide"
    },
    {
        "title": "HEU Emotion: A Large-scale Database for Multi-modal Emotion Recognition   in the Wild",
        "text": "os downloaded from Tumblr, Google, and Giphy, including 10 emotions and two modalities (facial expression and body posture). The second part includes corpus taken manually from movies, TV series, and variety shows, consisting of 10 emotions and three modal"
    },
    {
        "title": "HEU Emotion: A Large-scale Database for Multi-modal Emotion Recognition   in the Wild",
        "text": "ities (facial expression, body posture, and emotional speech). HEU Emotion is by far the most extensive multi-modal emotional database with 9,951 subjects. In order to provide a benchmark for emotion recognition, we used many conventional machine learning "
    },
    {
        "title": "HEU Emotion: A Large-scale Database for Multi-modal Emotion Recognition   in the Wild",
        "text": "and deep learning methods to evaluate HEU Emotion. We proposed a Multi-modal Attention module to fuse multi-modal features adaptively. After multi-modal fusion, the recognition accuracies for the two parts increased by 2.19% and 4.01% respectively over tho"
    },
    {
        "title": "HEU Emotion: A Large-scale Database for Multi-modal Emotion Recognition   in the Wild",
        "text": "se of single-modal facial expression recognition."
    },
    {
        "title": "Localized Spectral Graph Filter Frames: A Unifying Framework, Survey of   Design Considerations, and Numerical Comparison (Extended Cut)",
        "text": "Representing data residing on a graph as a linear combination of building block signals can enable efficient and insightful visual or statistical analysis of the data, and such representations prove useful as regularizers in signal processing and machine l"
    },
    {
        "title": "Localized Spectral Graph Filter Frames: A Unifying Framework, Survey of   Design Considerations, and Numerical Comparison (Extended Cut)",
        "text": "earning tasks. Designing collections of building block signals -- or more formally, dictionaries of atoms -- that specifically account for the underlying graph structure as well as any available representative training signals has been an active area of re"
    },
    {
        "title": "Localized Spectral Graph Filter Frames: A Unifying Framework, Survey of   Design Considerations, and Numerical Comparison (Extended Cut)",
        "text": "search over the last decade. In this article, we survey a particular class of dictionaries called localized spectral graph filter frames, whose atoms are created by localizing spectral patterns to different regions of the graph. After showing how this clas"
    },
    {
        "title": "Localized Spectral Graph Filter Frames: A Unifying Framework, Survey of   Design Considerations, and Numerical Comparison (Extended Cut)",
        "text": "s encompasses a variety of approaches from spectral graph wavelets to graph filter banks, we focus on the two main questions of how to design the spectral filters and how to select the center vertices to which the patterns are localized. Throughout, we emp"
    },
    {
        "title": "Localized Spectral Graph Filter Frames: A Unifying Framework, Survey of   Design Considerations, and Numerical Comparison (Extended Cut)",
        "text": "hasize computationally efficient methods that ensure the resulting transforms and their inverses can be applied to data residing on large, sparse graphs. We demonstrate how this class of transform methods can be used in signal processing tasks such as deno"
    },
    {
        "title": "Localized Spectral Graph Filter Frames: A Unifying Framework, Survey of   Design Considerations, and Numerical Comparison (Extended Cut)",
        "text": "ising and non-linear approximation, and provide code for readers to experiment with these methods in new application domains."
    },
    {
        "title": "Equivariant and Invariant Reynolds Networks",
        "text": "Invariant and equivariant networks are useful in learning data with symmetry, including images, sets, point clouds, and graphs. In this paper, we consider invariant and equivariant networks for symmetries of finite groups. Invariant and equivariant network"
    },
    {
        "title": "Equivariant and Invariant Reynolds Networks",
        "text": "s have been constructed by various researchers using Reynolds operators. However, Reynolds operators are computationally expensive when the order of the group is large because they use the sum over the whole group, which poses an implementation difficulty."
    },
    {
        "title": "Equivariant and Invariant Reynolds Networks",
        "text": " To overcome this difficulty, we consider representing the Reynolds operator as a sum over a subset instead of a sum over the whole group. We call such a subset a Reynolds design, and an operator defined by a sum over a Reynolds design a reductive Reynolds"
    },
    {
        "title": "Equivariant and Invariant Reynolds Networks",
        "text": " operator. For example, in the case of a graph with $n$ nodes, the computational complexity of the reductive Reynolds operator is reduced to $O(n^2)$, while the computational complexity of the Reynolds operator is $O(n!)$. We construct learning models base"
    },
    {
        "title": "Equivariant and Invariant Reynolds Networks",
        "text": "d on the reductive Reynolds operator called equivariant and invariant Reynolds networks (ReyNets) and prove that they have universal approximation property. Reynolds designs for equivariant ReyNets are derived from combinatorial observations with Young dia"
    },
    {
        "title": "Equivariant and Invariant Reynolds Networks",
        "text": "grams, while Reynolds designs for invariant ReyNets are derived from invariants called Reynolds dimensions defined on the set of invariant polynomials. Numerical experiments show that the performance of our models is comparable to state-of-the-art methods."
    },
    {
        "title": "Protecting Neural Networks with Hierarchical Random Switching: Towards   Better Robustness-Accuracy Trade-off for Stochastic Defenses",
        "text": "Despite achieving remarkable success in various domains, recent studies have uncovered the vulnerability of deep neural networks to adversarial perturbations, creating concerns on model generalizability and new threats such as prediction-evasive misclassif"
    },
    {
        "title": "Protecting Neural Networks with Hierarchical Random Switching: Towards   Better Robustness-Accuracy Trade-off for Stochastic Defenses",
        "text": "ication or stealthy reprogramming. Among different defense proposals, stochastic network defenses such as random neuron activation pruning or random perturbation to layer inputs are shown to be promising for attack mitigation. However, one critical drawbac"
    },
    {
        "title": "Protecting Neural Networks with Hierarchical Random Switching: Towards   Better Robustness-Accuracy Trade-off for Stochastic Defenses",
        "text": "k of current defenses is that the robustness enhancement is at the cost of noticeable performance degradation on legitimate data, e.g., large drop in test accuracy. This paper is motivated by pursuing for a better trade-off between adversarial robustness a"
    },
    {
        "title": "Protecting Neural Networks with Hierarchical Random Switching: Towards   Better Robustness-Accuracy Trade-off for Stochastic Defenses",
        "text": "nd test accuracy for stochastic network defenses. We propose Defense Efficiency Score (DES), a comprehensive metric that measures the gain in unsuccessful attack attempts at the cost of drop in test accuracy of any defense. To achieve a better DES, we prop"
    },
    {
        "title": "Protecting Neural Networks with Hierarchical Random Switching: Towards   Better Robustness-Accuracy Trade-off for Stochastic Defenses",
        "text": "ose hierarchical random switching (HRS), which protects neural networks through a novel randomization scheme. A HRS-protected model contains several blocks of randomly switching channels to prevent adversaries from exploiting fixed model structures and par"
    },
    {
        "title": "Protecting Neural Networks with Hierarchical Random Switching: Towards   Better Robustness-Accuracy Trade-off for Stochastic Defenses",
        "text": "ameters for their malicious purposes. Extensive experiments show that HRS is superior in defending against state-of-the-art white-box and adaptive adversarial misclassification attacks. We also demonstrate the effectiveness of HRS in defending adversarial "
    },
    {
        "title": "Protecting Neural Networks with Hierarchical Random Switching: Towards   Better Robustness-Accuracy Trade-off for Stochastic Defenses",
        "text": "reprogramming, which is the first defense against adversarial programs. Moreover, in most settings the average DES of HRS is at least 5X higher than current stochastic network defenses, validating its significantly improved robustness-accuracy trade-off."
    },
    {
        "title": "On Adaptivity in Information-constrained Online Learning",
        "text": "We study how to adapt to smoothly-varying ('easy') environments in well-known online learning problems where acquiring information is expensive. For the problem of label efficient prediction, which is a budgeted version of prediction with expert advice, we"
    },
    {
        "title": "On Adaptivity in Information-constrained Online Learning",
        "text": " present an online algorithm whose regret depends optimally on the number of labels allowed and $Q^*$ (the quadratic variation of the losses of the best action in hindsight), along with a parameter-free counterpart whose regret depends optimally on $Q$ (th"
    },
    {
        "title": "On Adaptivity in Information-constrained Online Learning",
        "text": "e quadratic variation of the losses of all the actions). These quantities can be significantly smaller than $T$ (the total time horizon), yielding an improvement over existing, variation-independent results for the problem. We then extend our analysis to h"
    },
    {
        "title": "On Adaptivity in Information-constrained Online Learning",
        "text": "andle label efficient prediction with bandit feedback, i.e., label efficient bandits. Our work builds upon the framework of optimistic online mirror descent, and leverages second order corrections along with a carefully designed hybrid regularizer that enc"
    },
    {
        "title": "On Adaptivity in Information-constrained Online Learning",
        "text": "odes the constrained information structure of the problem. We then consider revealing action-partial monitoring games -- a version of label efficient prediction with additive information costs, which in general are known to lie in the \\textit{hard} class o"
    },
    {
        "title": "On Adaptivity in Information-constrained Online Learning",
        "text": "f games having minimax regret of order $T^{\\frac{2}{3}}$. We provide a strategy with an $\\mathcal{O}((Q^*T)^{\\frac{1}{3}})$ bound for revealing action games, along with one with a $\\mathcal{O}((QT)^{\\frac{1}{3}})$ bound for the full class of hard partial m"
    },
    {
        "title": "On Adaptivity in Information-constrained Online Learning",
        "text": "onitoring games, both being strict improvements over current bounds."
    },
    {
        "title": "ChainGAN: A sequential approach to GANs",
        "text": "We propose a new architecture and training methodology for generative adversarial networks. Current approaches attempt to learn the transformation from a noise sample to a generated data sample in one shot. Our proposed generator architecture, called $\\tex"
    },
    {
        "title": "ChainGAN: A sequential approach to GANs",
        "text": "tit{ChainGAN}$, uses a two-step process. It first attempts to transform a noise vector into a crude sample, similar to a traditional generator. Next, a chain of networks, called $\\textit{editors}$, attempt to sequentially enhance this sample. We train each"
    },
    {
        "title": "ChainGAN: A sequential approach to GANs",
        "text": " of these units independently, instead of with end-to-end backpropagation on the entire chain. Our model is robust, efficient, and flexible as we can apply it to various network architectures. We provide rationale for our choices and experimentally evaluat"
    },
    {
        "title": "ChainGAN: A sequential approach to GANs",
        "text": "e our model, achieving competitive results on several datasets."
    },
    {
        "title": "Modeling Human Categorization of Natural Images Using Deep Feature   Representations",
        "text": "Over the last few decades, psychologists have developed sophisticated formal models of human categorization using simple artificial stimuli. In this paper, we use modern machine learning methods to extend this work into the realm of naturalistic stimuli, e"
    },
    {
        "title": "Modeling Human Categorization of Natural Images Using Deep Feature   Representations",
        "text": "nabling human categorization to be studied over the complex visual domain in which it evolved and developed. We show that representations derived from a convolutional neural network can be used to model behavior over a database of >300,000 human natural im"
    },
    {
        "title": "Modeling Human Categorization of Natural Images Using Deep Feature   Representations",
        "text": "age classifications, and find that a group of models based on these representations perform well, near the reliability of human judgments. Interestingly, this group includes both exemplar and prototype models, contrasting with the dominance of exemplar mod"
    },
    {
        "title": "Modeling Human Categorization of Natural Images Using Deep Feature   Representations",
        "text": "els in previous work. We are able to improve the performance of the remaining models by preprocessing neural network representations to more closely capture human similarity judgments."
    }
]